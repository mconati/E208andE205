{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8a165ac093a29dee",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# HW 4: Implementing a Neural Network\n",
    "\n",
    "In this homework assignment, you will implement a neural network from scratch using [`numpy`](https://numpy.org/).\n",
    "\n",
    "There are three parts to this assignment:\n",
    "- In part 1, you will implement a neural network that processes a single training sample at a time.  \n",
    "- In part 2, you will vectorize your implementation to process all training samples in parallel.\n",
    "- In part 3, you will use your neural network implementation to train an autoencoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d215173ace0eb924",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "A few important notes:\n",
    "* For autograding purposes, many cells are set in *read-only* mode.  This means you cannot edit, delete, or move these cells. \n",
    "\n",
    "* For parts 1 and 2, you should not modify any code except where you see `# YOUR CODE HERE`.  Please remove `raise NotImplementedError()` and write your code in its place.\n",
    "\n",
    "* We provide code cells that check your code as you progress through the assignment. These tests will help catch obvious errors, but passing these tests is not a guarantee that your implementation is correct. Your notebook will be graded on a more complete set of tests which are not visible to you.\n",
    "\n",
    "* You may add new cells anywhere in the notebook while working, but make sure to delete them before submitting your assignment.  Please do not change the sequence of the cells.  This is to ensure that autograding works properly.\n",
    "\n",
    "* If you run cells out of order, it may change variable values which may cause tests to fail.  The easiest way to avoid this problem is to use the ***Restart & Run all*** option available in the **Kernel** tab.  Before submitting, you should do this to ensure that your notebook can be run from beginning to end without errors.  This is how your notebook will be graded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d269cca8e2af3e60",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Part 1\n",
    "\n",
    "In this part you will implement a simple neural network with one hidden layer.  This implementation will process one data point at a time.  You will have to implement functions to do the following:\n",
    "\n",
    "* initialize network weights\n",
    "* calculate sigmoid function\n",
    "* calculate softmax function\n",
    "* forward propagation\n",
    "* calculate cross-entropy loss\n",
    "* backward propagation\n",
    "* update network weights\n",
    "* train the network on a set of data points\n",
    "* use the trained network to make a prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-aa90e76427fb3726",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's first define our notation:\n",
    "\n",
    "* The input $x$ and output $y$ are vectors of shape $(1, n_0)$ and $(1, n_2)$ respectively. \n",
    "\n",
    "* The number of nodes in the input, hidden, and output layers are $n_0$, $n_1$, and $n_2$ respectively.\n",
    "\n",
    "* The weights connecting the input and hidden layer are given by the matrix $W_1$ of shape $(n_0, n_1)$.\n",
    "\n",
    "* The weights connecting the hidden and output layer are given by the matrix $W_2$ of shape $(n_1, n_2)$.\n",
    "\n",
    "* The bias terms feeding into the hidden layer are given by the vector $b_1$ of shape $(1, n_1)$.\n",
    "\n",
    "* The bias terms feeding into the output layer are given by the vector $b_2$ of shape $(1, n_2)$.\n",
    "\n",
    "* The sigmoid activations for the hidden layer are given by the vector $a_1$ of shape $(1, n_1)$.\n",
    "\n",
    "* The softmax outputs at the output layer are given by the vector $a_2$ of shape $(1, n_2)$.\n",
    "\n",
    "Make sure to double check your dimensions when debugging.  Caution: An array of shape (1, n) is *not* the same as an array of shape (n, )."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-319c10bb5a5c256b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's define our layer sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-16443fd8b77928f3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "n0 = 4 # size of input layer\n",
    "n1 = 7 # size of hidden layer\n",
    "n2 = 3 # size of output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3398bc2c1228257c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8b9e781573ae18c1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Initialize weight matrices\n",
    "\n",
    "Initialize the weight matrices $W_1$ and $W_2$ with small random values.  You can use [`np.random.randn()`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.randn.html) and multiply the matrices by $0.01$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "weights",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def initialize_weights(n0, n1, n2):\n",
    "    '''\n",
    "    Returns a tuple (W1, W2) containing the initialized weight matrices.\n",
    "    '''\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    # Make weights\n",
    "    W1 = np.random.randn(n0,n1) * 0.01\n",
    "    W2 = np.random.randn(n1,n2) * 0.01\n",
    "    return (W1, W2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-83f22147650c23d9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "W1, W2 = initialize_weights(n0, n1, n2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "weights_test",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(W1, np.ndarray), \"W1 should be a numpy array.\"\n",
    "assert isinstance(W2, np.ndarray), \"W2 should be a numpy array.\"\n",
    "\n",
    "assert W1.shape[0] == n0, \"The number of rows in W1 is incorrect.\"\n",
    "assert W1.shape[1] == n1, \"The number of columns in W1 is incorrect.\"\n",
    "\n",
    "assert W2.shape[0] == n1, \"The number of rows in W2 is incorrect.\"\n",
    "assert W2.shape[1] == n2, \"The number of columns in W2 is incorrect.\"\n",
    "\n",
    "assert np.sum(np.abs(W1)) != 0, \"All the elements in W1 should not be zero.\"\n",
    "assert np.sum(np.abs(W2)) != 0, \"All the elements in W2 should not be zero.\"\n",
    "\n",
    "assert np.round(W1[0,0], decimals=8) == 0.01764052, \"Your matrices are not normalized correctly.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-da0bc716eed96fc2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Initialize bias terms\n",
    "\n",
    "Initialize the bias terms $b_1$ and $b_2$ with zeros.  You can use [`np.zeros()`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.zeros.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-efc325bb7130d00b",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def initialize_bias(n0, n1, n2):\n",
    "    '''\n",
    "    Returns a tuple of vectors (b1, b2) with the initialized bias terms.\n",
    "    '''\n",
    "    \n",
    "    #Make bias terms\n",
    "    b1 = np.zeros((1,n1))\n",
    "    b2 = np.zeros((1,n2))\n",
    "    \n",
    "    return (b1, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c0ba658379bd6651",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "b1, b2 = initialize_bias(n0, n1, n2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-713f6dea75488458",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(b1, np.ndarray), \"b1 should be a numpy array.\"\n",
    "assert isinstance(b2, np.ndarray), \"b2 should be a numpy array.\"\n",
    "\n",
    "assert b1.shape[0] == 1, \"The number of rows in b1 is incorrect.\"\n",
    "assert b1.shape[1] == n1, \"The number of columns in b1 is incorrect.\"\n",
    "\n",
    "assert b2.shape[0] == 1, \"The number of rows in b2 is incorrect.\"\n",
    "assert b2.shape[1] == n2, \"The number of columns in b2 is incorrect.\"\n",
    "\n",
    "assert np.all(b1 == 0), \"The values in b1 should be all zeros, but they are not.\"\n",
    "assert np.all(b2 == 0), \"The values in b2 should be all zeros, but they are not.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cb9f5e5331dc3cac",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Sigmoid activation function\n",
    "\n",
    "$$sig(z) = \\frac{1}{1+e^{-z}}$$\n",
    "\n",
    "Define the sigmoid activation function.  If the input is a numpy array, the function should apply the sigmoid activation function to each element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "sigmoid",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    '''\n",
    "    Returns the sigmoid function value for each element in z.\n",
    "    '''\n",
    "    \n",
    "    #Implement the sigmoid with np\n",
    "    s = 1/(1+np.exp(-1*z))\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "sigmoid_test",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert sigmoid(0) == 0.5, \"Returns incorrect value for z = 0.\"\n",
    "assert sigmoid(np.Inf) == 1, \"Returns incorrect value for z = infinity.\"\n",
    "assert sigmoid(np.NINF) == 0, \"Returns incorrect value for z = -infinity.\"\n",
    "assert np.isnan(sigmoid(np.nan)), \"Returns incorrect value when z is nan.\"\n",
    "\n",
    "assert np.allclose(np.round(sigmoid(np.array([1, 2, 3])), decimals=8), np.array([0.73105858, 0.88079708, 0.95257413])), \"Returns incorrect values for 1D input.\"\n",
    "assert np.allclose(np.round(sigmoid(np.array([[-1, 6], [-3, np.Inf], [4, np.NINF]])), decimals=8),  np.array([[0.26894142, 0.99752738], [0.04742587, 1], [0.98201379, 0]])), \"Returns incorrect values for 2D input.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Softmax function\n",
    "\n",
    "$$ softmax([z_1, \\dots, z_k]) = \\left[\\frac{e^{z_1}}{e^{z_1}+ \\dots + e^{z_k}}, \\dots, \\frac{e^{z_k}}{e^{z_1}+ \\dots + e^{z_k}}\\right]$$\n",
    "\n",
    "The softmax function takes an input vector of length $k$ and returns a valid probability distribution.  In other words, it converts a set of $k$ numerical scores into a set of $k$ probabilities.  This is useful for multi-class classification, where we want to know the probability that the input is from a certain class.  Note that adding a constant scalar to all terms does not change the output:\n",
    "\n",
    "$$ softmax([z_1, z_2, \\dots, z_k]) = softmax([z_1 + c, z_2 + c, \\dots, z_k + c])$$\n",
    "\n",
    "In practice, it is useful to first subtract $c = max\\{z_1, z_2, \\dots, z_k\\}$ from the input terms in order to avoid numerical overflow or underflow.\n",
    "\n",
    "Define the softmax function.  Since we will use this function in both part 1 and part 2, we will implement a vectorized version.  Your implementation should use vectorized operations and broadcasting, so it should not have any for loops.  You may need to use [`np.max()`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.max.html), [`np.exp()`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.exp.html), and [`np.sum()`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.sum.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-998f7478d5a5400a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    '''\n",
    "    Applies the softmax function to each row of a matrix Z.\n",
    "    This implementation should be vectorized, so it should not contain any for loops.\n",
    "    '''\n",
    "    assert isinstance(Z, np.ndarray) and Z.ndim == 2, \"Input should be a 2D numpy array.\"\n",
    "    \n",
    "    #Calculate exponentials(with the offset to avoid overflow)\n",
    "    expZ = np.exp(Z-np.max(Z))\n",
    "    #Divide each by the sum of the rest\n",
    "    Z = np.divide(expZ, np.reshape(np.sum(expZ, axis = 1), (Z.shape[0],1)))\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-0c4593a4b7f9af0a",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert np.allclose(np.round(softmax(np.array([[0, 0, 0, 0]])), decimals=8), np.array([[0.25, 0.25, 0.25, 0.25]])), \"Returns incorrect result for all zero matrix.\"\n",
    "assert np.allclose(np.round(softmax(np.array([[1, 2, 3]])), decimals=8), np.array([[0.09003057, 0.24472847, 0.66524096]])), \"Returns incorrect result for non-zero single row matrix.\"\n",
    "assert np.allclose(np.round(softmax(np.array([[-2, 0, 10]])), decimals=8), np.array([[6.1400000e-06, 4.5400000e-05, 9.9994846e-01]])), \"Returns incorrect result for single row matrix with.\"\n",
    "assert np.allclose(np.round(softmax(np.array([[0, 0], [-3.2, 1.1], [4, 3.4]])), decimals=8),  np.array([[0.5, 0.5],[0.01338692, 0.98661308],[0.64565631, 0.35434369]])), \"Returns incorrect result for non-zero multiple row matrix.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-8a103c355d843bb5",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert np.allclose(np.round(softmax(np.array([[1e6, 1e6, 1e6, 1e6]])), decimals=8), np.array([[0.25, 0.25, 0.25, 0.25]])), \"Your implementation does not handle numerical overflow.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7ae9305e3ea32a70",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Forward propagation\n",
    "\n",
    "The process of propagating the input through the layers of the neural network to get the output is called forward propagation.  In our case, forward propagation consists of the following steps:\n",
    "\n",
    "* The input vector $x$ is fed into the network, multiplied by the weight matrix $W_1$, and the bias terms $b_1$ are added to the result.  The resulting vector $z_1$ is passed through the sigmoid activation function to produce the hidden layer activations $a_1$.\n",
    "* The activations $a_1$ are fed as input to the second layer, multiplied by the weight matrix $W_2$, and the bias terms $b_2$ are added to the result.  The resulting vector $z_2$ is passed through the softmax function to produce the output activations $a_2$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1bceffed8cdda3aa",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Implement the forward propagation function.  You can use [`np.matmul()`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.matmul.html) or `@` for matrix multiplication.  Be sure to check the dimensions of your variables in order to determine the correct order of matrix multiplication. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3df38176993b76ee",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def forward_propagation(W1, W2, b1, b2, x):\n",
    "    '''\n",
    "    Propagates the input vector x through the neural network.  Returns a tuple of \n",
    "    vectors (a1, a2) containing the activations for the hidden and output layers.\n",
    "    '''\n",
    "    assert x.shape == (1, n0), \"Input does not have correct dimensions.\"\n",
    "    \n",
    "    # Pass through each layer and activation\n",
    "\n",
    "    a1 = sigmoid(x@W1 + b1)\n",
    "    a2 = softmax(a1@W2 + b2)\n",
    "\n",
    "    return (a1, a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b65765e64f2915d6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "x = np.array([3, 4, 1, 2]).reshape((1, n0)) # reshape from size (n0, ) to size (1, n0)\n",
    "(a1, a2) = forward_propagation(W1, W2, b1, b2, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-8d260be3d1c66bca",
     "locked": true,
     "points": 4,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert a1.shape[0] == 1, \"The number of rows in a1 is incorrect.\"\n",
    "assert a1.shape[1] == n1, \"The number of columns in a1 is incorrect.\"\n",
    "\n",
    "assert a2.shape[0] == 1, \"The number of rows in a2 is incorrect.\"\n",
    "assert a2.shape[1] == n2, \"The number of columns in a2 is incorrect.\"\n",
    "assert np.allclose(np.round(a1, decimals=8), np.array([[0.51608902, 0.50712488, 0.51146888, 0.52905026, 0.52204596, 0.49837435, 0.50102402]])), \"Incorrect values in a1.\"\n",
    "assert np.allclose(np.round(a2, decimals=8), np.array([[0.33920574, 0.33361887, 0.32717539]])), \"Incorrect values in a2.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-8056f9b2b5c1e2ce",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell has hidden tests, do not delete or edit it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-entropy loss\n",
    "\n",
    "The cross-entropy loss for the $i^{th}$ sample is given by\n",
    "\n",
    "$$ J^{(i)} = - \\sum_{j=1}^k y^{(i)}_j \\log(p^{(i)}_j)$$\n",
    "\n",
    "The average cross-entropy loss across all samples is given by\n",
    "\n",
    "$$ J = - \\frac{1}{m} \\sum_{i=1}^m \\sum_{j=1}^k y^{(i)}_j \\log(p^{(i)}_j)$$\n",
    "\n",
    "where $y^{(i)}$ is the one-hot encoded label for the $i^{th}$ data point and $p^{(i)}_j$ is the probability of the $j^{th}$ class for the $i^{th}$ data point.  Note that the inner summation will only contain a single non-zero term.\n",
    "\n",
    "Implement the cross-entropy loss function.  Since we will use this function in both part 1 and part 2, this implementation should be vectorized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-749b6215a15f3c0f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def cross_entropy_loss(A, Y):\n",
    "    '''\n",
    "    Calculates the cross-entropy loss for a set of output probabilities and target labels.\n",
    "    \n",
    "    A is a matrix where each row specifies the output layer activations for a single input.\n",
    "    Y is a matrix where each row specifies the one-hot encoded target label for a single input.\n",
    "    The computed cross entropy loss should be a scalar.    \n",
    "    '''\n",
    "    \n",
    "    # Return CE loss\n",
    "    return (np.sum(np.multiply(np.log(A),Y)))/(-A.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ec0a15927370755f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "A = np.array([[.25, .20, .05, .5], [.7, .1, .1, .1]])\n",
    "Y = np.array([[0, 1, 0, 0], [1, 0, 0, 0]])\n",
    "J = cross_entropy_loss(A, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-1b4e932372754d4f",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert np.isscalar(J), \"Output is not a scalar.\"\n",
    "assert np.round(J, decimals=8) == 0.98305643, \"Returns incorrect loss value.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-337219485a399076",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Initialize gradients for weights and bias terms\n",
    "\n",
    "Initialize the gradients with zeros.  The shape of the gradients should be the same as the shape of the corresponding weight and bias terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f4d3109f25579939",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def initialize_gradients(n0, n1, n2):\n",
    "    '''\n",
    "    Initializes the gradients with zeroes and returns a tuple of matrices (dW1, dW2, db1, db2)\n",
    "    corresponding to the gradients of W1, W2, b1, and b2, respectively.\n",
    "    '''\n",
    "    \n",
    "    # Create zeros matricies matching shapes of W1, W2, b1, b2\n",
    "    dW1 = np.zeros((n0,n1))\n",
    "    dW2 = np.zeros((n1,n2))\n",
    "    db1 = np.zeros((1,n1))\n",
    "    db2 = np.zeros((1,n2))\n",
    "    \n",
    "    \n",
    "    return (dW1, dW2, db1, db2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-75af41da31d6fcb5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "(dW1, dW2, db1, db2) = initialize_gradients(n0, n1, n2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-b24cebb300db0bcc",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(dW1, np.ndarray), \"dW1 should be a numpy array.\"\n",
    "assert isinstance(dW2, np.ndarray), \"dW2 should be a numpy array.\"\n",
    "\n",
    "assert dW1.shape[0] == n0, \"The number of rows in dW1 is incorrect.\"\n",
    "assert dW1.shape[1] == n1, \"The number of columns in dW1 is incorrect.\"\n",
    "\n",
    "assert dW2.shape[0] == n1, \"The number of rows in dW2 is incorrect.\"\n",
    "assert dW2.shape[1] == n2, \"The number of columns in dW2 is incorrect.\"\n",
    "\n",
    "assert np.sum(np.abs(dW1)) == 0, \"All the elements in dW1 should be initialized to zero.\"\n",
    "assert np.sum(np.abs(dW2)) == 0, \"All the elements in dW2 should be initialized to zero.\"\n",
    "\n",
    "assert isinstance(db1, np.ndarray), \"db1 should be a numpy array.\"\n",
    "assert isinstance(db2, np.ndarray), \"db2 should be a numpy array.\"\n",
    "\n",
    "assert db1.shape[0] == 1, \"The number of rows in db1 is incorrect.\"\n",
    "assert db1.shape[1] == n1, \"The number of columns in db1 is incorrect.\"\n",
    "\n",
    "assert db2.shape[0] == 1, \"The number of rows in db2 is incorrect.\"\n",
    "assert db2.shape[1] == n2, \"The number of columns in db2 is incorrect.\"\n",
    "\n",
    "assert np.sum(np.abs(db1)) == 0, \"All the elements in db1 should be initialized to zero.\"\n",
    "assert np.sum(np.abs(db2)) == 0, \"All the elements in db2 should be initialized to zero.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e693c5b5445a71bc",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Backward propagation\n",
    "\n",
    "Backward propagation refers to the process of taking the error of the output prediction and propagating it backwards through the neural network in order to calculate the gradients of the network weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e544397f7c3728a5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "***Exercise:***\n",
    "\n",
    "Derive the equations to calculate $\\frac{dJ}{da_2}$, $\\frac{dJ}{dz_2}$, $\\frac{dJ}{db_2}$, $\\frac{dJ}{dW_2}$, $\\frac{dJ}{da_1}$, $\\frac{dJ}{dz_1}$, $\\frac{dJ}{db_1}$, and $\\frac{dJ}{dW_1}$ for a single sample.  You may re-use any results derived in class without re-proving it.  Please express your equations in matrix form, denoting matrix multiplication with $A*B$ and elementwise multiplication as $A.*B$.\n",
    "\n",
    "Steps: \n",
    "1. Calculate $\\frac{dJ}{da_2}$, the gradient of the error with respect to the output probabilities $a_2$.  In this case, $J$ is the cross-entropy loss for a single sample.  Express your result in vectorized form in terms of $a_2$ and $y$, the one hot encoded target labels.\n",
    "\n",
    "2. Now calculate $\\frac{dJ}{dz_2}$ with the chain rule.  To simplify the notations for this derivation, denote $a_2$ by $a$ and $z_2$ by $z$ and use the subscripts for indexing the elements of the vector (otherwise the notation will get very confusing).  Once you have derived a final result, switch back to the original notation and express your answer in vectorized form in terms of $a_2$, $z_2$, and $y$.  In this simplified notation, the gradient for the $i^{th}$ element is: $$\\frac{dJ}{dz_i} = \\sum_{i=1}^{k} \\frac{dJ}{da_j} \\frac{da_j}{dz_i}$$  You may simply re-use the results we derived in class, though re-deriving it from scratch yourself is one of the best ways to build mastery!\n",
    "\n",
    "3.  Express $z_2$ in terms of $W_2$ and $b_2$ and calculate $\\frac{dz_2}{dW_2}$ and $\\frac{dz_2}{db_2}$.  Calculate $\\frac{dJ}{dW_2}$ and $\\frac{dJ}{db_2}$ using the chain rule:\n",
    "\n",
    "$$\\frac{dJ}{dW_2} = \\frac{dJ}{dz_2} \\frac{dz_2}{dW_2}$$\n",
    "\n",
    "$$\\frac{dJ}{db_2} = \\frac{dJ}{dz_2} \\frac{dz_2}{db_2}$$\n",
    "\n",
    "4. Express $a_1$ in terms of $z_2$ and calculate $\\frac{dJ}{da_1}$.  Repeat steps 1 through 3 to calculate the derivative of $J$ w.r.t $z_1, W_1, b_1$.\n",
    "\n",
    "Note that the gradient of $J$ w.r.t. a matrix variable should have the same dimensions as the matrix variable.  One way to verify your derivation is to make sure all matrix operations have compatible dimensions.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-3a740c37e6a74cf1",
     "locked": false,
     "points": 20,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "***Put your equations below***\n",
    "#### For a sample k\n",
    "\n",
    "dJ/da2:  \n",
    "$\\frac{dJ}{da_2} = [\\frac{-y_1}{\\hat{y_1}},\\frac{-y_2}{\\hat{y_2}},...\\frac{-y_{s2}}{\\hat{y_{s2}}}]$  (from class)\n",
    "\n",
    "\n",
    "$\\frac{dJ}{da_2} = \\bar{y} .* \\bar{a_2}^{ -1}$ \n",
    "\n",
    "dJ/dz2(from class):\n",
    "\n",
    "$\\frac{dJ}{dz_2} = \\bar{-y} + \\bar{a_2}$ \n",
    "\n",
    "dJ/db2(from class):\n",
    "\n",
    "$\\frac{dJ}{db_2} = \\frac{dJ}{dz_2}$ \n",
    "\n",
    "dJ/dW2:\n",
    "\n",
    "$\\frac{dJ}{dW_2} = \\bar{a}_{1}^T*\\frac{dJ}{dz_2}$  (from class)\n",
    "\n",
    "$\\frac{dJ}{dW_2} = \\bar{a}_{1}^T*\\bar{-y} + \\bar{a}_{1}^T*\\bar{a_2}$\n",
    "\n",
    "dJ/da1:\n",
    "\n",
    "$\\frac{dJ}{da_1} = \\frac{dJ}{dz_2} * W_2^T $ (from class)\n",
    "\n",
    "$\\frac{dJ}{da_1} = \\bar{-y}* W_2^T + \\bar{a_2}* W_2^T$\n",
    "\n",
    "dJ/dz1:\n",
    "\n",
    "$\\frac{dJ}{dz_1} = \\frac{dJ}{da_1} .*(1-\\bar{a}_1).*\\bar{a}_1$ (from class)\n",
    "\n",
    "$\\frac{dJ}{dz_1} = (\\bar{-y}* W_2^T + \\bar{a_2}* W_2^T) .*(1-\\bar{a}_1).*\\bar{a}_1$\n",
    "\n",
    "dJ/db1(from class):\n",
    "\n",
    "$\\frac{dJ}{db_1} = \\frac{dJ}{dz_1}$ (from class)\n",
    "\n",
    "$\\frac{dJ}{db_1} = (\\bar{-y}* W_2^T + \\bar{a_2}* W_2^T) .*(1-\\bar{a}_1).*\\bar{a}_1$\n",
    "\n",
    "dJ/dW1(from class):\n",
    "\n",
    "$\\frac{dJ}{dW_1} = \\bar{x}^T * \\frac{dJ}{dz_1}$ (from class)\n",
    "\n",
    "$\\frac{dJ}{dW_1} = \\bar{x}^T * ((\\bar{-y}* W_2^T + \\bar{a_2}* W_2^T) .*(1-\\bar{a}_1).*\\bar{a}_1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-92d2558c67c5b193",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "We will use the following notation in our backpropagation function:\n",
    "\n",
    "`dW1`: $\\frac{dJ}{dW_1}$\n",
    "\n",
    "`dW2 `: $\\frac{dJ}{dW_2}$\n",
    "\n",
    "`db1` : $\\frac{dJ}{db_1}$\n",
    "\n",
    "`db2` : $\\frac{dJ}{db_2}$\n",
    "\n",
    "`dz1` : $\\frac{dJ}{dz_1}$\n",
    "\n",
    "`dz2` : $\\frac{dJ}{dz_2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-91886a9522c98896",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Hints: \n",
    "- Be careful to distinguish between matrix multiplication and elementwise multiplication of matrices.  For matrix multiplication, you can use [`np.matmul()`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.matmul.html) or `@`.  For elementwise multiplication, you can use [`np.multiply()`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.multiply.html) or `*`.\n",
    "- Pay close attention to the shape of matrices and vectors to determine the correct order of matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-eb4f12428289fdc5",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def back_propagation(W1, W2, b1, b2, a1, a2, x, y):\n",
    "    '''\n",
    "    Calculates the gradients of the network weights for a single sample.\n",
    "    Returns a tuple of matrices (dW1, dW2, db1, db2).\n",
    "    '''\n",
    "    assert x.shape == (1, n0)\n",
    "    \n",
    "    #If the input data is not the right shape (1,n2), reshape it\n",
    "    if len(y.shape) == 1:\n",
    "        y = y.reshape((1, y.shape[0]))\n",
    "    #Move through each of the derivatives from above\n",
    "    da2 = y *(1/a2)\n",
    "    dz2 = -y  + a2\n",
    "    db2 = dz2\n",
    "    dW2 = a1.T@-y +a1.T@a2\n",
    "    da1 = -y@W2.T + a2@W2.T\n",
    "    dz1 = da1 *(1-a1)*a1\n",
    "    db1 = dz1\n",
    "    dW1 = x.T@dz1\n",
    "\n",
    "    return (dW1, dW2, db1, db2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-eef6ef875cee6d3b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "x = np.array([[3, 4, 1, -1]])\n",
    "y = np.array([1, 0, 0])\n",
    "dW1, dW2, db1, db2 = back_propagation(W1, W2, b1, b2, a1, a2, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-669871e75c09463",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(db1, np.ndarray), \"db1 should be a numpy array.\"\n",
    "assert isinstance(db2, np.ndarray), \"db2 should be a numpy array.\"\n",
    "\n",
    "assert db2.shape[0] == 1, \"The number of rows in db2 is incorrect.\"\n",
    "assert db2.shape[1] == n2, \"The number of columns in db2 is incorrect.\"\n",
    "\n",
    "assert db1.shape[0] == 1, \"The number of rows in db1 is incorrect.\"\n",
    "assert db1.shape[1] == n1, \"The number of columns in db1 is incorrect.\"\n",
    "\n",
    "assert isinstance(dW1, np.ndarray), \"dW1 should be a numpy array.\"\n",
    "assert isinstance(dW2, np.ndarray), \"dW2 should be a numpy array.\"\n",
    "\n",
    "assert dW2.shape[0] == n1, \"The number of rows in dW2 is incorrect.\"\n",
    "assert dW2.shape[1] == n2, \"The number of columns in dW2 is incorrect.\"\n",
    "\n",
    "assert dW1.shape[0] == n0, \"The number of rows in dW1 is incorrect.\"\n",
    "assert dW1.shape[1] == n1, \"The number of columns in dW1 is incorrect.\"\n",
    "\n",
    "assert np.allclose(np.round(dW1[:,0], decimals=8), np.array([-0.00353597, -0.00471462, -0.00117866,  0.00117866])), \"Incorrect values for dW1.\"\n",
    "assert np.allclose(np.round(db1[0,0:4], decimals=8), np.array([-0.00117866, -0.00298474,  0.00171055, -0.002548  ])), \"Incorrect values for db1.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-9c21fe5f7c29bb87",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell has hidden tests, do not delete or edit it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-065a748cac173fdc",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Updating network weights\n",
    "\n",
    "The network weights are updated as follows:\n",
    "\n",
    "$W_1 = W_1 - \\alpha * \\frac{1}{m} \\sum_{i=1}^{m} dW^{(i)}_1$  \n",
    "\n",
    "$W_2 = W_2 - \\alpha * \\frac{1}{m} \\sum_{i=1}^{m} dW^{(i)}_2$  \n",
    "\n",
    "$b_1 = b_1 - \\alpha * \\frac{1}{m} \\sum_{i=1}^{m} db^{(i)}_1$  \n",
    "\n",
    "$b_2 = b_2 - \\alpha * \\frac{1}{m} \\sum_{i=1}^{m} db^{(i)}_2$\n",
    "\n",
    "where $\\alpha$ is the learning rate, $m$ is the number of training examples, and $dW^{(i)}_1$, $dW^{(i)}_2$, $db^{(i)}_1$, and $db^{(i)}_2$ are the gradients for the $i^{th}$ training sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f5cf67f3f7bcd0fc",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def update_parameters(W1, W2, b1, b2, dW1, dW2, db1, db2, alpha):\n",
    "    '''\n",
    "    Returns the weights after a single gradient descent update.\n",
    "    dW1, dW2, db1, and db2 are the gradients averaged across all training samples.\n",
    "    W1, W2, b1, and b2 are the network weights, which should be updated and returned.\n",
    "    '''\n",
    "    #Gradient descent on all the network weights\n",
    "    W1 = W1-alpha*dW1\n",
    "    W2 = W2-alpha*dW2\n",
    "    b1 = b1-alpha*db1\n",
    "    b2 = b2-alpha*db2\n",
    "    \n",
    "    return (W1, W2, b1, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0134d8ebccc1ed19",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "alpha = .1\n",
    "W1_prev, W2_prev, b1_prev, b2_prev = W1.copy(), W2.copy(), b1.copy(), b2.copy()\n",
    "W1, W2, b1, b2 = update_parameters(W1, W2, b1, b2, dW1, dW2, db1, db2, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-669871e75c094763",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(b1, np.ndarray), \"b1 should be a numpy array.\"\n",
    "assert isinstance(b2, np.ndarray), \"b2 should be a numpy array.\"\n",
    "\n",
    "assert b1.shape[0] == 1, \"The number of rows in b1 is incorrect.\"\n",
    "assert b1.shape[1] == n1, \"The number of columns in b1 is incorrect.\"\n",
    "\n",
    "assert b2.shape[0] == 1, \"The number of rows in b2 is incorrect.\"\n",
    "assert b2.shape[1] == n2, \"The number of columns in b2 is incorrect.\"\n",
    "\n",
    "assert isinstance(W1, np.ndarray), \"W1 should be a numpy array.\"\n",
    "assert isinstance(W2, np.ndarray), \"W2 should be a numpy array.\"\n",
    "\n",
    "assert W1.shape[0] == n0, \"The number of rows in W1 is incorrect.\"\n",
    "assert W1.shape[1] == n1, \"The number of columns in W1 is incorrect.\"\n",
    "\n",
    "assert W2.shape[0] == n1, \"The number of rows in W2 is incorrect.\"\n",
    "assert W2.shape[1] == n2, \"The number of columns in W2 is incorrect.\"\n",
    "\n",
    "assert np.allclose(np.round((W1 - W1_prev)[:,0], decimals=8), np.array([ 0.0003536 ,  0.00047146,  0.00011787, -0.00011787])), \"Incorrect update to W1.\"\n",
    "assert np.allclose(np.round((b1 - b1_prev)[0,0:4], decimals=8), np.array([ 0.00011787,  0.00029847, -0.00017105,  0.0002548 ])), \"Incorrect updatae to b1.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-ddecace1ceec426b",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell has hidden tests, do not delete or edit it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-aa3bc08c52b19d23",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Training\n",
    "\n",
    "In this function we will bring all the functions together to iteratively update the weights `W1`, `b1`, `W2`, and `b2` on a set of training examples.\n",
    "\n",
    "* The outer for loop iterates over the dataset `num_epochs` times, updating the weights once per epoch.  Each update should be based on the gradients averaged across all training samples in a single epoch.\n",
    "* The inner for loop iterates over the training samples in the dataset, accumulating gradients from each data point.  To calculate the gradient on each data point, you will need to do forward and backward propagation.\n",
    "* We will also want to calculate the average cross entropy loss across the training samples, so that we can monitor the training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1002d9b35d78cdce",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def training(X, Y, num_hidden, alpha, num_epochs):\n",
    "    '''\n",
    "    Learn the network weights on a set of training examples.  This implementation is not vectorized.\n",
    "    \n",
    "    Inputs\n",
    "    ------\n",
    "    X: a matrix containing the training data, where each row corresponds to a single training example\n",
    "    Y: a matrix containing the target labels, where each row contains a one hot encoded target label.\n",
    "    num_hidden: specifies the number of hidden units in the neural network.\n",
    "    alpha: the learning rate for gradient descent\n",
    "    num_epochs: the number of times to iterate over the dataset\n",
    "    \n",
    "    Returns a tuple of matrices (W1, W2, b1, b2) containing the learned network weights.\n",
    "    '''\n",
    "    \n",
    "    n0 = X.shape[1]\n",
    "    n1 = num_hidden\n",
    "    n2 = Y.shape[1]\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    W1, W2 = initialize_weights(n0, n1, n2)\n",
    "    b1, b2 = initialize_bias(n0, n1, n2)\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        loss = 0\n",
    "        dW1, dW2, db1, db2 = initialize_gradients(n0, n1, n2)\n",
    "        \n",
    "        for i in range(m):\n",
    "            \n",
    "            x = X[i,:].reshape((1,-1))\n",
    "            y = Y[i,:].reshape((1,-1))\n",
    "            #Find the output with the weights on sample x\n",
    "            a1, a2 = forward_propagation(W1, W2, b1, b2, x)\n",
    "            #Find the gradients with sample x\n",
    "            tempdW1, tempdW2, tempdb1, tempdb2 = back_propagation(W1, W2, b1, b2, a1, a2, x, y)\n",
    "            #Add the gradient with sample x to a running total\n",
    "            dW1 += tempdW1\n",
    "            dW2 += tempdW2\n",
    "            db1 += tempdb1\n",
    "            db2 += tempdb2\n",
    "            #Add the loss of the output for sample x to a running total\n",
    "            loss += cross_entropy_loss(a2, y)\n",
    "        \n",
    "        #Divide all the running totals by the number of samples\n",
    "        dW1 = dW1 / m\n",
    "        dW2 = dW2 / m\n",
    "        db1 = db1 / m\n",
    "        db2 = db2 / m\n",
    "        loss = loss / m\n",
    "        #Update with the gradients\n",
    "        W1, W2, b1, b2 = update_parameters(W1, W2, b1, b2, dW1, dW2, db1, db2, alpha)\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            print('Epoch {}: Training Loss = {:.8f}'.format(epoch, loss))\n",
    "        \n",
    "    return W1, W2, b1, b2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e4a3aa07781d2935",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "To test our training function, we can generate a small dataset suitable for a classification task using [`make_classification`](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html) in [scikit-learn](https://scikit-learn.org/stable/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "m = 100\n",
    "X, Y = make_classification(n_samples=m, n_features=n0, n_redundant=0, n_informative=n0, n_classes=n2, random_state=1)\n",
    "plt.scatter(X[:, 0], X[:, 1], marker='o', c=Y, s=25, edgecolor='k');\n",
    "Y = np.eye(n2)[Y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-01cb6166d1d9cf72",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Training Loss = 1.09854246\n",
      "Epoch 100: Training Loss = 0.66769965\n",
      "Epoch 200: Training Loss = 0.62075226\n",
      "Epoch 300: Training Loss = 0.53313843\n",
      "Epoch 400: Training Loss = 0.43255480\n",
      "Epoch 500: Training Loss = 0.37279295\n",
      "Epoch 600: Training Loss = 0.33923461\n",
      "Epoch 700: Training Loss = 0.31482535\n",
      "Epoch 800: Training Loss = 0.29518944\n",
      "Epoch 900: Training Loss = 0.27612031\n",
      "Epoch 1000: Training Loss = 0.25750832\n",
      "Epoch 1100: Training Loss = 0.24231282\n",
      "Epoch 1200: Training Loss = 0.22891277\n",
      "Epoch 1300: Training Loss = 0.21669590\n",
      "Epoch 1400: Training Loss = 0.20563874\n",
      "Epoch 1500: Training Loss = 0.19562613\n",
      "Epoch 1600: Training Loss = 0.18646676\n",
      "Epoch 1700: Training Loss = 0.17798751\n",
      "Epoch 1800: Training Loss = 0.16999538\n",
      "Epoch 1900: Training Loss = 0.16210703\n",
      "Epoch 2000: Training Loss = 0.15379149\n",
      "Epoch 2100: Training Loss = 0.14540899\n",
      "Epoch 2200: Training Loss = 0.13679592\n",
      "Epoch 2300: Training Loss = 0.12806248\n",
      "Epoch 2400: Training Loss = 0.11971815\n",
      "Epoch 2500: Training Loss = 0.11181989\n",
      "Epoch 2600: Training Loss = 0.10432723\n",
      "Epoch 2700: Training Loss = 0.09727166\n",
      "Epoch 2800: Training Loss = 0.09075262\n",
      "Epoch 2900: Training Loss = 0.08483663\n"
     ]
    }
   ],
   "source": [
    "W1, W2, b1, b2 = training(X, Y, n1, 1, 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-e73ff98fe76d271b",
     "locked": true,
     "points": 4,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(b1, np.ndarray), \"b1 should be a numpy array.\"\n",
    "assert isinstance(b2, np.ndarray), \"b2 should be a numpy array.\"\n",
    "\n",
    "assert b1.shape[0] == 1, \"The number of rows in b1 is incorrect.\"\n",
    "assert b1.shape[1] == n1, \"The number of columns in b1 is incorrect.\"\n",
    "\n",
    "assert b2.shape[0] == 1, \"The number of rows in b2 is incorrect.\"\n",
    "assert b2.shape[1] == n2, \"The number of columns in b2 is incorrect.\"\n",
    "\n",
    "assert isinstance(W1, np.ndarray), \"W1 should be a numpy array.\"\n",
    "assert isinstance(W2, np.ndarray), \"W2 should be a numpy array.\"\n",
    "\n",
    "assert W1.shape[0] == n0, \"The number of rows in W1 is incorrect.\"\n",
    "assert W1.shape[1] == n1, \"The number of columns in W1 is incorrect.\"\n",
    "\n",
    "assert W2.shape[0] == n1, \"The number of rows in W2 is incorrect.\"\n",
    "assert W2.shape[1] == n2, \"The number of columns in W2 is incorrect.\"\n",
    "\n",
    "assert np.allclose(np.round(W1[0,:], decimals=8), np.array([1.26422707, -0.70762336, 7.20233002, -2.53263138, 3.40793382, -5.59849532, 3.61526723])), \"Trained weights W1 are incorrect.\"\n",
    "assert np.allclose(np.round(b1, decimals = 8), np.array([6.93259983, 6.38592697, -1.368562, 3.32583488, 4.44835927, 2.09694021, -1.09501032])), \"Trained weights b1 are incorrect.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b9705fa0b5d5ff07",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (764279130.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_22274/764279130.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    For grading purpose: weight accuracy\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "For grading purpose: weight accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ca398026a3c2ec38",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "##### Prediction\n",
    "\n",
    "Write a function to predict the label and class probabilities for a single input using the trained neural network.  You can use the function [`np.argmax`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.argmax.html) to find the largest element in an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-20971a390d8ed68a",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def predict(W1, W2, b1, b2, x):\n",
    "    '''\n",
    "    Returns the predicted label y and class probabilities p for a single input.\n",
    "    The predicted label should be an integer, and the class probabilities should be a vector.\n",
    "    '''\n",
    "    assert isinstance(x, np.ndarray) and x.shape == (1, n0), \"Input should be a numpy array of shape (1, n0).\"\n",
    "    \n",
    "    #Find the outputs with our weights. These are our predictions\n",
    "    a1, p = forward_propagation(W1,W2,b1,b2,x)\n",
    "    #The maximum is the index of our predicted class\n",
    "    y = np.argmax(p)\n",
    "    \n",
    "    return y, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-beecf6d027507509",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "x = np.array([[.9, 2.1, 0.1, -.1]])\n",
    "y, p = predict(W1, W2, b1, b2, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-61b6afcf6bd5d13e",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert not isinstance(y, np.ndarray), \"y should be an integer, not a numpy array.\"\n",
    "assert isinstance(p, np.ndarray), \"p should be a numpy array.\"\n",
    "\n",
    "assert y == 1, \"Predicted label is incorrect.\"\n",
    "assert np.allclose(np.round(p, decimals=8), np.array([1.0800000e-06, 9.7427047e-01, 2.5728450e-02])), \"Class probabilities are incorrect.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-bf6fe98ccc32c7d8",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell has hidden tests, do not delete or edit it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations, you just implemented your first neural network!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-964b64a3161c7b1b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Part 2\n",
    " \n",
    "In the second part of this assignment, you will re-implement several functions using vectorization.  The main reason for vectorizing is to speed up the training process by performing operations in parallel rather than sequentially.  Rather than processing each training sample individually, you will stack all of the training examples into a matrix and rewrite the following functions to process all training samples simultaneously:\n",
    "\n",
    "* forward propagation    \n",
    "* backward propagation      \n",
    "* training the network\n",
    "* making predictions on new inputs\n",
    "\n",
    "You will also write a function for gradient checking to verify that your backpropagation implementation is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define our notation: \n",
    "\n",
    "* The number of nodes in the input, hidden, and output layers are $n_0$, $n_1$, and $n_2$ respectively.\n",
    "\n",
    "* The number of training samples in the data set is $m$.\n",
    "\n",
    "* The input $X$ is a matrix of shape $(m, n_0)$ where each row corresponds to a training example.\n",
    "\n",
    "* The output $Y$ is a matrix of shape $(m, n_2)$ where each row corresponds to a one hot encoded target label.\n",
    "\n",
    "* The weights connecting the input and hidden layer are given by the matrix $W_1$ of shape $(n_0, n_1)$.\n",
    "\n",
    "* The weights connecting the hidden and output layer are given by the matrix $W_2$ of shape $(n_1, n_2)$.\n",
    "\n",
    "* The bias terms feeding into the hidden layer are given by the vector $b_1$ of shape $(1, n_1)$.\n",
    "\n",
    "* The bias terms feeding into the output layer are given by the vector $b_2$ of shape $(1, n_2)$.\n",
    "\n",
    "* The sigmoid activations for the hidden layer are given by the matrix $A_1$ of shape $(m, n_1)$.\n",
    "\n",
    "* The softmax outputs at the output layer are given by the matrix $A_2$ of shape $(m, n_2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d38530a1b9120d37",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "##### Forward propagation with vectorization\n",
    "\n",
    "Re-implement the forward propagation function to handle all training samples simultaneously.  Make sure to take advantage of broadcasting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5d2b143c49cfe1f8",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def forward_propagation_using_vectorization(W1, W2, b1, b2, X):\n",
    "    '''\n",
    "    Performs forward propagation on a set of data samples.\n",
    "    X is a matrix of input data, where each row corresponds to a single data sample.\n",
    "    \n",
    "    Returns a tuple of matrices (A1, A2) containing the activations at the hidden and output layers\n",
    "    for all training examples in X.\n",
    "    '''\n",
    "    #Matrix multiply and sigmoid activation\n",
    "    A1 = sigmoid(np.matmul(X,W1) + b1)\n",
    "    #Matrix multiply and softmax activation\n",
    "    A2 = softmax(np.matmul(A1,W2) + b2)\n",
    "    \n",
    "    return (A1, A2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-286bc4d6d399d25d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "(A1, A2) = forward_propagation_using_vectorization(W1, W2, b1, b2, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-207d184f712491b0",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "m = X.shape[0]\n",
    "\n",
    "assert A1.shape[0] == m, \"The number of rows in A1 is incorrect.\"\n",
    "assert A1.shape[1] == n1, \"The number of columns in A1 is incorrect.\"\n",
    "\n",
    "assert A2.shape[0] == m, \"The number of rows in A2 is incorrect.\"\n",
    "assert A2.shape[1] == n2, \"The number of columns in A2 is incorrect.\"\n",
    "\n",
    "assert np.allclose(np.round(A1[50,0:4], decimals=8), np.array([9.9999983e-01, 9.9999989e-01, 1.0000000e+00, 1.8640000e-05])), \"Incorrect values in A1.\"\n",
    "assert np.allclose(np.round(A2[80,:], decimals=8), np.array([0.97858353, 0., 0.02141646])), \"Incorrect values in A2.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-e71fc1861eae18cb",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell has hidden tests, do not delete or edit it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8100d0944daf08bd",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "##### Backward propagation using vectorization.\n",
    "\n",
    "Re-implement the backward propagation function to handle all training samples simultaneously. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6e63a3081ade3038",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def back_propagation_using_vectorization(W1, W2, b1, b2, A1, A2, X, Y):\n",
    "    '''\n",
    "    Performs backpropagation on a set of training samples using vectorization.\n",
    "    X is a matrix of input data, where each row corresponds to a single data sample.\n",
    "    Y is a matrix of target labels, where each row corresponds to a one hot encoded label.\n",
    "    \n",
    "    Returns a tuple of matrices (dW1, dW2, db1, db2) containing the average gradients across all training examples.\n",
    "    '''\n",
    "    m = X.shape[0]\n",
    "\n",
    "    # Save the shape of the biases for later\n",
    "    db1Shape = b1.shape\n",
    "    db2Shape = b2.shape\n",
    "    #Run through the gradients from part 1)\n",
    "    #This time, being careful with matrix dimensions\n",
    "    da2 = Y*np.reciprocal(A2)\n",
    "    dz2 = -Y  + A2\n",
    "    db2 = dz2\n",
    "    dW2 = np.matmul(A1.T,-Y) + np.matmul(A1.T, A2)\n",
    "    da1 = np.matmul(-Y,W2.T) + np.matmul(A2, W2.T)\n",
    "    dz1 = da1 *(1-A1)*A1\n",
    "    db1 = dz1\n",
    "    #Shape the biases into the shape of b1 and b2. Make sure to normalize sums\n",
    "    db1 = np.reshape(np.sum(db1, axis=0)/m, db1Shape)\n",
    "    db2 = np.reshape(np.sum(db2, axis=0)/m, db2Shape)\n",
    "    dW1 = np.matmul(X.T,dz1)\n",
    "    \n",
    "    return (dW1/m, dW2/m, db1, db2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-11a32da3b4ff94ca",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "dW1, dW2, db1, db2 = back_propagation_using_vectorization(W1, W2, b1, b2, A1, A2, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-800d0fae4f1c4077",
     "locked": true,
     "points": 6,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(dW1, np.ndarray), \"dW1 should be a numpy array.\"\n",
    "assert isinstance(dW2, np.ndarray), \"dW2 should be a numpy array.\"\n",
    "\n",
    "assert dW2.shape[0] == n1, \"The number of rows in dW2 is incorrect.\"\n",
    "assert dW2.shape[1] == n2, \"The number of columns in dW2 is incorrect.\"\n",
    "\n",
    "assert dW1.shape[0] == n0, \"The number of rows in dW1 is incorrect.\"\n",
    "assert dW1.shape[1] == n1, \"The number of columns in dW1 is incorrect.\"\n",
    "\n",
    "assert isinstance(db1, np.ndarray), \"db1 should be a numpy array.\"\n",
    "assert isinstance(db2, np.ndarray), \"db2 should be a numpy array.\"\n",
    "\n",
    "assert db2.shape[0] == 1, \"The number of rows in db2 is incorrect.\"\n",
    "assert db2.shape[1] == n2, \"The number of columns in db2 is incorrect.\"\n",
    "\n",
    "assert db1.shape[0] == 1, \"The number of rows in db1 is incorrect.\"\n",
    "assert db1.shape[1] == n1, \"The number of columns in db1 is incorrect.\"\n",
    "\n",
    "assert np.allclose(np.round(dW1[:,5], decimals=8), np.array([ 0.00140916, -0.00019847, -0.00077643, -0.00097965])), \"Incorrect values in dW1.\"\n",
    "assert np.allclose(np.round(db1[0,0:4], decimals=8), np.array([-0.00079979, -0.00057636, -0.00034883, -0.00045457])), \"Incorrect values in db1.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-f484c7afa1051b81",
     "locked": true,
     "points": 6,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell has hidden tests, do not delete or edit it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Checking\n",
    "\n",
    "You will implement a gradient checking function to verify that your backpropagation implementation is correct.  The gradient checking consists of the following steps:\n",
    "- Set your network weights to arbitrary default values.\n",
    "- Use your forward and backward propagation functions to calculate the gradients of the network weights.  These are the values that we would like to verify.\n",
    "- For each network weight, compute the approximate gradient by doing the following:\n",
    "    - Add a small epsilon to the weight/bias element of interest to create a modified set of network weights.\n",
    "    - Perform forward propagation and calculate the loss for the modified network weights ($J_+$).\n",
    "    - Subtract a small epsilon from the weight/bias element of interest to create another modified set of network weights.\n",
    "    - Perform forward propagation and calculate the loss of these modified network weights ($J_-$).\n",
    "    - Compute the approximate gradient for the element of interest as $\\frac{J_+ - J_-}{2 * \\epsilon}$.\n",
    "- Once you have computed an approximate gradient for each network weight, compare your approximate gradient to your backpropagated gradient to verify that they approximately match.\n",
    "\n",
    "To simplify your implementation, we will provide utility functions for flattening all network weights into a vector and extracting the network weights from a flattened vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-03f94f0db38a2648",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def flattened(W1, W2, b1, b2):\n",
    "    V = np.concatenate((W1.flatten(), W2.flatten(), b1.flatten(), b2.flatten()))\n",
    "    return V\n",
    "\n",
    "def reshaped(V, n0, n1, n2):\n",
    "    t1 = n0*n1\n",
    "    t2 = t1 + n1*n2\n",
    "    t3 = t2 + n1\n",
    "    W1 = V[:t1].reshape(n0, n1)\n",
    "    W2 = V[t1:t2].reshape(n1, n2)\n",
    "    b1 = V[t2:t3].reshape(1, n1)\n",
    "    b2 = V[t3:].reshape(1, n2)\n",
    "    return W1, W2, b1, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8eed26ec413655ba",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def gradient_checking(W1, W2, b1, b2, X, Y, epsilon = 1e-8):\n",
    "    \n",
    "    n0, n1, n2 = W1.shape[0], W2.shape[0], W2.shape[1]\n",
    "    (A1, A2) = forward_propagation_using_vectorization(W1, W2, b1, b2, X)\n",
    "    dW1, dW2, db1, db2 = back_propagation_using_vectorization(W1, W2, b1, b2, A1, A2, X, Y)\n",
    "    V = flattened(W1, W2, b1, b2)\n",
    "    grad = flattened(dW1, dW2, db1, db2)\n",
    "    grad_approx = np.zeros_like(grad)\n",
    "    \n",
    "    for i in range(len(V)):\n",
    "        \n",
    "        V_plus = V.copy()\n",
    "        V_plus[i] += epsilon\n",
    "        V_minus = V.copy()\n",
    "        V_minus[i] -= epsilon\n",
    "        \n",
    "        W1_plus, W2_plus, b1_plus, b2_plus = reshaped(V_plus, n0, n1, n2)\n",
    "        W1_minus, W2_minus, b1_minus, b2_minus = reshaped(V_minus, n0, n1, n2)\n",
    "        #Find predictions for the plus and minus conditions\n",
    "        A1p, A2p = forward_propagation_using_vectorization(W1_plus, W2_plus, b1_plus, b2_plus, X)\n",
    "        A1m, A2m = forward_propagation_using_vectorization(W1_minus, W2_minus, b1_minus, b2_minus, X)\n",
    "        #Find loss for each condition\n",
    "        lossp = cross_entropy_loss(A2p, Y)\n",
    "        lossm = cross_entropy_loss(A2m, Y)\n",
    "        #Find the approximate gradient value with the grad checking equation\n",
    "        a_gradval = (lossp - lossm)/(2*epsilon)\n",
    "        #add it to approximate gradients array\n",
    "        grad_approx[i] = a_gradval\n",
    "        \n",
    "    maxerror = np.max(np.abs(grad - grad_approx))\n",
    "    print(\"Max error in gradient approximation: {}\".format(maxerror))\n",
    "    if maxerror < epsilon:\n",
    "        print('Passes gradient checking')\n",
    "    else:\n",
    "        print('Fails gradient checking')\n",
    "    return maxerror"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-82fa87c2c64e21d3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max error in gradient approximation: 1.5818385791770834e-09\n",
      "Passes gradient checking\n"
     ]
    }
   ],
   "source": [
    "maxerror = gradient_checking(W1, W2, b1, b2, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-9ac25807fda5c343",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert maxerror < 1e-8, \"Gradient checking fails.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3a2b9dc065ea4fb5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "##### Training using vectorization\n",
    "\n",
    "Re-implement the training function with vectorization.  Note that the inner for loop will be eliminated, since the forward and backward propagation functions now process the entire training set simultaneously.  You will still iterate multiple times thru the entire training dataset, so the outer for loop will remain.  Your vectorized implementation should be much, much faster than your earlier implementation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-236b66d1780cbab5",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def training_using_vectorization(X, Y, num_hidden, alpha, num_epochs):\n",
    "    '''\n",
    "    Learn the network weights on a set of training examples.  This implementation is vectorized.\n",
    "    \n",
    "    Inputs\n",
    "    ------\n",
    "    X: a matrix containing the training data, where each row corresponds to a single training example\n",
    "    Y: a matrix containing the target labels, where each row contains a one hot encoded target label.\n",
    "    num_hidden: specifies the number of hidden units in the neural network.\n",
    "    alpha: the learning rate for gradient descent\n",
    "    num_epochs: the number of times to iterate over the dataset\n",
    "    \n",
    "    Returns a tuple of matrices (W1, W2, b1, b2) containing the learned network weights.\n",
    "    '''\n",
    "    \n",
    "    n0 = X.shape[1] \n",
    "    n1 = num_hidden\n",
    "    n2 = Y.shape[1]\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    W1, W2 = initialize_weights(n0, n1, n2)\n",
    "    b1, b2 = initialize_bias(n0, n1, n2)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        #Find the prediction with all weights\n",
    "        A1, A2 = forward_propagation_using_vectorization(W1, W2, b1, b2, X)\n",
    "        #backpropagate for gradients\n",
    "        dW1, dW2, db1, db2 = back_propagation_using_vectorization(W1, W2, b1, b2, A1, A2, X, Y)\n",
    "        #Calculate loss\n",
    "        loss = cross_entropy_loss(A2, Y)\n",
    "        #Update with gradients\n",
    "        W1, W2, b1, b2 = update_parameters(W1, W2, b1, b2, dW1, dW2, db1, db2, alpha)\n",
    "        \n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            print('Epoch {}: Training Loss = {:.8f}'.format(epoch, loss))    \n",
    "    \n",
    "    return W1, W2, b1, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-27d66c80e96932e6",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Training Loss = 1.09854246\n",
      "Epoch 100: Training Loss = 0.66769965\n",
      "Epoch 200: Training Loss = 0.62075226\n",
      "Epoch 300: Training Loss = 0.53313843\n",
      "Epoch 400: Training Loss = 0.43255480\n",
      "Epoch 500: Training Loss = 0.37279295\n",
      "Epoch 600: Training Loss = 0.33923461\n",
      "Epoch 700: Training Loss = 0.31482535\n",
      "Epoch 800: Training Loss = 0.29518944\n",
      "Epoch 900: Training Loss = 0.27612031\n",
      "Epoch 1000: Training Loss = 0.25750832\n",
      "Epoch 1100: Training Loss = 0.24231282\n",
      "Epoch 1200: Training Loss = 0.22891277\n",
      "Epoch 1300: Training Loss = 0.21669590\n",
      "Epoch 1400: Training Loss = 0.20563874\n",
      "Epoch 1500: Training Loss = 0.19562613\n",
      "Epoch 1600: Training Loss = 0.18646676\n",
      "Epoch 1700: Training Loss = 0.17798751\n",
      "Epoch 1800: Training Loss = 0.16999538\n",
      "Epoch 1900: Training Loss = 0.16210703\n",
      "Epoch 2000: Training Loss = 0.15379149\n",
      "Epoch 2100: Training Loss = 0.14540899\n",
      "Epoch 2200: Training Loss = 0.13679592\n",
      "Epoch 2300: Training Loss = 0.12806248\n",
      "Epoch 2400: Training Loss = 0.11971815\n",
      "Epoch 2500: Training Loss = 0.11181989\n",
      "Epoch 2600: Training Loss = 0.10432723\n",
      "Epoch 2700: Training Loss = 0.09727166\n",
      "Epoch 2800: Training Loss = 0.09075262\n",
      "Epoch 2900: Training Loss = 0.08483663\n"
     ]
    }
   ],
   "source": [
    "W1, W2, b1, b2 = training_using_vectorization(X, Y, n1, 1, 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-efbd6e34ff91e003",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(b1, np.ndarray), \"b1 should be a numpy array.\"\n",
    "assert isinstance(b2, np.ndarray), \"b2 should be a numpy array.\"\n",
    "\n",
    "assert b1.shape[0] == 1, \"The number of rows in b1 is incorrect.\"\n",
    "assert b1.shape[1] == n1, \"The number of columns in b1 is incorrect.\"\n",
    "\n",
    "assert b2.shape[0] == 1, \"The number of rows in b2 is incorrect.\"\n",
    "assert b2.shape[1] == n2, \"The number of columns in b2 is incorrect.\"\n",
    "\n",
    "assert isinstance(W1, np.ndarray), \"W1 should be a numpy array.\"\n",
    "assert isinstance(W2, np.ndarray), \"W2 should be a numpy array.\"\n",
    "\n",
    "assert W1.shape[0] == n0, \"The number of rows in W1 is incorrect.\"\n",
    "assert W1.shape[1] == n1, \"The number of columns in W1 is incorrect.\"\n",
    "\n",
    "assert W2.shape[0] == n1, \"The number of rows in W2 is incorrect.\"\n",
    "assert W2.shape[1] == n2, \"The number of columns in W2 is incorrect.\"\n",
    "\n",
    "assert np.allclose(np.round(W1[0,:], decimals=8), np.array([1.26422707, -0.70762336, 7.20233002, -2.53263138, 3.40793382, -5.59849532, 3.61526723]))\n",
    "assert np.allclose(np.round(b1, decimals = 8), np.array([6.93259983, 6.38592697, -1.368562, 3.32583488, 4.44835927, 2.09694021, -1.09501032]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2b3da38b1db9f488",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (37804519.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_22274/37804519.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    For grading purpose: accuracy\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "For grading purpose: accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-14c926d99aa1cf95",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "##### Prediction using vectorization\n",
    "\n",
    "Re-implement the prediction function using vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-07ee2343ff3cedbe",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def predict_using_vectorization(W1, W2, b1, b2, X):\n",
    "    '''\n",
    "    Returns the predicted labels 'Y' and class probabilities 'P' for a set of data samples.\n",
    "    X is a matrix specifying the data, where each row corresponds to a single data sample.\n",
    "    Y is a vector of integers specifying the predicted class labels.\n",
    "    P is matrix of probabilities, where each row specifies the class probabilities for a single data sample.\n",
    "    '''\n",
    "    #Forward propagate for predictions\n",
    "    A1, P = forward_propagation_using_vectorization(W1, W2, b1, b2, X)\n",
    "    #Maximum value index is predicted class\n",
    "    Y = np.argmax(P, axis = 1)\n",
    "    return Y, P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-aff2ffbdcac1001a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "Y_pred, P = predict_using_vectorization(W1, W2, b1, b2, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-747730af2b492951",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(Y_pred, np.ndarray) and Y_pred.shape == (X.shape[0],), \"Y should be a numpy array of shape (m,)\"\n",
    "assert isinstance(P, np.ndarray) and P.shape == (X.shape[0], n2), \"P should be a numpy array of shape (m, n2).\"\n",
    "assert np.array_equal(Y_pred[0:5], np.array([2, 0, 2, 1, 1])), \"Incorrect predicted labels.\"\n",
    "assert np.allclose(np.round(P[15,:], decimals = 8), np.array([2.8000000e-07, 9.9999825e-01, 1.4700000e-06])), \"Incorrect class probabilities (test 1).\"\n",
    "assert np.allclose(np.round(P[25,:], decimals=8), np.array([0.14677544, 0.06200546, 0.7912191 ])), \"Incorrect class probabilities (test 2).\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3\n",
    "\n",
    "In the third part of this assignment, you will use the functions you implemented to train an autoencoder.  An autoencoder is a neural network whose inputs and outputs are identical.  In other words, it is a network that is trained to learn the identity function.  You will train a neural network with 8 inputs, 3 hidden units, and 8 outputs.\n",
    "\n",
    "Why is learning the identity function useful?  By using a smaller number of hidden units than the number of inputs, we can force the neural network to learn a representation of the input that is more compact.\n",
    "\n",
    "This part of the assignment is open-ended.  Your goal is to do the following:\n",
    "- Create a set of inputs and outputs to train the autoencoder.  The data samples should be one hot encoded labels for eight different classes.  Since there are only 8 different possibilities, your data set should contain 8 data samples.\n",
    "- Train a neural network with 3 hidden units using the functions you implemented in parts 1 and 2.  Train the network until the training loss has approximately converged.\n",
    "- Look at the 3-dimensional representation for each of the 8 possible inputs.  Comment on what you observe.\n",
    "\n",
    "You may use as many cells and code blocks as necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-aad55a7c617b447c",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Create a set of inputs\n",
    "\n",
    "# Define our network layer sizes and training set size\n",
    "n0 = 8 # size of input layer\n",
    "n1 = 3 # size of hidden layer\n",
    "n2 = 8 # size of output layer\n",
    "N = 100000 # size of training set\n",
    "X = []\n",
    "\n",
    "#Create data:\n",
    "for i in range(N):\n",
    "    index = int(np.random.rand(1) * 8) #Randomly decide a value 1-8\n",
    "    #Create a training sample with a 1 at decided index\n",
    "    temp = np.zeros(8)\n",
    "    temp[index] = 1\n",
    "    X.append(temp)\n",
    "#Make the training set a numpy array and set labels equal to inputs\n",
    "#for autoencoder\n",
    "X = np.array(X)\n",
    "Y = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-ed724509fb170f58",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Training Loss = 2.07950805\n",
      "Epoch 100: Training Loss = 2.06730969\n",
      "Epoch 200: Training Loss = 1.39399517\n",
      "Epoch 300: Training Loss = 0.50617516\n",
      "Epoch 400: Training Loss = 0.20823116\n",
      "Epoch 500: Training Loss = 0.12266651\n",
      "Epoch 600: Training Loss = 0.08590644\n",
      "Epoch 700: Training Loss = 0.06587001\n",
      "Epoch 800: Training Loss = 0.05334363\n",
      "Epoch 900: Training Loss = 0.04479530\n",
      "Epoch 1000: Training Loss = 0.03859810\n",
      "Epoch 1100: Training Loss = 0.03390270\n",
      "Epoch 1200: Training Loss = 0.03022369\n",
      "Epoch 1300: Training Loss = 0.02726402\n",
      "Epoch 1400: Training Loss = 0.02483188\n",
      "Epoch 1500: Training Loss = 0.02279797\n",
      "Epoch 1600: Training Loss = 0.02107200\n",
      "Epoch 1700: Training Loss = 0.01958901\n",
      "Epoch 1800: Training Loss = 0.01830112\n",
      "Epoch 1900: Training Loss = 0.01717221\n"
     ]
    }
   ],
   "source": [
    "# Train your network. Use as many cells and code blocks as necessary.\n",
    "\n",
    "# Run the training loop on our dataset\n",
    "# Can reduce the iterations or size of train set to improve runtime\n",
    "W1, W2, b1, b2 = training_using_vectorization(X, Y, n1, 1, 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-ffd7f3754e72c5fd",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "The autoencoder works by encoding the 8 inputs to be as far away from each other as possible in 3D space. This is further explored below. Let's look at the values and a graph of the 8 representations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-6f81978177a30c69",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Representations generated by the model:\n",
      "[[ 4.   -3.89  2.91]]\n",
      "[[ 3.05  3.47 -3.95]]\n",
      "[[-3.69 -3.38 -2.77]]\n",
      "[[-3.44 -3.9   4.21]]\n",
      "[[3.5  3.4  3.21]]\n",
      "[[-3.96  4.11 -3.38]]\n",
      "[[ 3.4  -3.55 -4.22]]\n",
      "[[-3.69  3.01  3.53]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Look at the 3-dimensional representation for each of the 8 possible inputs. \n",
    "#Import tools for graphing\n",
    "from mpl_toolkits import mplot3d\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Create a figure for 3D plotting\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "plt.title('3D representation from autoencoder')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "#Create a vector for storing the 3D representations of the 8D inputs\n",
    "repns = []\n",
    "print(\"Representations generated by the model:\")\n",
    "#For each representation, plot in 3D space\n",
    "for i in range(8):\n",
    "    x = np.zeros(8)\n",
    "    x[i] = 1 #Create one of the inputs\n",
    "    rep = np.matmul(x, W1)+b1 #Run through model for 3D representation\n",
    "    repns.append(rep) #Save representation for later\n",
    "    #Plot it in 3D!\n",
    "    line = np.array(([0, rep[0][0]],[0, rep[0][1]],[0, rep[0][2]]))\n",
    "    ax.plot3D(line[0], line[1], line[2])\n",
    "    #Also print the vectors for each representation in 3D\n",
    "    print(np.around(rep,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Encoder looks like it is building a cube! Each of the representations points towards a different corner of the cube, making the maximum separation of the eight representations in three dimensional space. This makes sense because having the most possible separation in 3D makes it easiest to discern each input when converting back to 8D in the output layer.\n",
    "\n",
    "We can also look at the angle between the representations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Angles between the representations in 3D space\n",
      "Row 1 is angles from representation 1 to all other representations, etc\n",
      "\n",
      "[[  0.  109.5 105.5  71.1  74.1 176.5  68.2 115.8]\n",
      " [109.5   0.  110.3 178.5  74.   67.3  68.1 114.1]\n",
      " [105.5 110.3   0.   68.2 175.4  74.6  72.5 100.7]\n",
      " [ 71.1 178.5  68.2   0.  107.5 112.  111.2  66.5]\n",
      " [ 74.1  74.  175.4 107.5   0.  106.1 111.3  75.5]\n",
      " [176.5  67.3  74.6 112.  106.1   0.  108.7  67.4]\n",
      " [ 68.2  68.1  72.5 111.2 111.3 108.7   0.  173.1]\n",
      " [115.8 114.1 100.7  66.5  75.5  67.4 173.1   0. ]]\n"
     ]
    }
   ],
   "source": [
    "#Create some functions for calculating angles between 3d vectors\n",
    "import math\n",
    "def dotproduct(v1, v2):\n",
    "  return sum((a*b) for a, b in zip(v1, v2))\n",
    "\n",
    "def length(v):\n",
    "  return math.sqrt(dotproduct(v, v))\n",
    "\n",
    "def angle(v1, v2):\n",
    "  return math.acos(dotproduct(v1, v2) / (length(v1) * length(v2)))\n",
    "\n",
    "#Find the angle between every representation and all other representations\n",
    "#Put them in a matrix angles\n",
    "angles = np.zeros((8,8))\n",
    "for i in range(8):\n",
    "    for j in range(8):\n",
    "        if i!=j:\n",
    "            angles[i][j]=angle(repns[i][0], repns[j][0])\n",
    "#Convert radians to degrees\n",
    "print(\"Angles between the representations in 3D space\")\n",
    "print(\"Row 1 is angles from representation 1 to all other representations, etc\\n\")\n",
    "print(np.around(angles * 180/math.pi, 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a perfect cube(which creates the most separation between eight points in 3D), we would expect each representation to have an angle of 0 with itself, angle of 70.5 deg with three other representations, angle of 109.5 deg with three more representations, and angle of 180 deg with the final representation. The angles produced by our autoencoder match this very closely! The neural net was able to produce a nearly optimal separation of the eight representations in 3D space.\n",
    "\n",
    "Generally, it seems that for any number of inputs n and hidden layers h, the autoencoder tries to map the n inputs to h-dimensional vectors with the most separation between them."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
