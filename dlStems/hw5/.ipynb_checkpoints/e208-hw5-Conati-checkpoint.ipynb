{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "499bd9686299961c3426d6bb2de4f1df",
     "grade": false,
     "grade_id": "cell-53bb2218cccd55eb",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# HW 5: Neural Network Improvements\n",
    "\n",
    "In this homework assignment, you will implement a more complex neural network with several improvements.\n",
    "\n",
    "There are three parts to this assignment:\n",
    "- In part 1, you will implement a neural network with 2 hidden layers, rectified linear units, regularization, and mini-batch gradient descent.\n",
    "- In part 2, you will incorporate dropout into your neural network.\n",
    "- In part 3, you will use your neural network to classify handwritten digits.\n",
    "\n",
    "A few important notes:\n",
    "* For autograding purposes, many cells are set in *read-only* mode.  This means you cannot edit, delete, or move these cells. \n",
    "\n",
    "* For part 1, you should not modify any code except where you see `# YOUR CODE HERE`.  Please remove `raise NotImplementedError()` and write your code in its place.\n",
    "\n",
    "* We provide code cells that check your code as you progress through the assignment. These tests will help catch obvious errors, but passing these tests is not a guarantee that your implementation is correct. Your notebook will be graded on a more complete set of tests which are not visible to you.\n",
    "\n",
    "* You may add new cells anywhere in the notebook while working, but make sure to delete them before submitting your assignment.  Please do not change the sequence of the cells.  This is to ensure that autograding works properly.\n",
    "\n",
    "* If you run cells out of order, it may change variable values which may cause tests to fail.  The easiest way to avoid this problem is to use the ***Restart & Run all*** option available in the **Kernel** tab.  Before submitting, you should do this to ensure that your notebook can be run from beginning to end without errors.  This is how your notebook will be graded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8746473534dd39b0be0a59eedea98aec",
     "grade": false,
     "grade_id": "cell-91dcc7999a9e85dd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Part 1\n",
    "\n",
    "In the first part of this assignment, you will re-implement the neural network from last week but with the following changes:\n",
    "- two hidden layers instead of one\n",
    "- the activations of the hidden layers are rectified linear units (ReLU), not sigmoid functions\n",
    "- initializing weights with He initialization\n",
    "- adding dropout to the hidden layers\n",
    "- loss function will incorporate a regularization term\n",
    "- mini-batch gradient descent instead of batch gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "471e2e2af45a59b2f1cbe8de6e4e2bd1",
     "grade": false,
     "grade_id": "cell-cd53681a3204e71b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's define our notation:\n",
    "\n",
    "* The number of nodes in the input, first hidden, second hidden, and output layers are $n_0$, $n_1$, $n_2$, and $n_3$ respectively.\n",
    "\n",
    "* The batch size is given by $m$.\n",
    "\n",
    "* Each input batch $X$ is an array of shape $(m, n_0)$, and the corresponding target labels $Y$ is an array of shape $(m, n_3)$.\n",
    "\n",
    "* The weights connecting the layers are given by the matrices $W_1$, $W_2$ and $W_3$ of shapes $(n_0, n_1)$, $(n_1, n_2)$ and $(n_2, n_3)$ respectively.\n",
    "\n",
    "* The bias terms are given by the arrays $b_1$, $b_2$ and $b_3$ of shapes $(1, n_1)$, $(1, n_2)$ and $(1, n_3)$ respectively.\n",
    "\n",
    "* The activation function for the hidden layers is a rectified linear unit (ReLU).\n",
    "\n",
    "* The activation function for the output layer is the softmax function.\n",
    "\n",
    "* The activations for the layers are given by the matrices $A_1$, $A_2$ and $A_3$ of shapes $(m, n_1)$, $(m, n_2)$ and $(m, n_3)$ respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fe8ba6b7726b0ee5b041eee35b2932da",
     "grade": false,
     "grade_id": "cell-14d8474dec121569",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "n0 = 4 # size of input layer\n",
    "n1 = 3 # size of 1st hidden layer\n",
    "n2 = 5 # size of 2nd hidden layer\n",
    "n3 = 3 # size of output layer\n",
    "m = 50 # batch size\n",
    "N = 1000 # total number of training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "46371e266ddf0f146d984358ea9c36de",
     "grade": false,
     "grade_id": "cell-3398bc2c1228257c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "47adf9142dc6eec3cc0f453160c8515b",
     "grade": false,
     "grade_id": "cell-2275f6e6b316168f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "To test our functions, we can generate a dataset suitable for a classification task using [`make_classification`](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html) in [scikit-learn](https://scikit-learn.org/stable/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2da7bcdc84b0afde30ec7baed2ad0918",
     "grade": false,
     "grade_id": "cell-5d9607397bc0369f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X_all, Y_all = make_classification(n_samples=2*N, n_features=n0, n_redundant=0, n_informative=n0, n_classes=n3, random_state=1)\n",
    "Y_all = np.eye(n3)[Y_all] # one hot encode\n",
    "X_train = X_all[0:N,:] # split into train and validation sets\n",
    "Y_train = Y_all[0:N,:]\n",
    "X_val = X_all[N:,:]\n",
    "Y_val = Y_all[N:,:]\n",
    "X = X_train[0:m,:] # single batch of training samples\n",
    "Y = Y_train[0:m, :]\n",
    "x = X_train[0,:].reshape((1,-1)) # single sample\n",
    "y = Y_train[0,:].reshape((1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e19b0ff0ea8ac93dd9870267fd102e6c",
     "grade": false,
     "grade_id": "cell-84fbccbd04359d64",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert X.shape[1] == n0\n",
    "assert Y.shape[1] == n3\n",
    "assert X.shape[0] == Y.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "326014d869a54b12b472f9264a638f1d",
     "grade": false,
     "grade_id": "cell-b1a4985230a29c6d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "#### Initialize weight matrices\n",
    "\n",
    "Initialize the weight matrices $W_1$, $W_2$, and $W_3$ using He initialization.  This scheme initializes weight matrices with random normally distributed values with variance $\\frac{2}{n}$, where $n$ is the number of inputs to the layer.  He initialization is specifically designed for neural networks using ReLU activation functions and is described in this [research paper](https://arxiv.org/abs/1502.01852)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "385370a79dda44b9baa2b803fecd9921",
     "grade": false,
     "grade_id": "weights",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def initialize_weights(n0, n1, n2, n3):\n",
    "    '''\n",
    "    Returns a tuple (W1, W2, W3) containing the initialized weight matrices.  Uses He initialization.\n",
    "    '''\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    mu = 0\n",
    "    #Initialize with He initialization. Use the given sizes and find standard deviations\n",
    "    w1Size  = (n0,n1)\n",
    "    w1Sigma = np.sqrt(2/n0)\n",
    "    w2Size = (n1,n2)\n",
    "    w2Sigma = np.sqrt(2/n1)\n",
    "    w3Size = (n2,n3)\n",
    "    w3Sigma = np.sqrt(2/n2)\n",
    "    \n",
    "    #Initialize with the standard deviations\n",
    "    W1 = np.random.normal(mu, w1Sigma, w1Size)\n",
    "    W2 = np.random.normal(mu, w2Sigma, w2Size)\n",
    "    W3 = np.random.normal(mu, w3Sigma, w3Size)\n",
    "    \n",
    "    return (W1, W2, W3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a298ad420cc0d1402f3f6144cf3da174",
     "grade": false,
     "grade_id": "cell-83f22147650c23d9",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "W1, W2, W3 = initialize_weights(n0, n1, n2, n3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "111868af59ea931249517f9771e76e34",
     "grade": true,
     "grade_id": "weights_test",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(W1, np.ndarray), \"W1 should be a numpy array.\"\n",
    "assert isinstance(W2, np.ndarray), \"W2 should be a numpy array.\"\n",
    "assert isinstance(W3, np.ndarray), \"W3 should be a numpy array.\"\n",
    "\n",
    "assert W1.shape[0] == n0, \"The number of rows in W1 is incorrect.\"\n",
    "assert W1.shape[1] == n1, \"The number of columns in W1 is incorrect.\"\n",
    "\n",
    "assert W2.shape[0] == n1, \"The number of rows in W2 is incorrect.\"\n",
    "assert W2.shape[1] == n2, \"The number of columns in W2 is incorrect.\"\n",
    "\n",
    "assert W3.shape[0] == n2, \"The number of rows in W3 is incorrect.\"\n",
    "assert W3.shape[1] == n3, \"The number of columns in W3 is incorrect.\"\n",
    "\n",
    "assert np.sum(np.abs(W1)) != 0, \"All the elements in W1 should not be zero.\"\n",
    "assert np.sum(np.abs(W2)) != 0, \"All the elements in W2 should not be zero.\"\n",
    "assert np.sum(np.abs(W3)) != 0, \"All the elements in W3 should not be zero.\"\n",
    "\n",
    "assert np.allclose(np.round(W1[0,:], decimals=8), np.array([1.24737338, 0.28295388, 0.69207227])), \"Incorrect variance for weights in W1.\"\n",
    "assert np.allclose(np.round(W2[0,:], decimals=8), np.array([0.6213847 , 0.09934723, 0.36241281, 0.27244395, 1.21991045])), \"Incorrect variance for weights in W2.\"\n",
    "assert np.allclose(np.round(W3[0,:], decimals=8), np.array([-0.11838546,  0.96941469,  0.92930408])), \"Incorrect variance for weights in W3.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f03e285a7fd9dcad901f1d8b2e64fecb",
     "grade": false,
     "grade_id": "cell-483ee48adcdd5c99",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "#### Initialize bias terms\n",
    "\n",
    "Initialize the bias terms $b_1$, $b_2$, and $b_3$ to $0.01$.  By using a small positive bias, we ensure that all rectified linear units fire in the beginning and are able to propagate gradients during backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "22debed5a21b2e64a1201f116163c846",
     "grade": false,
     "grade_id": "cell-df2b5c453a4a7798",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def initialize_bias(n0, n1, n2, n3):\n",
    "    '''\n",
    "    Returns a tuple of vectors (b1, b2, b3) with the initialized bias terms.\n",
    "    '''\n",
    "    \n",
    "    # Initialize the biases to 0.01\n",
    "    b1 = np.zeros((1,n1))+0.01\n",
    "    b2 = np.zeros((1,n2))+0.01\n",
    "    b3 = np.zeros((1,n3))+0.01\n",
    "    return (b1, b2, b3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d7742bb3a941ee47c88026035d4a451c",
     "grade": false,
     "grade_id": "cell-f8f214ee35fb5f3a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "b1, b2, b3 = initialize_bias(n0, n1, n2, n3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4a312dae46eab70fb09a88e9d146cf47",
     "grade": true,
     "grade_id": "cell-1559c5504a1e54a1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(b1, np.ndarray), \"b1 should be a numpy array.\"\n",
    "assert isinstance(b2, np.ndarray), \"b2 should be a numpy array.\"\n",
    "assert isinstance(b3, np.ndarray), \"b3 should be a numpy array.\"\n",
    "\n",
    "assert b1.shape[0] == 1, \"The number of rows in b1 is incorrect.\"\n",
    "assert b1.shape[1] == n1, \"The number of columns in b1 is incorrect.\"\n",
    "\n",
    "assert b2.shape[0] == 1, \"The number of rows in b2 is incorrect.\"\n",
    "assert b2.shape[1] == n2, \"The number of columns in b2 is incorrect.\"\n",
    "\n",
    "assert b3.shape[0] == 1, \"The number of rows in b3 is incorrect.\"\n",
    "assert b3.shape[1] == n3, \"The number of columns in b3 is incorrect.\"\n",
    "\n",
    "assert np.all(b1 == .01), \"The values in b1 should be all zeros, but they are not.\"\n",
    "assert np.all(b2 == .01), \"The values in b2 should be all zeros, but they are not.\"\n",
    "assert np.all(b3 == .01), \"The values in b3 should be all zeros, but they are not.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c83e06a4f3ab96696d950a6a2d1eb266",
     "grade": false,
     "grade_id": "cell-9833c54bdcbd88c1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "#### Rectified Linear Unit (ReLU)\n",
    "\n",
    "$$relu(z) = max(0, z)$$\n",
    "\n",
    "Define the rectified linear unit activation function. If the input is a numpy array, the function should be applied to each element.  You can use the [`np.maximum`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.maximum.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0d6bdfe6ad8bdeaa1ab51056828eb55a",
     "grade": false,
     "grade_id": "cell-37c2b10f025bff91",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def ReLU(z):\n",
    "    '''\n",
    "    Applies the rectified linear unit activation function to each element in z.\n",
    "    '''\n",
    "    #Return the maximum\n",
    "    return np.maximum(0,z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "520a7c4733bb4795d143d49939e238ac",
     "grade": true,
     "grade_id": "cell-659fe4f6e526f900",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert ReLU(0) == 0, \"Incorrect value for t=0\"\n",
    "assert ReLU(1.234) == 1.234, \"Incorrect result for a positive input.\"\n",
    "assert ReLU(-2.35) == 0, \"Incorrect result for a negative input.\"\n",
    "\n",
    "assert np.array_equal(ReLU(np.array([1.15, -2, 3])), np.array([1.15, 0, 3])), \"Incorrect result for 1D numpy array.\"\n",
    "assert np.array_equal(ReLU(np.array([[-1, 6], [-3, 0.1], [4, 7]])), np.array([[0, 6], [0, 0.1], [4, 7]])), \"Incorrect result for 2D numpy array.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Softmax function\n",
    "\n",
    "$$ softmax([z_1, \\dots, z_k]) = \\left[\\frac{e^{z_1}}{e^{z_1}+ \\dots + e^{z_k}}, \\dots, \\frac{e^{z_k}}{e^{z_1}+ \\dots + e^{z_k}}\\right]$$\n",
    "\n",
    "Since you implemented this last week, the implementation is provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    '''\n",
    "    Applies the softmax function to each row of a matrix Z.\n",
    "    This implementation should be vectorized, so it should not contain any for loops.\n",
    "    '''\n",
    "    assert isinstance(Z, np.ndarray) and Z.ndim == 2, \"Input should be a 2D numpy array.\"\n",
    "    \n",
    "    maxVals = np.max(Z, axis=1, keepdims=True)\n",
    "\n",
    "    Znorm = Z - maxVals\n",
    "    num = np.exp(Znorm)\n",
    "    den = np.sum(num, axis=1, keepdims=True)\n",
    "    result = num / den\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "db4dc054fdd3a697991d63dd7e649181",
     "grade": false,
     "grade_id": "cell-54390a37fc311cc5",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "#### Forward propagation\n",
    "\n",
    "Implement the forward propagation function.  Be sure to check the dimensions of your variables in order to determine the correct order of matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b0af05bce4680d705d7b2b11bb37ff64",
     "grade": false,
     "grade_id": "cell-69573e1f56723449",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def forward_propagation(W1, W2, W3, b1, b2, b3, X):\n",
    "    '''\n",
    "    Performs forward propagation on a batch of inputs.\n",
    "    X is a matrix containing a batch of inputs, where each row corresponds to a single data sample.\n",
    "    \n",
    "    Returns a tuple of matrices (A1, A2, A3) containing the activations at the hidden and output layers\n",
    "    for all training examples in X.\n",
    "    '''\n",
    "    #Apply first layer and relu\n",
    "    A1 = ReLU(np.matmul(X, W1)+b1)\n",
    "    #Apply second layer and relu\n",
    "    A2 = ReLU(np.matmul(A1, W2)+b2)\n",
    "    #Apply third layer and relu\n",
    "    A3 = softmax(np.matmul(A2, W3)+b3)\n",
    "    return (A1, A2, A3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0b800eb577c4373e3e61d7b18185262e",
     "grade": false,
     "grade_id": "cell-96bfa58c8835a5c0",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "W1, W2, W3 = initialize_weights(n0, n1, n2, n3)\n",
    "b1, b2, b3 = initialize_bias(n0, n1, n2, n3)\n",
    "(A1, A2, A3) = forward_propagation(W1, W2, W3, b1, b2, b3, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b7ce0a6ea9b10932529c6e91f1ab307e",
     "grade": true,
     "grade_id": "cell-af11f071a11dd0e2",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert A1.shape[0] == m, \"The number of rows in A1 is incorrect.\"\n",
    "assert A1.shape[1] == n1, \"The number of columns in A1 is incorrect.\"\n",
    "\n",
    "assert A2.shape[0] == m, \"The number of rows in A2 is incorrect.\"\n",
    "assert A2.shape[1] == n2, \"The number of columns in A2 is incorrect.\"\n",
    "\n",
    "assert A3.shape[0] == m, \"The number of rows in A3 is incorrect.\"\n",
    "assert A3.shape[1] == n3, \"The number of columns in A3 is incorrect.\"\n",
    "\n",
    "assert np.allclose(np.round(A1[0,:], decimals=8), np.array([0., 0., 1.00318339])), \"Incorrect values in A1.\"\n",
    "assert np.allclose(np.round(A2[0,:], decimals=8), np.array([0.71805607, 0., 1.86914649, 0., 0.04748061])), \"Incorrect values in A2.\"\n",
    "assert np.allclose(np.round(A3[0,:], decimals=8), np.array([0.02416447, 0.35563074, 0.62020479])), \"Incorrect values in A3.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-entropy loss with regularization\n",
    "\n",
    "In last week's assignment we used the cross-entropy loss function:\n",
    "\n",
    "$$ J = - \\frac{1}{m} \\sum_{i=1}^m \\sum_{j=1}^k y^{(i)}_j \\log(p^{(i)}_j)$$\n",
    "\n",
    "where $y^{(i)}$ is the one-hot encoded label for the $i^{th}$ data point and $p^{(i)}_j$ is the probability of the $j^{th}$ class for the $i^{th}$ data point in the batch.\n",
    "\n",
    "This week we will use a cross-entropy loss function with a regularization term:\n",
    "\n",
    "$$ J = - \\frac{1}{m} \\sum_{i=1}^m \\sum_{j=1}^k \\biggl( y^{(i)}_j \\log(p^{(i)}_j) \\biggr) + \\frac{\\lambda}{2m} \\sum_{i=1}^{3} \\lVert W_i \\rVert_F^2$$\n",
    "\n",
    "where the matrix norm is the [Frobenius norm](http://mathworld.wolfram.com/FrobeniusNorm.html) and $\\lambda$ is a hyperparameter that controls the weight of the regularization term.  The regularization term penalizes large weight values, which makes the weights smaller and reduces the risk of overfitting.  Note that only the weights in $W_1$, $W_2$, and $W_3$ are penalized, not the bias terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "91630bcd92c08de6dc3c8c8cc312a328",
     "grade": false,
     "grade_id": "cell-223a535afdfade8c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def cross_entropy_loss_with_regularization(A, Y, W1, W2, W3, lambd):\n",
    "    '''\n",
    "    Calculates the cross-entropy loss for a set of output probabilities and target labels.\n",
    "    The loss also includes a regularization term on the elements of the weight matrices.\n",
    "    \n",
    "    A is a matrix where each row specifies the output layer activations for a single input.\n",
    "    Y is a matrix where each row specifies the one-hot encoded target label for a single input.\n",
    "    W1, W2, and W3 are the weight matrices of the layers in the neural network\n",
    "    lambd is a hyperparameter specifying the weighting of the regularization penalty.\n",
    "    The computed loss should be a scalar.    \n",
    "    '''\n",
    "    m = A.shape[0]\n",
    "    \n",
    "    #Find the norm of each weights matrix\n",
    "    W1f = np.square(np.linalg.norm(W1)) *(lambd/100)\n",
    "    W2f = np.square(np.linalg.norm(W2)) *(lambd/100)\n",
    "    W3f = np.square(np.linalg.norm(W3)) *(lambd/100)\n",
    "    \n",
    "    #return CE loss with the sum of the norms\n",
    "    return np.sum(np.multiply(np.log(A),Y))/(-m) + (np.sum([W1f, W2f, W3f]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fbe53fd0977e21c75ad8b4a00cbe9cfe",
     "grade": false,
     "grade_id": "cell-d55cb50af313921f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "W1, W2, W3 = initialize_weights(n0, n1, n2, n3)\n",
    "b1, b2, b3 = initialize_bias(n0, n1, n2, n3)\n",
    "(A1, A2, A3) = forward_propagation(W1, W2, W3, b1, b2, b3, X)\n",
    "lambd = .01\n",
    "J = cross_entropy_loss_with_regularization(A3, Y, W1, W2, W3, lambd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a97bb0e275fa793c95cf1d63cc80b809",
     "grade": true,
     "grade_id": "cell-64ef5ff065f08b74",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert np.isscalar(J), \"Output is not a scalar.\"\n",
    "assert np.round(J, decimals=8) == 2.65933389, \"Returns incorrect loss value.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e1fba75df26cc60bbd5177e48bfa993c",
     "grade": true,
     "grade_id": "cell-55ec1a62d5b60797",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "757b245c7d70bec1dcf2f558d79924e9",
     "grade": true,
     "grade_id": "cell-88531bd3fc4f91e6",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fde54c00a5cd91e5879937347c46a765",
     "grade": true,
     "grade_id": "cell-0282ebd4ccd9fada",
     "locked": false,
     "points": 15,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "***Solution:***\n",
    "\n",
    "Write your gradient equations for a single input $x$, followed by the gradient equations for a batch of samples $X$.\n",
    "\n",
    "Following our computation graph:\n",
    "#### For a sample k:\n",
    "\n",
    "$dJ/da_{3}$:  \n",
    "$\\frac{dJ}{da_{3}} = [\\frac{-y_k}{\\hat{y_k}}]$ \n",
    "\n",
    "\n",
    "$dJ/dz_{3}$:\n",
    "\n",
    "$\\frac{dJ}{dz_{3}} = \\bar{-y_k} + \\bar{a_{3k}}$ \n",
    "\n",
    "$dJ/db_{3}$:\n",
    "\n",
    "$\\frac{dJ}{db_{3}} = \\frac{dJ}{dz_{3}}$ \n",
    "\n",
    "$dJ/dW_{3}$:\n",
    "\n",
    "$\\frac{dJ}{dW_{3}} = \\frac{dJ}{dz_{3}}*a_{2k} + lambd*W3$ \n",
    "\n",
    "$dJ/da_2$:  \n",
    "\n",
    "$\\frac{dJ}{da_2} = \\frac{dJ}{dz_3} * W_{3k} $  \n",
    "\n",
    "$dJ/dz_2$:  \n",
    "\n",
    "$\\frac{dJ}{dz_2} = \\frac{dJ}{da_2}* \\frac{da_2}{dz_2}$\n",
    "\n",
    "where $\\frac{da_2}{dz_2}$ is the derivative of reLU with respect to inputs\n",
    "\n",
    "$\\frac{dJ}{dz_2} = \\frac{dJ}{da_2}$ if $z_2 >0$, else 0  \n",
    "\n",
    "$dJ/dW_2$:\n",
    "\n",
    "$\\frac{dJ}{dW_2} = \\frac{dJ}{dz_2}* a_{1k} + lambd*W2$\n",
    "\n",
    "$dJ/db_2$:    \n",
    "\n",
    "$\\frac{dJ}{db_{2}} = \\frac{dJ}{dz_{2}}$ \n",
    "\n",
    "$dJ/da_1$:  \n",
    "\n",
    "$\\frac{dJ}{da_1} = \\frac{dJ}{dz_2} * W_{2k} $  \n",
    "\n",
    "$dJ/dz_1$:  \n",
    "\n",
    "$\\frac{dJ}{dz_1} = \\frac{dJ}{da_1}* \\frac{da_1}{dz_1}$\n",
    "\n",
    "where $\\frac{da_1}{dz_1}$ is the derivative of reLU with respect to inputs\n",
    "\n",
    "$\\frac{dJ}{dz_1} = \\frac{dJ}{da_1}$ if $z_1 >0$, else 0  \n",
    "\n",
    "$dJ/dW_1$:\n",
    "\n",
    "$\\frac{dJ}{dW_1} = \\frac{dJ}{dz_1}* x_{k} + lambd*W1$\n",
    "\n",
    "$dJ/db_1$:    \n",
    "\n",
    "$\\frac{dJ}{db_{1}} = \\frac{dJ}{dz_{1}}$ \n",
    "\n",
    "## For a batch X\n",
    "\n",
    "dJ/da3:  \n",
    "$\\frac{dJ}{da_3} = [\\frac{-y_1}{\\hat{y_1}},\\frac{-y_2}{\\hat{y_2}},...\\frac{-y_{s3}}{\\hat{y_{s3}}}]$  \n",
    "\n",
    "\n",
    "$\\frac{dJ}{da_3} = \\bar{y} .* \\bar{a_3}^{ -1}$ \n",
    "\n",
    "dJ/dz3:\n",
    "\n",
    "$\\frac{dJ}{dz_3} = \\bar{-y} + \\bar{a_3}$ \n",
    "\n",
    "dJ/db3:\n",
    "\n",
    "$\\frac{dJ}{db_3} = sum(\\frac{dJ}{dz_3})/m$ \n",
    "\n",
    "dJ/dW3:\n",
    "\n",
    "$\\frac{dJ}{dW_3} = \\bar{a}_{2}^T*\\frac{dJ}{dz_3} + \\frac{dJ}{dW3}$  \n",
    "\n",
    "$\\frac{dJ}{dW_3} = (\\bar{a}_{2}^T*\\frac{dJ}{dz_3} + lambd*W3)/m$\n",
    "\n",
    "dJ/da2:\n",
    "\n",
    "$\\frac{dJ}{da_2} = \\frac{dJ}{dz_3} * W_3^T $  \n",
    "\n",
    "dJ/dz2:\n",
    "\n",
    "$\\frac{dJ}{dz_2} =  \\frac{dJ}{da_2}$ if $z_2 >0$, else 0 \n",
    "\n",
    "dJ/db2:\n",
    "\n",
    "$\\frac{dJ}{db_2} = sum(\\frac{dJ}{dz_2})/m$ \n",
    "\n",
    "dJ/dW2:\n",
    "\n",
    "$\\frac{dJ}{dW_2} = (\\bar{a}_{1}^T*\\frac{dJ}{dz_2} + lambd*W2)/m$ \n",
    "\n",
    "dJ/da1:\n",
    "\n",
    "$\\frac{dJ}{da_1} = \\frac{dJ}{dz_2} * W_2^T $  \n",
    "\n",
    "dJ/dz1:\n",
    "\n",
    "$\\frac{dJ}{dz_1} =  \\frac{dJ}{da_1}$ if $z_1 >0$, else 0 \n",
    "\n",
    "dJ/db1:\n",
    "\n",
    "$\\frac{dJ}{db_1} = sum(\\frac{dJ}{dz_1})/m$ \n",
    "\n",
    "dJ/dW1:\n",
    "\n",
    "$\\frac{dJ}{dW_1} = (\\bar{X}^T*\\frac{dJ}{dz_1} + lambd*W1)/m$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d7244e495134fb5231508cebf4241c2d",
     "grade": false,
     "grade_id": "cell-3bd2d0321e6a270f",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def back_propagation(W1, W2, W3, b1, b2, b3, A1, A2, A3, X, Y, lambd):\n",
    "    '''    \n",
    "    Performs backpropagation on a batch of training samples using vectorization.\n",
    "    X is a matrix containing a batch of data, where each row corresponds to a single data sample.\n",
    "    Y is a matrix containing the target labels, where each row corresponds to a one hot encoded label.\n",
    "    lambd is a hyperparameter specifying the weight of the regularization term.\n",
    "    \n",
    "    Returns a tuple of matrices (dW1, dW2, dW3, db1, db2, db3) containing the gradients averaged across the batch.\n",
    "    '''\n",
    "    m = X.shape[0]\n",
    "    #Save the bias shapes\n",
    "    db1Shape = b1.shape\n",
    "    db2Shape = b2.shape\n",
    "    db3Shape = b3.shape\n",
    "    \n",
    "    #Calculate the derivatives based on the equations above\n",
    "    da3 = -Y*np.reciprocal(A3)\n",
    "    dz3 = -Y+A3\n",
    "    db3 =  np.reshape(np.sum(dz3, axis=0)/m, db3Shape)\n",
    "    dW3 = (np.matmul(A2.T, dz3) +lambd*W3)/m\n",
    "    da2 = np.matmul(dz3, W3.T)\n",
    "    dz2 = np.multiply(da2, np.where(A2>0, 1, 0))\n",
    "    db2 =  np.reshape(np.sum(dz2, axis=0)/m, db2Shape)\n",
    "    dW2 = (np.matmul(A1.T,dz2) + lambd*W2)/m\n",
    "    da1 = np.matmul(dz2, W2.T)\n",
    "    dz1 = np.multiply(da1, np.where(A1>0, 1, 0))\n",
    "    db1 =  np.reshape(np.sum(dz1, axis=0)/m, db1Shape)\n",
    "    dW1 = (np.matmul(X.T,dz1) + lambd*W1)/m\n",
    "\n",
    "    return (dW1, dW2, dW3, db1, db2, db3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f14f27988fa68c389bf9054953181bcd",
     "grade": false,
     "grade_id": "cell-c45f5e70697e5bec",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "W1, W2, W3 = initialize_weights(n0, n1, n2, n3)\n",
    "b1, b2, b3 = initialize_bias(n0, n1, n2, n3)\n",
    "(A1, A2, A3) = forward_propagation(W1, W2, W3, b1, b2, b3, X)\n",
    "lambd = .01\n",
    "dW1, dW2, dW3, db1, db2, db3 = back_propagation(W1, W2, W3, b1, b2, b3, A1, A2, A3, X, Y, lambd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c588bf26a935891131a3ffff68bb8540",
     "grade": true,
     "grade_id": "cell-20ac674e23f838da",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(dW1, np.ndarray), \"dW1 should be a numpy array.\"\n",
    "assert isinstance(dW2, np.ndarray), \"dW2 should be a numpy array.\"\n",
    "assert isinstance(dW3, np.ndarray), \"dW3 should be a numpy array.\"\n",
    "\n",
    "assert dW3.shape[0] == n2, \"The number of rows in dW3 is incorrect.\"\n",
    "assert dW3.shape[1] == n3, \"The number of columns in dW3 is incorrect.\"\n",
    "\n",
    "assert dW2.shape[0] == n1, \"The number of rows in dW2 is incorrect.\"\n",
    "assert dW2.shape[1] == n2, \"The number of columns in dW2 is incorrect.\"\n",
    "\n",
    "assert dW1.shape[0] == n0, \"The number of rows in dW1 is incorrect.\"\n",
    "assert dW1.shape[1] == n1, \"The number of columns in dW1 is incorrect.\"\n",
    "\n",
    "\n",
    "assert isinstance(db1, np.ndarray), \"db1 should be a numpy array.\"\n",
    "assert isinstance(db2, np.ndarray), \"db2 should be a numpy array.\"\n",
    "assert isinstance(db3, np.ndarray), \"db3 should be a numpy array.\"\n",
    "\n",
    "assert db3.shape[0] == 1, \"The number of rows in db3 is incorrect.\"\n",
    "assert db3.shape[1] == n3, \"The number of columns in db3 is incorrect.\"\n",
    "\n",
    "assert db2.shape[0] == 1, \"The number of rows in db2 is incorrect.\"\n",
    "assert db2.shape[1] == n2, \"The number of columns in db2 is incorrect.\"\n",
    "\n",
    "assert db1.shape[0] == 1, \"The number of rows in db1 is incorrect.\"\n",
    "assert db1.shape[1] == n1, \"The number of columns in db1 is incorrect.\"\n",
    "\n",
    "assert np.allclose(np.round(dW3[0,:], decimals=8), np.array([-0.33152692, -0.16852274,  0.50040573])), \"Incorrect values for dW3.\"\n",
    "assert np.allclose(np.round(dW2[0,:], decimals=8), np.array([-0.12900891,  0.10285185, -0.10161577, -0.00150705,  0.06391473])), \"Incorrect values for dW2.\"\n",
    "assert np.allclose(np.round(dW1[0,:], decimals=8), np.array([-0.0194978 ,  0.00929456, -0.1503903 ])), \"Incorrect values for dW1.\"\n",
    "assert np.allclose(np.round(db3, decimals=8), np.array([-0.29658961,  0.02880348,  0.26778614])), \"Incorrect values for db3.\"\n",
    "assert np.allclose(np.round(db2, decimals=8), np.array([ 0.30485599,  0.01887831,  0.38868551, -0.01298385, -0.20289378])), \"Incorrect values for db2.\"\n",
    "assert np.allclose(np.round(db1, decimals=8), np.array([-0.01850444, -0.05875769,  0.95161451])), \"Incorrect values for db1.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a0e0a883387cb37337a1dd825a7714df",
     "grade": true,
     "grade_id": "cell-739f93c6194a9758",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e277ecd55d6c84ed92b16fcdb5d258cf",
     "grade": false,
     "grade_id": "cell-bc2f8ca5ae03dc3a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Gradient Checking\n",
    "\n",
    "You will implement a gradient checking function to verify that your backpropagation implementation is correct.  The gradient checking consists of the following steps:\n",
    "- Set your network weights to arbitrary default values.\n",
    "- Use your forward and backward propagation functions to calculate the gradients of the network weights.  These are the values that we would like to verify.\n",
    "- For each network weight, compute the approximate gradient by doing the following:\n",
    "    - Add a small epsilon to the weight/bias element of interest to create a modified set of network weights.\n",
    "    - Perform forward propagation and calculate the loss for the modified network weights ($J_+$).\n",
    "    - Subtract a small epsilon from the weight/bias element of interest to create another modified set of network weights.\n",
    "    - Perform forward propagation and calculate the loss of these modified network weights ($J_-$).\n",
    "    - Compute the approximate gradient for the element of interest as $\\frac{J_+ - J_-}{2 * \\epsilon}$.\n",
    "- Once you have computed an approximate gradient for each network weight, compare your approximate gradient to your backpropagated gradient to verify that they approximately match.\n",
    "\n",
    "To simplify your implementation, we will provide utility functions for flattening all network weights into a vector and extracting the network weights from a flattened vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1dea04790ab50b5586b15e56a7dcc249",
     "grade": false,
     "grade_id": "cell-31864df829254e48",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def flattened(W1, W2, W3, b1, b2, b3):\n",
    "    V = np.concatenate((W1.flatten(), W2.flatten(), W3.flatten(), b1.flatten(), b2.flatten(), b3.flatten()))\n",
    "    return V\n",
    "\n",
    "def reshaped(V, n0, n1, n2, n3):\n",
    "    t1 = n0*n1\n",
    "    t2 = t1 + n1*n2\n",
    "    t3 = t2 + n2*n3\n",
    "    t4 = t3 + n1\n",
    "    t5 = t4 + n2\n",
    "    W1 = V[:t1].reshape(n0, n1)\n",
    "    W2 = V[t1:t2].reshape(n1, n2)\n",
    "    W3 = V[t2:t3].reshape(n2, n3)\n",
    "    b1 = V[t3:t4].reshape(1, n1)\n",
    "    b2 = V[t4:t5].reshape(1, n2)\n",
    "    b3 = V[t5:].reshape(1, n3)\n",
    "    return W1, W2, W3, b1, b2, b3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2b86e5f8abe43daaaee77ffe413bd9d0",
     "grade": false,
     "grade_id": "cell-adfe37aab91a69f3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def gradient_checking(W1, W2, W3, b1, b2, b3, X, Y, lambd, epsilon = 1e-8):\n",
    "    '''\n",
    "    Performs gradient checking to verify that the backpropagation function is correct.\n",
    "    '''\n",
    "    \n",
    "    n0, n1, n2, n3 = W1.shape[0], W2.shape[0], W3.shape[0], W3.shape[1]\n",
    "    (A1, A2, A3) = forward_propagation(W1, W2, W3, b1, b2, b3, X)\n",
    "    dW1, dW2, dW3, db1, db2, db3 = back_propagation(W1, W2, W3, b1, b2, b3, A1, A2, A3, X, Y, lambd)\n",
    "    V = flattened(W1, W2, W3, b1, b2, b3)\n",
    "    grad = flattened(dW1, dW2, dW3, db1, db2, db3)\n",
    "    grad_approx = np.zeros_like(grad)\n",
    "    \n",
    "    for i in range(len(V)):\n",
    "        \n",
    "        V_plus = V.copy()\n",
    "        V_plus[i] += epsilon\n",
    "        V_minus = V.copy()\n",
    "        V_minus[i] -= epsilon\n",
    "        \n",
    "        #Calculate the slightly shifted weights\n",
    "        W1_plus, W2_plus, W3_plus, b1_plus, b2_plus, b3_plus = reshaped(V_plus, n0, n1, n2, n3)\n",
    "        W1_minus, W2_minus, W3_minus, b1_minus, b2_minus, b3_minus = reshaped(V_minus, n0, n1, n2, n3)\n",
    "        #Forward propagate\n",
    "        A1p, A2p, A3p = forward_propagation(W1_plus, W2_plus, W3_plus, b1_plus, b2_plus, b3_plus, X)\n",
    "        A1m, A2m, A3m = forward_propagation(W1_minus, W2_minus, W3_minus, b1_minus, b2_minus, b3_minus, X)\n",
    "        \n",
    "        #Find losses for each\n",
    "        lossp = cross_entropy_loss_with_regularization(A3p, Y, W1_plus, W2_plus, W3_plus, lambd)\n",
    "        lossm = cross_entropy_loss_with_regularization(A3m, Y, W1_minus, W2_minus, W3_minus, lambd)\n",
    "        \n",
    "        #Calculate the approximate grad based on the difference in losses\n",
    "        a_gradval = (lossp-lossm)/(2*epsilon)\n",
    "        grad_approx[i] = a_gradval\n",
    "        \n",
    "    err = np.linalg.norm(grad - grad_approx)\n",
    "    rel_err = err/(np.linalg.norm(grad) + np.linalg.norm(grad_approx))\n",
    "    print(\"Relative error in gradient approximation: {}\".format(rel_err))\n",
    "    if rel_err < 1e-7:\n",
    "        print('Passes gradient checking')\n",
    "    else:\n",
    "        print('Fails gradient checking')\n",
    "    return rel_err, grad, grad_approx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5762d903a8b2f03d953c517db31322f3",
     "grade": false,
     "grade_id": "cell-19d5a4352b92062e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative error in gradient approximation: 2.4243895879131173e-08\n",
      "Passes gradient checking\n"
     ]
    }
   ],
   "source": [
    "W1, W2, W3 = initialize_weights(n0, n1, n2, n3)\n",
    "b1, b2, b3 = initialize_bias(n0, n1, n2, n3)\n",
    "lambd = .01\n",
    "maxerror, g, ga = gradient_checking(W1, W2, W3, b1, b2, b3, X, Y, lambd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e5c16d6b34fce26c05319c9a3425b8c1",
     "grade": true,
     "grade_id": "cell-b289facaa43db6ec",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert maxerror < 1e-7, \"Gradient checking fails.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Updating network weights\n",
    "\n",
    "The network weights are updated as follows:\n",
    "\n",
    "$W_1 = W_1 - \\alpha * \\frac{dJ}{dW_1}$  \n",
    "\n",
    "$W_2 = W_2 - \\alpha * \\frac{dJ}{dW_2}$  \n",
    "\n",
    "$W_3 = W_3 - \\alpha * \\frac{dJ}{dW_3}$  \n",
    "\n",
    "$b_1 = b_1 - \\alpha * \\frac{dJ}{db_1}$  \n",
    "\n",
    "$b_2 = b_2 - \\alpha * \\frac{dJ}{db_2}$\n",
    "\n",
    "$b_3 = b_3 - \\alpha * \\frac{dJ}{db_3}$\n",
    "\n",
    "where $\\alpha$ is the learning rate and the gradients are estimated across a batch of training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a7a3f8e5b1f489d57856f9718e62dc81",
     "grade": false,
     "grade_id": "cell-74a044a2af474a7d",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def update_parameters(W1, W2, W3, b1, b2, b3, dW1, dW2, dW3, db1, db2, db3, alpha):\n",
    "    '''\n",
    "    Returns the weights after a single gradient descent update.\n",
    "    dW1, dW2, dW3, db1, db2, db3 are the gradients averaged across a batch of training samples.\n",
    "    W1, W2, W3, b1, b2, b3 are the network weights, which should be updated and returned.\n",
    "    '''\n",
    "    \n",
    "    # Modify weights based on learning rates and grads\n",
    "    W1 = W1-alpha*dW1\n",
    "    W2 = W2-alpha*dW2\n",
    "    W3 = W3-alpha*dW3\n",
    "    b1 = b1-alpha*db1\n",
    "    b2 = b2-alpha*db2\n",
    "    b3 = b3-alpha*db3\n",
    "    return (W1, W2, W3, b1, b2, b3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "db130d28b3e6272a35277e5c773bb97d",
     "grade": false,
     "grade_id": "cell-de672d58d3aab116",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "alpha = .1\n",
    "W1, W2, W3 = initialize_weights(n0, n1, n2, n3)\n",
    "b1, b2, b3 = initialize_bias(n0, n1, n2, n3)\n",
    "(A1, A2, A3) = forward_propagation(W1, W2, W3, b1, b2, b3, X)\n",
    "lambd = .01\n",
    "dW1, dW2, dW3, db1, db2, db3 = back_propagation(W1, W2, W3, b1, b2, b3, A1, A2, A3, X, Y, lambd)\n",
    "W1, W2, W3, b1, b2, b3 = update_parameters(W1, W2, W3, b1, b2, b3, dW1, dW2, dW3, db1, db2, db3, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e815afddc6bb07e2f8f094fe2faaf617",
     "grade": true,
     "grade_id": "cell-669871e75c09473",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(W1, np.ndarray), \"W1 should be a numpy array.\"\n",
    "assert isinstance(W2, np.ndarray), \"W2 should be a numpy array.\"\n",
    "assert isinstance(W3, np.ndarray), \"W3 should be a numpy array.\"\n",
    "\n",
    "assert W1.shape[0] == n0, \"The number of rows in W1 is incorrect.\"\n",
    "assert W1.shape[1] == n1, \"The number of columns in W1 is incorrect.\"\n",
    "\n",
    "assert W2.shape[0] == n1, \"The number of rows in W2 is incorrect.\"\n",
    "assert W2.shape[1] == n2, \"The number of columns in W2 is incorrect.\"\n",
    "\n",
    "assert W3.shape[0] == n2, \"The number of rows in W3 is incorrect.\"\n",
    "assert W3.shape[1] == n3, \"The number of columns in W3 is incorrect.\"\n",
    "\n",
    "assert isinstance(b1, np.ndarray), \"b1 should be a numpy array.\"\n",
    "assert isinstance(b2, np.ndarray), \"b2 should be a numpy array.\"\n",
    "assert isinstance(b3, np.ndarray), \"b3 should be a numpy array.\"\n",
    "\n",
    "assert b1.shape[0] == 1, \"The number of rows in b1 is incorrect.\"\n",
    "assert b1.shape[1] == n1, \"The number of columns in b1 is incorrect.\"\n",
    "\n",
    "assert b2.shape[0] == 1, \"The number of rows in b2 is incorrect.\"\n",
    "assert b2.shape[1] == n2, \"The number of columns in b2 is incorrect.\"\n",
    "\n",
    "assert b3.shape[0] == 1, \"The number of rows in b3 is incorrect.\"\n",
    "assert b3.shape[1] == n3, \"The number of columns in b3 is incorrect.\"\n",
    "\n",
    "assert np.allclose(np.round(W1[0,:], decimals=8), np.array([1.24932316, 0.28202442, 0.7071113 ])), \"Incorrect values for W1.\"\n",
    "assert np.allclose(np.round(b1[0,:], decimals=8), np.array([ 0.01185044,  0.01587577, -0.08516145])), \"Incorrect values for b1.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "be0a4ebf231f94bd76e1dd206dffb2c0",
     "grade": false,
     "grade_id": "cell-9584d9af97aac2af",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Training\n",
    "\n",
    "In this function we will bring all the functions together to update the network weights using mini-batch gradient descent.  There are three main differences from the batch gradient descent training function you implemented last week:\n",
    "1.  Instead of estimating the gradients on the entire training set, we will estimate the gradients and update the weights on small batches of samples.  This means that we will make many updates to the weights in a single epoch.\n",
    "2.  After each epoch, we will shuffle the order of the training samples.  This avoids learning characteristics that are specific to particular groupings of data points in the batches.\n",
    "3.  After each epoch, we will also evaluate the performance of the neural network on a separate validation set.  This is necessary to identify how much we are overfitting to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7b972a16f18777fce422b1452098ec66",
     "grade": false,
     "grade_id": "cell-1d845cae609b7243",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def training(X_train, Y_train, X_val, Y_val, n1, n2, batch_size, alpha, lambd, num_epochs):\n",
    "    '''\n",
    "    Learn the network weights on a set of training examples using mini-batch gradient descent.\n",
    "    \n",
    "    Inputs\n",
    "    ------\n",
    "    X_train: a matrix containing the training data, where each row corresponds to a single training example\n",
    "    Y_train: a matrix containing the training target labels, where each row contains a one hot encoded target label.\n",
    "    X_val: a matrix containing the validation data\n",
    "    Y_val: a matrix containing the validation target labels\n",
    "    n1: the number of units in the first hidden layer\n",
    "    n2: the number of units in the second hidden layer\n",
    "    batch_size: size of each batch used in mini-batch gradient descent\n",
    "    alpha: the learning rate for gradient descent\n",
    "    lambd: hyperparameter specifying the weight of the regularization term\n",
    "    num_epochs: the number of times to iterate over the dataset\n",
    "    \n",
    "    Returns a tuple of matrices (W1, W2, W3, b1, b2, b3, hist) containing the learned network weights,\n",
    "    along with a history of the training and validation loss values after each training epoch.\n",
    "    '''\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    n0 = X_train.shape[1] \n",
    "    n3 = Y_train.shape[1]\n",
    "    N = X_train.shape[0]\n",
    "    \n",
    "    W1, W2, W3 = initialize_weights(n0, n1, n2, n3)\n",
    "    b1, b2, b3 = initialize_bias(n0, n1, n2, n3)\n",
    "    hist = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        train_idxs_all = np.arange(N) # shuffle training data\n",
    "        np.random.shuffle(train_idxs_all) \n",
    "        train_loss = 0 # accumulate losses from mini-batches \n",
    "        \n",
    "        for i in range(0, N, batch_size):\n",
    "            \n",
    "            idxs_batch = train_idxs_all[i:min(i+batch_size,N)]\n",
    "            X_batch = X_train[idxs_batch,:]\n",
    "            Y_batch = Y_train[idxs_batch,:]\n",
    "            \n",
    "            #Forward propagate for predictions/intermediate vals\n",
    "            (A1, A2, A3) = forward_propagation(W1, W2, W3, b1, b2, b3, X_batch)\n",
    "            \n",
    "            #backpropagate for grads\n",
    "            dW1,dW2,dW3,db1,db2,db3 = back_propagation(W1, W2, W3, b1, b2, b3, A1, A2, A3, X_batch, Y_batch, lambd)\n",
    "            \n",
    "            #Calculate the loss\n",
    "            batch_loss = cross_entropy_loss_with_regularization(A3, Y_batch, W1, W2, W3, lambd)\n",
    "            \n",
    "            #Add it to total loss\n",
    "            train_loss += batch_loss * X_batch.shape[0]\n",
    "            \n",
    "            #Update params\n",
    "            W1, W2, W3, b1, b2, b3 = update_parameters(W1, W2, W3, b1, b2, b3, dW1, dW2, dW3, db1, db2, db3, alpha)\n",
    "            \n",
    "        train_loss = train_loss / N\n",
    "        \n",
    "        #Calculate valid loss\n",
    "        A1,A2,A3 = forward_propagation(W1,W2,W3,b1,b2,b3,X_val)\n",
    "        val_loss = cross_entropy_loss_with_regularization(A3, Y_val, W1, W2, W3, lambd)\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print('Epoch {}: Training Loss = {:.6f}, Validation Loss = {:.6f}'.format(epoch+1, train_loss, val_loss))    \n",
    "        hist.append((train_loss, val_loss))\n",
    "    \n",
    "    return W1, W2, W3, b1, b2, b3, hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "11db266daba5e6518a32cc2de8d0626b",
     "grade": false,
     "grade_id": "cell-993e7f2f2d75ae32",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Training Loss = 0.842556, Validation Loss = 0.833713\n",
      "Epoch 20: Training Loss = 0.816144, Validation Loss = 0.890102\n",
      "Epoch 30: Training Loss = 0.790352, Validation Loss = 0.795190\n",
      "Epoch 40: Training Loss = 0.770361, Validation Loss = 0.764392\n",
      "Epoch 50: Training Loss = 0.755709, Validation Loss = 0.738133\n"
     ]
    }
   ],
   "source": [
    "W1, W2, W3 = initialize_weights(n0, n1, n2, n3)\n",
    "alpha = .1\n",
    "lambd = .01\n",
    "W1, W2, W3, b1, b2, b3, _ = training(X_train, Y_train, X_val, Y_val, n1, n2, m, alpha, lambd, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b41ae096e784ecf789febcaeb087210b",
     "grade": true,
     "grade_id": "cell-a5fc87554a198e20",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(b1, np.ndarray), \"b1 should be a numpy array.\"\n",
    "assert isinstance(b2, np.ndarray), \"b2 should be a numpy array.\"\n",
    "assert isinstance(b3, np.ndarray), \"b3 should be a numpy array.\"\n",
    "\n",
    "assert b1.shape[0] == 1, \"The number of rows in b1 is incorrect.\"\n",
    "assert b1.shape[1] == n1, \"The number of columns in b1 is incorrect.\"\n",
    "\n",
    "assert b2.shape[0] == 1, \"The number of rows in b2 is incorrect.\"\n",
    "assert b2.shape[1] == n2, \"The number of columns in b2 is incorrect.\"\n",
    "\n",
    "assert b3.shape[0] == 1, \"The number of rows in b3 is incorrect.\"\n",
    "assert b3.shape[1] == n3, \"The number of columns in b3 is incorrect.\"\n",
    "\n",
    "assert isinstance(W1, np.ndarray), \"W1 should be a numpy array.\"\n",
    "assert isinstance(W2, np.ndarray), \"W2 should be a numpy array.\"\n",
    "assert isinstance(W3, np.ndarray), \"W3 should be a numpy array.\"\n",
    "\n",
    "assert W1.shape[0] == n0, \"The number of rows in W1 is incorrect.\"\n",
    "assert W1.shape[1] == n1, \"The number of columns in W1 is incorrect.\"\n",
    "\n",
    "assert W2.shape[0] == n1, \"The number of rows in W2 is incorrect.\"\n",
    "assert W2.shape[1] == n2, \"The number of columns in W2 is incorrect.\"\n",
    "\n",
    "assert W3.shape[0] == n2, \"The number of rows in W3 is incorrect.\"\n",
    "assert W3.shape[1] == n3, \"The number of columns in W3 is incorrect.\"\n",
    "\n",
    "assert np.allclose(np.round(W1[0,:], decimals=8), np.array([ 1.00874988, -0.17402805,  0.39194318])), \"Incorrect learned weights for W1.\"\n",
    "assert np.allclose(np.round(b1[0,:], decimals=8), np.array([1.35886379, 0.19199265, 0.54218461])), \"Incorrect learned values for b1.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "33b706887ba9832b07a4a78766f8a508",
     "grade": false,
     "grade_id": "cell-7bc30eaf343fd46c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "##### Prediction\n",
    "\n",
    "Implement a function to make predictions on a batch of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4026830516f9714365f7b2bedcd773ca",
     "grade": false,
     "grade_id": "cell-270ecea5c6c3df85",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def predict(W1, W2, W3, b1, b2, b3, X):\n",
    "    '''\n",
    "    Returns the predicted labels 'Y' and class probabilities 'P' for a set of data samples.\n",
    "    \n",
    "    X is a matrix specifying the data, where each row corresponds to a single data sample.\n",
    "    Y is a vector of integers specifying the predicted class labels.\n",
    "    P is matrix of probabilities, where each row specifies the class probabilities for a single data sample.\n",
    "    '''\n",
    "    \n",
    "    (A1, A2, P) = forward_propagation(W1, W2, W3, b1, b2, b3, X)\n",
    "    Y = np.argmax(P, axis = 1)\n",
    "    \n",
    "    return Y, P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "54876e280311f565e07a4116aa9822f9",
     "grade": false,
     "grade_id": "cell-701c72964f8c67c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "Y_pred, P = predict(W1, W2, W3, b1, b2, b3, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f17a6779757e1995cfad502cb29a5af2",
     "grade": true,
     "grade_id": "cell-0f07e123d68eeff3",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(Y_pred, np.ndarray) and Y_pred.shape == (X.shape[0],), \"Y should be a numpy array of shape (m,)\"\n",
    "assert isinstance(P, np.ndarray) and P.shape == (X.shape[0], n3), \"P should be a numpy array of shape (m, n3).\"\n",
    "assert np.array_equal(Y_pred[5:10], np.array([1, 2, 0, 2, 1])), \"Incorrect predicted labels.\"\n",
    "assert np.allclose(np.round(P[15,:], decimals=8), np.array([0.01476187, 0.64671651, 0.33852162])), \"Incorrect class probabilities.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c27b07ed63243671574155de53be400b",
     "grade": false,
     "grade_id": "cell-df08f7ee3ea1fa85",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Part 2\n",
    "\n",
    "In the second part of this assignment, you will implement and incorporate dropout into your neural network.  Dropout is a powerful technique for reducing overfitting that has become a standard tool in the literature.\n",
    "\n",
    "Dropout is usually applied to the output of a hidden layer.  A dropout layer is unique in that it has different behavior during training and during testing:\n",
    "- During training, it will randomly ignore or \"drop out\" some of the layer outputs on each training sample with some probability $p$.  This makes the layer appear as if it has a different number of nodes with different connections to the previous and succeeding layers.  This forces the network to make good predictions even when some of its nodes are absent, which makes the network more robust.  To account for the fact that fewer units are active, the layer outputs are scaled by $\\frac{1}{1-p}$.  Note that if there is no dropout (i.e. $p=0$), then all units are active and the layer outputs are scaled by $1$.  If the dropout rate is $p=0.5$, then on average half of the units will be active and the layer outputs are scaled by $2$.\n",
    "- During testing, all of the layer outputs are kept and the layer outputs are not scaled.  This is equivalent to applying dropout with $p=0$.\n",
    "\n",
    "In the functions below, we will apply dropout to the outputs of both hidden layers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9d6cd9e9f06ff6cd52f7a6fee270b67a",
     "grade": false,
     "grade_id": "cell-553d2b70587a1115",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "##### Forward propagation with dropout\n",
    "\n",
    "Implement forward propagation with dropout.  Dropout can be implemented in the following way:\n",
    "\n",
    "* Calculate the hidden layer activations `A1`, which is a matrix of shape `(m, n1)`.\n",
    "* Generate a binary matrix `D1` to apply dropout.  `D1` has the same shape as `A1` and each of its elements is a random Bernoulli variable.  You can implement this by creating a matrix of random values uniformly distributed between $0$ and $1$ using [`np.random.random()`](https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.random.random.html), and then thresholding at a value `keep_prob`.  Note: our autograding tests assume that the entries of D1 and D2 are floats, not boolean. \n",
    "* Multiply `A1` and `D1` elementwise to drop out some of the nodes in `A1` and normalize the result by `keep_prob`.\n",
    "* Repeat the above steps with dropout for calculating `A2`.\n",
    "\n",
    "Note that dropout should not be applied to the final output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "51f65f41e6ffa036e19b895e7f48d55d",
     "grade": false,
     "grade_id": "cell-0ded528c4ccfbd84",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def forward_propagation_with_dropout(W1, W2, W3, b1, b2, b3, X, keep_prob, seed = None):\n",
    "    '''\n",
    "    Performs forward propagation on a batch of inputs, where dropout is applied to the output of both hidden layers.\n",
    "    X is a matrix containing a batch of inputs, where each row corresponds to a single data sample.\n",
    "    keep_prob specifies the probability of keeping a node in dropout.\n",
    "    seed is for the random number generator.  Specify a value if you want a deterministic output (e.g. for gradient checking)\n",
    "    \n",
    "    Returns a tuple of matrices (A1, A2, A3, D1, D2) containing the activations at the hidden and output layers\n",
    "    for all training examples in X, as well as the binary masks for applying dropout.  The activations A1 and A2\n",
    "    should be taken before dropout is applied.\n",
    "    '''\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "    normFact = 1/keep_prob\n",
    "    \n",
    "    A1 = ReLU(np.matmul(X, W1)+b1)\n",
    "    randomD1 = np.random.random(A1.shape)\n",
    "    D1 = np.where(randomD1<keep_prob, 1,0)\n",
    "    A2 = ReLU(np.matmul(np.multiply(A1, D1)*normFact, W2)+b2)\n",
    "    randomD2 = np.random.random(A2.shape)\n",
    "    D2 = np.where(randomD2<keep_prob, 1,0)\n",
    "    A3 = softmax(np.matmul(np.multiply(A2, D2)*normFact, W3)+b3)\n",
    "    \n",
    "    return (A1, A2, A3, D1, D2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5bba35de624246a5b1f901b4ca746365",
     "grade": false,
     "grade_id": "cell-46ce774ee00d4f0e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "keep_prob=0.8\n",
    "W1, W2, W3 = initialize_weights(n0, n1, n2, n3)\n",
    "b1, b2, b3 = initialize_bias(n0, n1, n2, n3)\n",
    "(A1, A2, A3, D1, D2) = forward_propagation_with_dropout(W1, W2, W3, b1, b2, b3, X, keep_prob, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "20872d41f1f0c8ab09f3ef638018cacd",
     "grade": true,
     "grade_id": "cell-c67aaf0218c689fc",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert A1.shape[0] == m, \"The number of rows in A1 is incorrect.\"\n",
    "assert A1.shape[1] == n1, \"The number of columns in A1 is incorrect.\"\n",
    "\n",
    "assert A2.shape[0] == m, \"The number of rows in A2 is incorrect.\"\n",
    "assert A2.shape[1] == n2, \"The number of columns in A2 is incorrect.\"\n",
    "\n",
    "assert A3.shape[0] == m, \"The number of rows in A3 is incorrect.\"\n",
    "assert A3.shape[1] == n3, \"The number of columns in A3 is incorrect.\"\n",
    "\n",
    "assert np.allclose(np.round( A1[0], decimals=8), np.array( [0.        , 0.        , 1.00318339] )), \"Incorrect values for A1 .\"\n",
    "assert np.allclose(np.round( A2[0], decimals=8), np.array( [0.89507008, 0.        , 2.33393312, 0.        , 0.05685076] )), \"Incorrect values for  A2 .\"\n",
    "assert np.allclose(np.round( A3[0], decimals=8), np.array( [0.00442725, 0.2941838 , 0.70138895] )), \"Incorrect values for  A3 .\"\n",
    "assert np.allclose(np.round( D1[0], decimals=8), np.array( [1, 1, 1] )), \"Incorrect values for  D1 .\"\n",
    "assert np.allclose(np.round( D2[0], decimals=8), np.array( [1, 0, 1, 1, 1] )), \"Incorrect values for  D2 .\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ec130c3b8724b03419fce9f611aaa0a8",
     "grade": false,
     "grade_id": "cell-d80e28a0b07d3e91",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "##### Backward propagation with dropout\n",
    "\n",
    "Implement backward propagation with dropout.  Think about how elementwise multiplication with a binary mask matrix or a constant scalar will affect your backpropagation equations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4526fea1486f84415358eaf5a64023dd",
     "grade": false,
     "grade_id": "cell-c5b22dc09f94ad45",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def back_propagation_with_dropout(W1, W2, W3, b1, b2, b3, A1, A2, A3, D1, D2, X, Y, lambd, keep_prob):\n",
    "    '''\n",
    "    Performs backpropagation on a batch of training samples, where dropout has been applied to the output of both hidden layers.\n",
    "    X is a matrix containing a batch of data, where each row corresponds to a single data sample.\n",
    "    Y is a matrix containing the target labels, where each row corresponds to a one hot encoded label.\n",
    "    D1 and D2 specify the binary mask matrices used to apply dropout to the current batch of samples.\n",
    "    lambd is a hyperparameter specifying the weight of the regularization term.\n",
    "    keep_prob specifies the probability of keeping a node when applying dropout.\n",
    "    \n",
    "    Returns a tuple of matrices (dW1, dW2, dW3, db1, db2, db3) containing the gradients averaged across the batch.\n",
    "    '''\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    \n",
    "    #Save shapes of biases\n",
    "    db1Shape = b1.shape\n",
    "    db2Shape = b2.shape\n",
    "    db3Shape = b3.shape\n",
    "    \n",
    "    #Calculate a normalization factor for dropout layers\n",
    "    normFact = 1/keep_prob\n",
    "\n",
    "    #Find intermediate grads with chain rule\n",
    "    da3 = -Y*np.reciprocal(A3)\n",
    "    dz3 = -Y+A3\n",
    "    db3 =  np.reshape(np.sum(dz3, axis=0)/m, db3Shape)\n",
    "    dW3 = (np.matmul(np.multiply(A2, D2).T * normFact, dz3) + lambd*W3)/m\n",
    "    dd2 = D2 * normFact\n",
    "    da2 = np.multiply(dd2, np.matmul(dz3, W3.T))\n",
    "    dz2 = np.multiply(da2, np.where(A2>0, 1, 0))\n",
    "    db2 =  np.reshape(np.sum(dz2, axis=0)/m, db2Shape)\n",
    "    dW2 = (np.matmul(np.multiply(A1, D1).T * normFact, dz2) + lambd*W2)/m\n",
    "    dd1 = D1 * normFact\n",
    "    da1 = np.multiply(dd1, np.matmul(dz2, W2.T))\n",
    "    dz1 = np.multiply(da1, np.where(A1>0, 1, 0))\n",
    "    db1 =  np.reshape(np.sum(dz1, axis=0)/m, db1Shape)\n",
    "    dW1 = (np.matmul(X.T,dz1) + lambd*W1)/m\n",
    "    \n",
    "    return (dW1, dW2, dW3, db1, db2, db3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3744d31fbbf5214b561cb093f61d452a",
     "grade": false,
     "grade_id": "cell-6adf7ad55a14b27d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "keep_prob=0.8\n",
    "lambd=.01\n",
    "W1, W2, W3 = initialize_weights(n0, n1, n2, n3)\n",
    "b1, b2, b3 = initialize_bias(n0, n1, n2, n3)\n",
    "(A1, A2, A3, D1, D2) = forward_propagation_with_dropout(W1, W2, W3, b1, b2, b3, X, keep_prob)\n",
    "dW1, dW2, dW3, db1, db2, db3 = back_propagation_with_dropout(W1, W2, W3, b1, b2, b3, A1, A2, A3, D1, D2, X, Y, lambd, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7292627415cd6df2f7dc40b235e6a359",
     "grade": true,
     "grade_id": "cell-4253c7acb8ae128a",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(dW1, np.ndarray), \"dW1 should be a numpy array.\"\n",
    "assert isinstance(dW2, np.ndarray), \"dW2 should be a numpy array.\"\n",
    "assert isinstance(dW3, np.ndarray), \"dW3 should be a numpy array.\"\n",
    "\n",
    "assert dW3.shape[0] == n2, \"The number of rows in dW3 is incorrect.\"\n",
    "assert dW3.shape[1] == n3, \"The number of columns in dW3 is incorrect.\"\n",
    "\n",
    "assert dW2.shape[0] == n1, \"The number of rows in dW2 is incorrect.\"\n",
    "assert dW2.shape[1] == n2, \"The number of columns in dW2 is incorrect.\"\n",
    "\n",
    "assert dW1.shape[0] == n0, \"The number of rows in dW1 is incorrect.\"\n",
    "assert dW1.shape[1] == n1, \"The number of columns in dW1 is incorrect.\"\n",
    "\n",
    "\n",
    "assert isinstance(db1, np.ndarray), \"db1 should be a numpy array.\"\n",
    "assert isinstance(db2, np.ndarray), \"db2 should be a numpy array.\"\n",
    "assert isinstance(db3, np.ndarray), \"db3 should be a numpy array.\"\n",
    "\n",
    "assert db3.shape[0] == 1, \"The number of rows in db3 is incorrect.\"\n",
    "assert db3.shape[1] == n3, \"The number of columns in db3 is incorrect.\"\n",
    "\n",
    "assert db2.shape[0] == 1, \"The number of rows in db2 is incorrect.\"\n",
    "assert db2.shape[1] == n2, \"The number of columns in db2 is incorrect.\"\n",
    "\n",
    "assert db1.shape[0] == 1, \"The number of rows in db1 is incorrect.\"\n",
    "assert db1.shape[1] == n1, \"The number of columns in db1 is incorrect.\"\n",
    "\n",
    "assert np.allclose(np.round( dW3[0], decimals=8), np.array( [-0.55225924, -0.07843022,  0.63104553] )), \"Incorrect values for dW3 .\"\n",
    "assert np.allclose(np.round( dW2[0], decimals=8), np.array( [ 0.11682459,  0.12856525,  0.04817769, -0.04148614,  0.04113111] )), \"Incorrect values for  dW2 .\"\n",
    "assert np.allclose(np.round( dW1[0], decimals=8), np.array( [-0.02193494, -0.01868819,  0.17371224])), \"Incorrect values for  dW1 .\"\n",
    "assert np.allclose(np.round( db3[0], decimals=8), np.array( [-0.27999885,  0.01612187,  0.26387698] )), \"Incorrect values for  db3 .\"\n",
    "assert np.allclose(np.round( db2[0], decimals=8), np.array( [ 0.39026195,  0.06514925,  0.4973931 , -0.03334392, -0.20734988] )), \"Incorrect values for  db2 .\"\n",
    "assert np.allclose(np.round( db1[0], decimals=8), np.array( [ 0.03812913, -0.07847543,  1.14225253] )), \"Incorrect values for  db1 .\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a752336200469547f0602318c377e97e",
     "grade": false,
     "grade_id": "cell-2788c5bd712338f6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's do gradient checking to verify our implementation of backpropagation with dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "affb336cb90ec980d38c021441c9f883",
     "grade": false,
     "grade_id": "cell-fd2fb743ae58607f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def gradient_checking_with_dropout(W1, W2, W3, b1, b2, b3, X, Y, lambd, keep_prob, epsilon = 1e-8):\n",
    "    '''\n",
    "    Performs gradient checking to verify that the backpropagation with dropout is correct.\n",
    "    When doing forward propagation, make sure to set the random seed so the computation is deterministic.\n",
    "    '''\n",
    "    \n",
    "    n0, n1, n2, n3 = W1.shape[0], W2.shape[0], W3.shape[0], W3.shape[1]\n",
    "    (A1, A2, A3, D1, D2) = forward_propagation_with_dropout(W1, W2, W3, b1, b2, b3, X, keep_prob, 0)\n",
    "    dW1, dW2, dW3, db1, db2, db3 = back_propagation_with_dropout(W1, W2, W3, b1, b2, b3, A1, A2, A3, D1, D2, X, Y, lambd, keep_prob)\n",
    "    V = flattened(W1, W2, W3, b1, b2, b3)\n",
    "    grad = flattened(dW1, dW2, dW3, db1, db2, db3)\n",
    "    grad_approx = np.zeros_like(grad)\n",
    "    \n",
    "    for i in range(len(V)):\n",
    "        \n",
    "        V_plus = V.copy()\n",
    "        V_plus[i] += epsilon\n",
    "        V_minus = V.copy()\n",
    "        V_minus[i] -= epsilon\n",
    "        \n",
    "        W1_plus, W2_plus, W3_plus, b1_plus, b2_plus, b3_plus = reshaped(V_plus, n0, n1, n2, n3)\n",
    "        W1_minus, W2_minus, W3_minus, b1_minus, b2_minus, b3_minus = reshaped(V_minus, n0, n1, n2, n3)\n",
    "        \n",
    "        A1p, A2p, A3p, D1p, D2p = forward_propagation_with_dropout(W1_plus, W2_plus, W3_plus, b1_plus, b2_plus, b3_plus, X, keep_prob, 0)\n",
    "        A1m, A2m, A3m, D1p, D2p = forward_propagation_with_dropout(W1_minus, W2_minus, W3_minus, b1_minus, b2_minus, b3_minus, X, keep_prob, 0)\n",
    "        \n",
    "        lossp = cross_entropy_loss_with_regularization(A3p, Y, W1_plus, W2_plus, W3_plus, lambd)\n",
    "        lossm = cross_entropy_loss_with_regularization(A3m, Y, W1_minus, W2_minus, W3_minus, lambd)\n",
    "        a_gradval = (lossp-lossm)/(2*epsilon)\n",
    "        grad_approx[i] = a_gradval\n",
    "    \n",
    "    \n",
    "    err = np.linalg.norm(grad - grad_approx)\n",
    "    rel_err = err/(np.linalg.norm(grad) + np.linalg.norm(grad_approx))\n",
    "    print(\"Relative error in gradient approximation: {}\".format(rel_err))\n",
    "    if rel_err < 1e-7:\n",
    "        print('Passes gradient checking')\n",
    "    else:\n",
    "        print('Fails gradient checking')\n",
    "    return rel_err, grad, grad_approx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3140801996ce3a57e6efa60f268eed84",
     "grade": false,
     "grade_id": "cell-6a8dc2cb614e8756",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative error in gradient approximation: 2.3907054916213286e-08\n",
      "Passes gradient checking\n"
     ]
    }
   ],
   "source": [
    "W1, W2, W3 = initialize_weights(n0, n1, n2, n3)\n",
    "b1, b2, b3 = initialize_bias(n0, n1, n2, n3)\n",
    "lambd = 0 # .01\n",
    "keep_prob = 1\n",
    "rel_err, g, ga = gradient_checking_with_dropout(W1, W2, W3, b1, b2, b3, X, Y, lambd, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "639378c3dd3a0279eeeff5cc243c2f89",
     "grade": true,
     "grade_id": "cell-10dee55611bc4509",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert maxerror < 1e-7, \"Gradient checking fails.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction with dropout\n",
    "\n",
    "At test time, the forward propagation is identical to a model without dropout.  So we can simply reuse our `predict()` function from part 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "be3374f2b32c137420dd649bf35983c3",
     "grade": false,
     "grade_id": "cell-b46dd3d6d7060f5b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "##### Training with dropout\n",
    "\n",
    "Re-implement the training function for updating the weights for the network with dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d1ca86df32568a245192715a7e54378f",
     "grade": false,
     "grade_id": "cell-42886100828ecf8e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def training_with_dropout(X_train, Y_train, X_val, Y_val, n1, n2, batch_size, alpha, lambd, keep_prob, num_epochs):\n",
    "    '''\n",
    "    Learn the network weights on a set of training examples using mini-batch gradient descent.\n",
    "    In this network, dropout has been applied to the output of both hidden layers.\n",
    "    \n",
    "    Inputs\n",
    "    ------\n",
    "    X_train: a matrix containing the training data, where each row corresponds to a single training example\n",
    "    Y_train: a matrix containing the training target labels, where each row contains a one hot encoded target label.\n",
    "    X_val: a matrix containing the validation data\n",
    "    Y_val: a matrix containing the validation target labels\n",
    "    n1: the number of units in the first hidden layer\n",
    "    n2: the number of units in the second hidden layer\n",
    "    batch_size: size of each batch used in mini-batch gradient descent\n",
    "    alpha: the learning rate for gradient descent\n",
    "    lambd: hyperparameter specifying the weight of the regularization term\n",
    "    keep_prob: a scalar specifying the amount of dropout to apply at the output of both hidden layers\n",
    "    num_epochs: the number of times to iterate over the dataset\n",
    "    \n",
    "    Returns a tuple of matrices (W1, W2, W3, b1, b2, b3, hist) containing the learned network weights,\n",
    "    along with a history of the training and validation loss values after each training epoch.\n",
    "    '''\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    n0 = X_train.shape[1] \n",
    "    n3 = Y_train.shape[1]\n",
    "    N = X_train.shape[0]\n",
    "    \n",
    "    W1, W2, W3 = initialize_weights(n0, n1, n2, n3)\n",
    "    b1, b2, b3 = initialize_bias(n0, n1, n2, n3)\n",
    "    hist = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        train_idxs_all = np.arange(N) # shuffle training data\n",
    "        np.random.shuffle(train_idxs_all) \n",
    "        train_loss = 0 # accumulate losses from mini-batches \n",
    "        \n",
    "        for i in range(0, N, batch_size):\n",
    "            \n",
    "            idxs_batch = train_idxs_all[i:min(i+batch_size,N)]\n",
    "            X_batch = X_train[idxs_batch,:]\n",
    "            Y_batch = Y_train[idxs_batch,:]\n",
    "            \n",
    "            #Forward prop for intermediate vals/predictions\n",
    "            (A1, A2, A3, D1, D2) = forward_propagation_with_dropout(W1, W2, W3, b1, b2, b3, X_batch, keep_prob)\n",
    "            #Back prop for grads\n",
    "            dW1, dW2, dW3, db1, db2, db3 = back_propagation_with_dropout(W1, W2, W3, b1, b2, b3, A1, A2, A3, D1, D2, X_batch, Y_batch, lambd, keep_prob)\n",
    "            #Calculate loss\n",
    "            batch_loss = cross_entropy_loss_with_regularization(A3, Y_batch, W1, W2, W3, lambd)\n",
    "            #Update params\n",
    "            W1, W2, W3, b1, b2, b3 = update_parameters(W1, W2, W3, b1, b2, b3, dW1, dW2, dW3, db1, db2, db3, alpha)\n",
    "            #Increment loss\n",
    "            train_loss += batch_loss * X_batch.shape[0]\n",
    "\n",
    "        train_loss = train_loss / N\n",
    "        \n",
    "        A1,A2,A3 = forward_propagation(W1, W2, W3, b1, b2, b3, X_val)\n",
    "        val_loss = cross_entropy_loss_with_regularization(A3, Y_val, W1, W2, W3, lambd)\n",
    "        \n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print('Epoch {}: Training Loss = {:.6f}, Validation Loss = {:.6f}'.format(epoch+1, train_loss, val_loss))    \n",
    "        hist.append((train_loss, val_loss))\n",
    "    \n",
    "    return W1, W2, W3, b1, b2, b3, hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f9c3e9b5ed91e33c3f72926f1d5ae434",
     "grade": false,
     "grade_id": "cell-152432545e070e9f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Training Loss = 0.956371, Validation Loss = 0.891655\n",
      "Epoch 20: Training Loss = 0.949176, Validation Loss = 0.876674\n",
      "Epoch 30: Training Loss = 0.912268, Validation Loss = 0.877019\n",
      "Epoch 40: Training Loss = 0.917550, Validation Loss = 0.864338\n",
      "Epoch 50: Training Loss = 0.901149, Validation Loss = 0.850146\n",
      "Epoch 60: Training Loss = 0.878729, Validation Loss = 0.841563\n",
      "Epoch 70: Training Loss = 0.882632, Validation Loss = 0.836226\n",
      "Epoch 80: Training Loss = 0.885223, Validation Loss = 0.834332\n",
      "Epoch 90: Training Loss = 0.863555, Validation Loss = 0.821191\n",
      "Epoch 100: Training Loss = 0.874188, Validation Loss = 0.811248\n",
      "Epoch 110: Training Loss = 0.846191, Validation Loss = 0.801888\n",
      "Epoch 120: Training Loss = 0.849830, Validation Loss = 0.802251\n",
      "Epoch 130: Training Loss = 0.849176, Validation Loss = 0.797540\n",
      "Epoch 140: Training Loss = 0.867697, Validation Loss = 0.800399\n",
      "Epoch 150: Training Loss = 0.858307, Validation Loss = 0.797226\n",
      "Epoch 160: Training Loss = 0.866312, Validation Loss = 0.797272\n",
      "Epoch 170: Training Loss = 0.866645, Validation Loss = 0.823274\n",
      "Epoch 180: Training Loss = 0.845612, Validation Loss = 0.796513\n",
      "Epoch 190: Training Loss = 0.858935, Validation Loss = 0.800636\n",
      "Epoch 200: Training Loss = 0.862859, Validation Loss = 0.803363\n",
      "Epoch 210: Training Loss = 0.852364, Validation Loss = 0.800615\n",
      "Epoch 220: Training Loss = 0.863342, Validation Loss = 0.799987\n",
      "Epoch 230: Training Loss = 0.858290, Validation Loss = 0.805379\n",
      "Epoch 240: Training Loss = 0.850377, Validation Loss = 0.802343\n",
      "Epoch 250: Training Loss = 0.850265, Validation Loss = 0.797179\n",
      "Epoch 260: Training Loss = 0.867191, Validation Loss = 0.803563\n",
      "Epoch 270: Training Loss = 0.851659, Validation Loss = 0.807147\n",
      "Epoch 280: Training Loss = 0.857810, Validation Loss = 0.803252\n",
      "Epoch 290: Training Loss = 0.871014, Validation Loss = 0.805253\n",
      "Epoch 300: Training Loss = 0.856887, Validation Loss = 0.806811\n",
      "Epoch 310: Training Loss = 0.853417, Validation Loss = 0.801310\n",
      "Epoch 320: Training Loss = 0.835137, Validation Loss = 0.805030\n",
      "Epoch 330: Training Loss = 0.849580, Validation Loss = 0.804103\n",
      "Epoch 340: Training Loss = 0.846316, Validation Loss = 0.806971\n",
      "Epoch 350: Training Loss = 0.856424, Validation Loss = 0.802499\n",
      "Epoch 360: Training Loss = 0.857968, Validation Loss = 0.806323\n",
      "Epoch 370: Training Loss = 0.849821, Validation Loss = 0.807120\n",
      "Epoch 380: Training Loss = 0.857460, Validation Loss = 0.806496\n",
      "Epoch 390: Training Loss = 0.853521, Validation Loss = 0.810007\n",
      "Epoch 400: Training Loss = 0.872678, Validation Loss = 0.806212\n",
      "Epoch 410: Training Loss = 0.863673, Validation Loss = 0.798947\n",
      "Epoch 420: Training Loss = 0.849506, Validation Loss = 0.806847\n",
      "Epoch 430: Training Loss = 0.860220, Validation Loss = 0.804235\n",
      "Epoch 440: Training Loss = 0.856372, Validation Loss = 0.809207\n",
      "Epoch 450: Training Loss = 0.862298, Validation Loss = 0.804661\n",
      "Epoch 460: Training Loss = 0.851561, Validation Loss = 0.800003\n",
      "Epoch 470: Training Loss = 0.857703, Validation Loss = 0.802246\n",
      "Epoch 480: Training Loss = 0.861088, Validation Loss = 0.808943\n",
      "Epoch 490: Training Loss = 0.845570, Validation Loss = 0.806965\n",
      "Epoch 500: Training Loss = 0.843344, Validation Loss = 0.809864\n"
     ]
    }
   ],
   "source": [
    "alpha = .1\n",
    "lambd = .01\n",
    "keep_prob = .8\n",
    "num_iters = 500\n",
    "W1, W2, W3, b1, b2, b3, _ = training_with_dropout(X_train, Y_train, X_val, Y_val, n1, n2, m, alpha, lambd, keep_prob, num_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "771b6c2bbee20379c95dc7b68657ec9a",
     "grade": true,
     "grade_id": "cell-c3922de82e24a5cd",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(b1, np.ndarray), \"b1 should be a numpy array.\"\n",
    "assert isinstance(b2, np.ndarray), \"b2 should be a numpy array.\"\n",
    "assert isinstance(b3, np.ndarray), \"b3 should be a numpy array.\"\n",
    "\n",
    "assert b1.shape[0] == 1, \"The number of rows in b1 is incorrect.\"\n",
    "assert b1.shape[1] == n1, \"The number of columns in b1 is incorrect.\"\n",
    "\n",
    "assert b2.shape[0] == 1, \"The number of rows in b2 is incorrect.\"\n",
    "assert b2.shape[1] == n2, \"The number of columns in b2 is incorrect.\"\n",
    "\n",
    "assert b3.shape[0] == 1, \"The number of rows in b3 is incorrect.\"\n",
    "assert b3.shape[1] == n3, \"The number of columns in b3 is incorrect.\"\n",
    "\n",
    "assert isinstance(W1, np.ndarray), \"W1 should be a numpy array.\"\n",
    "assert isinstance(W2, np.ndarray), \"W2 should be a numpy array.\"\n",
    "assert isinstance(W3, np.ndarray), \"W3 should be a numpy array.\"\n",
    "\n",
    "assert W1.shape[0] == n0, \"The number of rows in W1 is incorrect.\"\n",
    "assert W1.shape[1] == n1, \"The number of columns in W1 is incorrect.\"\n",
    "\n",
    "assert W2.shape[0] == n1, \"The number of rows in W2 is incorrect.\"\n",
    "assert W2.shape[1] == n2, \"The number of columns in W2 is incorrect.\"\n",
    "\n",
    "assert W3.shape[0] == n2, \"The number of rows in W3 is incorrect.\"\n",
    "assert W3.shape[1] == n3, \"The number of columns in W3 is incorrect.\"\n",
    "\n",
    "assert np.allclose(np.round(W1[0,:], decimals=8), np.array([ 0.98400256, -0.90958075,  0.69564401])), \"Incorrect learned weights for W1.\"\n",
    "assert np.allclose(np.round(b1[0,:], decimals=8), np.array([ 0.83243119, -0.99671026, -0.05940563])), \"Incorrect learned values for b1.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3\n",
    "\n",
    "In the third part of this assignment, you will use your neural network implementation to classify handwritten digits on the MNIST dataset.  You should explore different settings for various hyperparameters, including the hidden layer sizes, batch size, learning rate, regularization weight, dropout rate, and number of training epochs.  \n",
    "\n",
    "There are two deliverables for this part:\n",
    "- A well-labeled figure that shows the training and validation loss vs epoch.  Be careful to avoid overfitting on the training data.\n",
    "- A cell that prints out the classification accuracy of your trained neural network on the validation data.  Classification accuracy is simply the percent of the images that are classified as the correct digit.  You should aim for an accuracy percentage in the high 90s.\n",
    "\n",
    "You may use as many code and Markdown cells as needed.  We have included some code to retrieve the data.  You will need to set up and activate your virtual environment (see instructions posted on Sakai).  Also, make sure your jupyter notebook is running in your virtual environment by following [these instructions](https://anbasile.github.io/programming/2017/06/25/jupyter-venv/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "62d138375cd2f346f0b28750a6e6bfa3",
     "grade": false,
     "grade_id": "cell-c1c552e95bb6a257",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dd8c656f234743914e7cf2536016380d",
     "grade": false,
     "grade_id": "cell-d8ffbd9f0acd9bec",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def load_mnist_data():\n",
    "    '''\n",
    "    Retrieves and formats the MNIST data.\n",
    "    '''\n",
    "    (X_train, Y_train), (X_val, Y_val) = tf.keras.datasets.mnist.load_data()\n",
    "    X_train = X_train.reshape((X_train.shape[0], -1))/255.0\n",
    "    X_val = X_val.reshape((X_val.shape[0],-1))/255.0\n",
    "    Y_train = pd.get_dummies(Y_train).to_numpy() # convert to one-hot encoding\n",
    "    Y_val = pd.get_dummies(Y_val).to_numpy()\n",
    "    return X_train, Y_train, X_val, Y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9016e4c1ca8277a46f52fc68a5a71f55",
     "grade": false,
     "grade_id": "cell-c70911c050f7077a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X_train, Y_train, X_val, Y_val = load_mnist_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "56b3c839b283595435d39ecc5ba34a4b",
     "grade": false,
     "grade_id": "cell-e738f8e623c6f8d2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f9e54cfa430>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_img = X_train[0,:].reshape((28,28))\n",
    "plt.imshow(sample_img, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7f3f2fad791f0427f685f7d5bcd6441c",
     "grade": false,
     "grade_id": "cell-c1bc754a3f03ce26",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "For grading purpose: \n",
    "\n",
    "A well-labeled figure that shows the training and validation loss vs epoch. Be careful to avoid overfitting on the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "39d8d91e2e056c118945dd782bb17a40",
     "grade": false,
     "grade_id": "cell-5fab5bf8c9befea9",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "For grading purpose: \n",
    "\n",
    "A cell that prints out the classification accuracy of your trained neural network on the validation data. Classification accuracy is simply the percent of the images that are classified as the correct digit. You should aim for an accuracy percentage in the high 90s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define network parameters\n",
    "n0 = 784\n",
    "n1 = 50\n",
    "n2 = 20\n",
    "n3 = 10\n",
    "m = 5000\n",
    "#I had to cast Y_train and Y_val to floats in order for training to work as expected. When they were left\n",
    "#as uint8, taking -Y like I did in backpropagation would underflow 1 to 255 instead of -1 and break everything\n",
    "Y_train = Y_train.astype(float)\n",
    "Y_val = Y_val.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e261611cf4755b0ecb57df705db8d7e5",
     "grade": true,
     "grade_id": "cell-23f81057f8ed7781",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Intentionally Empty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Training Loss = 0.492185, Validation Loss = 0.287728\n",
      "Epoch 20: Training Loss = 0.385185, Validation Loss = 0.218936\n",
      "Epoch 30: Training Loss = 0.340737, Validation Loss = 0.194006\n",
      "Epoch 40: Training Loss = 0.305232, Validation Loss = 0.176432\n",
      "Epoch 50: Training Loss = 0.282605, Validation Loss = 0.167479\n",
      "Epoch 60: Training Loss = 0.268287, Validation Loss = 0.163203\n",
      "Epoch 70: Training Loss = 0.254828, Validation Loss = 0.158843\n",
      "Epoch 80: Training Loss = 0.240960, Validation Loss = 0.154281\n",
      "Epoch 90: Training Loss = 0.234467, Validation Loss = 0.151893\n",
      "Epoch 100: Training Loss = 0.226780, Validation Loss = 0.149808\n",
      "Epoch 110: Training Loss = 0.221223, Validation Loss = 0.149520\n",
      "Epoch 120: Training Loss = 0.215786, Validation Loss = 0.149554\n",
      "Epoch 130: Training Loss = 0.213429, Validation Loss = 0.148986\n",
      "Epoch 140: Training Loss = 0.206807, Validation Loss = 0.149318\n",
      "Epoch 150: Training Loss = 0.204138, Validation Loss = 0.147594\n",
      "Epoch 160: Training Loss = 0.200916, Validation Loss = 0.148716\n",
      "Epoch 170: Training Loss = 0.194821, Validation Loss = 0.147933\n",
      "Epoch 180: Training Loss = 0.191161, Validation Loss = 0.150129\n",
      "Epoch 190: Training Loss = 0.190549, Validation Loss = 0.150481\n",
      "Epoch 200: Training Loss = 0.188055, Validation Loss = 0.151293\n"
     ]
    }
   ],
   "source": [
    "#Additional parameters for learning rate, dropout, and regularization. Train for many iterations to see the training\n",
    "#curve\n",
    "alpha = .5\n",
    "lambd = .01\n",
    "keep_prob = .8\n",
    "num_iters = 200\n",
    "W1, W2, W3, b1, b2, b3, losses = training_with_dropout(X_train, Y_train, X_val, Y_val, n1, n2, m, alpha, lambd, keep_prob, num_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f9e54cb79d0>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot the training curve\n",
    "#Separate out the losses from training and the epochs\n",
    "valLosses = [x[1] for x in losses]\n",
    "trainLosses = [x[0] for x in losses]\n",
    "epochs = np.arange(num_iters)\n",
    "\n",
    "#Plot them \n",
    "plt.title('Losses vs epoch')\n",
    "plt.plot(epochs, valLosses, label='Validation loss')\n",
    "plt.plot(epochs, trainLosses, label='Train loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Losses(Cross Entropy with Regularization)')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Valdation loss looked  good at epoch 130, since it increases at epoch 140. Also, validation loss\n",
    "#### is lower than training loss since it doesn't use dropout. Using Epoch 130 for our weights:\n",
    "\n",
    "Let's find the classification accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Training Loss = 0.492185, Validation Loss = 0.287728\n",
      "Epoch 20: Training Loss = 0.385185, Validation Loss = 0.218936\n",
      "Epoch 30: Training Loss = 0.340737, Validation Loss = 0.194006\n",
      "Epoch 40: Training Loss = 0.305232, Validation Loss = 0.176432\n",
      "Epoch 50: Training Loss = 0.282605, Validation Loss = 0.167479\n",
      "Epoch 60: Training Loss = 0.268287, Validation Loss = 0.163203\n",
      "Epoch 70: Training Loss = 0.254828, Validation Loss = 0.158843\n",
      "Epoch 80: Training Loss = 0.240960, Validation Loss = 0.154281\n",
      "Epoch 90: Training Loss = 0.234467, Validation Loss = 0.151893\n",
      "Epoch 100: Training Loss = 0.226780, Validation Loss = 0.149808\n",
      "Epoch 110: Training Loss = 0.221223, Validation Loss = 0.149520\n",
      "Epoch 120: Training Loss = 0.215786, Validation Loss = 0.149554\n",
      "Epoch 130: Training Loss = 0.213429, Validation Loss = 0.148986\n"
     ]
    }
   ],
   "source": [
    "#This time, stop at 130 epochs since those weights didn't seem to overfit. At epoch 140, valid loss increases\n",
    "num_iters = 130\n",
    "W1, W2, W3, b1, b2, b3, losses = training_with_dropout(X_train, Y_train, X_val, Y_val, n1, n2, m, alpha, lambd, keep_prob, num_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix for validation set:\n",
      "[[ 968    0    6    1    1    3    6    1    5    5]\n",
      " [   1 1118    4    1    0    1    3    7    2    4]\n",
      " [   0    3 1002   12    2    1    1   21    2    0]\n",
      " [   0    3    1  970    0    9    0    2    8    7]\n",
      " [   0    0    7    0  949    0    4    2    6   19]\n",
      " [   1    1    0    5    0  855    6    0    5    7]\n",
      " [   5    2    3    0   10    9  935    0    5    2]\n",
      " [   1    1    6   11    1    1    0  987    3    7]\n",
      " [   4    7    3    8    2    8    3    1  934    6]\n",
      " [   0    0    0    2   17    5    0    7    4  952]]\n",
      "Accuracy:\n",
      "96.7%\n"
     ]
    }
   ],
   "source": [
    "#Run the predict function \n",
    "Y_pred, P = predict(W1, W2, W3, b1, b2, b3, X_val)\n",
    "#Extract the actual values\n",
    "Y_act = np.argmax(Y_val, axis = 1)\n",
    "\n",
    "#Make a confusion matrix and a tally for correct predictions\n",
    "confMat = np.zeros((n3,n3))\n",
    "correct = 0\n",
    "\n",
    "#Iterate through predictions, increment confusion matrix and correct tally\n",
    "for x in range(len(Y_pred)):\n",
    "    confMat[Y_pred[x]][Y_act[x]] +=1\n",
    "    if Y_pred[x] ==Y_act[x]:\n",
    "        correct +=1\n",
    "    \n",
    "print(\"Confusion matrix for validation set:\")\n",
    "print(confMat.astype(int))\n",
    "print(\"Accuracy:\")\n",
    "print(str(correct*100/len(Y_pred)) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural net worked well on the validation set with a classification accuracy of 96.7%!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
