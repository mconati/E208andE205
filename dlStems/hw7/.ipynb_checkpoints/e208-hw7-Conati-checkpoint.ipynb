{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 7: Implementing a Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework assignment, you will implement a convolutional neural network from scratch using numpy.\n",
    "\n",
    "There are three parts to this assignment:\n",
    "- In part 1, you will implement the building blocks for a CNN.\n",
    "- In part 2, you will put the building blocks together to form a CNN model.\n",
    "- In part 3, you will use your CNN implementation to recognize digits on MNIST."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few important notes:\n",
    "* You will need to set up and activate your virtual environment.  I have posted a .yaml file on Sakai, and you can look up instructions online on how to create a virtual environment from a .yaml file.  Make sure your jupyter notebook is running in your virtual environment by following [these instructions](https://anbasile.github.io/programming/2017/06/25/jupyter-venv/).\n",
    "\n",
    "* For autograding purposes, many cells are set in *read-only* mode.  This means you cannot edit, delete, or move these cells. \n",
    "\n",
    "* For parts 1 and 2, you should not modify any code except where you see `# YOUR CODE HERE`.  Please remove `raise NotImplementedError()` and write your code in its place.\n",
    "\n",
    "* We provide code cells that check your code as you progress through the assignment. These tests will help catch obvious errors, but passing these tests is not a guarantee that your implementation is correct. Your notebook will be graded on a more complete set of tests which are not visible to you.\n",
    "\n",
    "* You may add new cells anywhere in the notebook while working, but make sure to delete them before submitting your assignment.  Please do not change the sequence of the cells.  This is to ensure that autograding works properly.\n",
    "\n",
    "* If you run cells out of order, it may change variable values which may cause tests to fail.  The easiest way to avoid this problem is to use the ***Restart & Run all*** option available in the **Kernel** tab.  Before submitting, you should do this to ensure that your notebook can be run from beginning to end without errors.  This is how your notebook will be graded.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "62b6ba46f2a292c11400afc4e021c20a",
     "grade": false,
     "grade_id": "cell-f4f5c1685c54cff7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Part 1\n",
    "\n",
    "In this part you will implement forward propagation and backward propagation for the building blocks in a CNN.  There are six building blocks:\n",
    "\n",
    "* rectified linear unit\n",
    "* linear layer\n",
    "* softmax\n",
    "* flatten\n",
    "* max pooling\n",
    "* convolutional layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8d1d4d9838ddc08f5c95ddcb4f2c26fc",
     "grade": false,
     "grade_id": "cell-592b739c828d6751",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "You will implement everything from scratch in numpy, so we only need to import one package!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6447bacead0c074e72b608de7d705bca",
     "grade": false,
     "grade_id": "cell-f04e267d04e69235",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0c6de3bfe18d206e90b0b6844f6c80bd",
     "grade": false,
     "grade_id": "cell-89932ee6114d5555",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Rectified Linear Unit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e8d07656d72841e825cb54dfda50ae28",
     "grade": false,
     "grade_id": "cell-7a35bf3ff69fbde2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Implement the forward and backward propagation functions for the ReLU activation function.  Even though you've implemented this before, try doing this without looking at your notes so you can truly implement the CNN from scratch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3b4030f9d5420238d9476bb173c4ab16",
     "grade": false,
     "grade_id": "cell-8dc60306488b6211",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def ReLU(Z):\n",
    "    '''\n",
    "    Applies the rectified linear unit activation function to each element in z.\n",
    "    \n",
    "    Inputs\n",
    "        Z : input to the activation function, size (batch, m)\n",
    "        \n",
    "    Outputs\n",
    "        A : output of the activation function, size (batch, m)\n",
    "    '''\n",
    "    #Return zero if less than 0\n",
    "    A = np.maximum(0,Z)\n",
    "    \n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "73d05ac39345f6419833610ce297c200",
     "grade": true,
     "grade_id": "cell-a3ea10d6009c287d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert ReLU(0) == 0, \"Incorrect value for t=0\"\n",
    "assert ReLU(1.234) == 1.234, \"Incorrect result for a positive input.\"\n",
    "assert ReLU(-2.35) == 0, \"Incorrect result for a negative input.\"\n",
    "assert np.array_equal(ReLU(np.array([1.15, -2, 3])), np.array([1.15, 0, 3])), \"Incorrect result for 1D numpy array.\"\n",
    "assert np.array_equal(ReLU(np.array([[-1, 6], [-3, 0.1], [4, 7]])), np.array([[0, 6], [0, 0.1], [4, 7]])), \"Incorrect result for 2D numpy array.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b96e4d61d68806663bbb5e5ac8303021",
     "grade": false,
     "grade_id": "cell-2c8ce35b51b079e4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def ReLU_backprop(dA, Z):\n",
    "    '''\n",
    "    Performs backprop on the ReLU activation function.\n",
    "    \n",
    "    Inputs\n",
    "        dA: the gradient of the loss with respect to the output of the ReLU function\n",
    "        Z: the input to the ReLU function during the forward propagation stage\n",
    "        \n",
    "    Outputs\n",
    "        dZ: the gradient of the loss with respect to the input of the ReLU function\n",
    "    '''\n",
    "    #Zero where Z is zero, otherwise pass grad\n",
    "    dZ =  np.multiply(dA, np.where(Z>0, 1, 0))\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "647d8eda2d591e152ee7ecd2b3d18c7e",
     "grade": true,
     "grade_id": "cell-c75a4afece6e5498",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert np.array_equal(ReLU_backprop(np.array([1.15, -2, 3]), np.array([-4, 2, .1])), np.array([0, -2, 3])), \"Incorrect result for 1D numpy array.\"\n",
    "assert np.array_equal(ReLU_backprop(np.array([[-1, 6], [-3, 0.3], [4, 7]]), np.array([[2, -1], [-.1, .1], [3, 5]])), np.array([[-1, 0], [0, .3], [4, 7]])), \"Incorrect result for 2D numpy array.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7ca78bc7f10a09ee4fc8c6bd3fb443d9",
     "grade": false,
     "grade_id": "cell-5f4e9af51bbe7a75",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Linear Layer\n",
    "\n",
    "Implement the forward and backward propagation functions for a linear layer (without an activation function).  Again, try doing this without looking at your notes!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bc78b2167ed04ed714e2999e15458552",
     "grade": false,
     "grade_id": "cell-a413729d206bc40d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def linear_layer(X, W, b):\n",
    "    '''\n",
    "    Performs a forward propagation through a linear layer.\n",
    "    \n",
    "    Inputs\n",
    "        X : input vector of size (batch, n)\n",
    "        W : multiplicative weight matrix, size (n, m)\n",
    "        b : bias coefficients, size (1, m)\n",
    "        \n",
    "    Outputs\n",
    "        Z : output vector of size (batch, m)\n",
    "    '''\n",
    "    assert X.ndim == 2, \"Input should be a matrix\"\n",
    "    #Linear layer\n",
    "    Z = X@W + b\n",
    "    \n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5a34962d2ec93796f6545ff50ebc896e",
     "grade": true,
     "grade_id": "cell-14cae5c0abde91bb",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X = np.arange(6).reshape((2,3))\n",
    "W = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "b = np.array([[-1, -2]])\n",
    "Z_ref = np.array([[12, 14],[39, 50]])\n",
    "Z = linear_layer(X, W, b)\n",
    "assert Z.shape == (2, 2), \"Result is incorrect shape\"\n",
    "assert np.array_equal(Z, Z_ref), \"Result has incorrect values\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b51af22af3728674efe47ffd591c39c6",
     "grade": false,
     "grade_id": "cell-c7fafc546667b861",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def linear_layer_backprop(dZ, X, W, b):\n",
    "    '''\n",
    "    Performs a backward propagation through a linear layer.\n",
    "    \n",
    "    Inputs\n",
    "        dZ : gradient of loss with respect to the output of the linear layer, size (batch, m)\n",
    "        X : input to the linear layer during forward propagation, size (batch, n)\n",
    "        W : multiplicative weight matrix, size (n, m)\n",
    "        b : bias coefficients, size (1, m)\n",
    "        \n",
    "    Outputs\n",
    "        dX : gradient of loss with respect to the input of the linear layer, size (batch, n)\n",
    "        dW : gradient of loss with respect to multiplicative weights, size (n, m)\n",
    "        db : gradient of loss with respect to bias coefficients, size (1, m)\n",
    "    '''\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    #Save shape for b\n",
    "    dbShape = b.shape\n",
    "    \n",
    "    #Calculate grads\n",
    "    db =  np.reshape(np.sum(dZ, axis=0)/m, dbShape)\n",
    "    dW = (np.matmul(X.T, dZ))/m\n",
    "    dX = np.matmul(dZ, W.T)\n",
    "    \n",
    "    return dX, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7cf87ce1fe5ee77d0e7ab1a5d6aa6c87",
     "grade": true,
     "grade_id": "cell-535406197505fc5d",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "dZ = np.array([[5, -1], [-2, 1]])\n",
    "X = np.array([[1,2,3],[4,5,6]])\n",
    "W = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "b = np.array([[-1, -2]])\n",
    "dX_ref = np.array([[ 3, 11, 19], [0, -2, -4]])\n",
    "dW_ref = np.array([[-1.5, 1.5], [0, 1.5], [1.5, 1.5]])\n",
    "db_ref = np.array([[1.5, 0]])\n",
    "dX, dW, db = linear_layer_backprop(dZ, X, W, b)\n",
    "assert np.array_equal(dX, dX_ref), \"Incorrect result for dX\"\n",
    "assert np.array_equal(dW, dW_ref), \"Incorrect result for dW\"\n",
    "assert np.array_equal(db, db_ref), \"Incorrect result for db\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "63ceeea315da45867ed0b55207267f2c",
     "grade": false,
     "grade_id": "cell-9c1043840336b342",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d19081bdd4f24db9dfa99763e67d06ba",
     "grade": false,
     "grade_id": "cell-edeb3428a8be4704",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Implement the forward and backpropagation functions for softmax.  Try doing this without looking at your previous homeworks!  Remember to subtract the max in order to prevent overflow.\n",
    "\n",
    "$$ softmax([z_1, \\dots, z_k]) = \\left[\\frac{e^{z_1}}{e^{z_1}+ \\dots + e^{z_k}}, \\dots, \\frac{e^{z_k}}{e^{z_1}+ \\dots + e^{z_k}}\\right]$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "106fc6809db4f44f7d3444373fd5a358",
     "grade": false,
     "grade_id": "cell-c5b45f3cb62b67a0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    '''\n",
    "    Applies the softmax function to each row of a matrix Z.\n",
    "    This implementation should be vectorized, so it should not contain any for loops.\n",
    "    \n",
    "    Inputs\n",
    "        Z : input tensor of shape (batch, m)\n",
    "        \n",
    "    Outputs\n",
    "        A : output tensor of shape (batch, m)\n",
    "    '''\n",
    "    assert isinstance(Z, np.ndarray) and Z.ndim == 2, \"Input should be a 2D numpy array.\"\n",
    "    \n",
    "    maxVals = np.max(Z, axis=1, keepdims=True)\n",
    "\n",
    "    #Calculate softmax\n",
    "    Z = Z - maxVals\n",
    "    numerator = np.exp(Z)\n",
    "    denominator = np.sum(numerator, axis=1, keepdims=True)\n",
    "    A = numerator/denominator\n",
    "    \n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9b91808a1e84592d497482ffee7e614b",
     "grade": true,
     "grade_id": "cell-5f81c57a1f60f20b",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert np.allclose(np.round(softmax(np.array([[0, 0, 0, 0]])), decimals=8), np.array([[0.25, 0.25, 0.25, 0.25]])), \"Returns incorrect result for all zero matrix.\"\n",
    "assert np.allclose(np.round(softmax(np.array([[1e6, 1e6, 1e6, 1e6]])), decimals=8), np.array([[0.25, 0.25, 0.25, 0.25]])), \"Your implementation does not handle numerical overflow.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9da8045ddbb2dfaa65de691fa24ab95d",
     "grade": true,
     "grade_id": "cell-4337489c34278104",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert np.allclose(np.round(softmax(np.array([[1, 2, 3]])), decimals=8), np.array([[0.09003057, 0.24472847, 0.66524096]])), \"Returns incorrect result for non-zero single row matrix.\"\n",
    "assert np.allclose(np.round(softmax(np.array([[-2, 0, 10]])), decimals=8), np.array([[6.1400000e-06, 4.5400000e-05, 9.9994846e-01]])), \"Returns incorrect result for single row matrix with .\"\n",
    "assert np.allclose(np.round(softmax(np.array([[0, 0], [-3.2, 1.1], [4, 3.4]])), decimals=8),  np.array([[0.5, 0.5],[0.01338692, 0.98661308],[0.64565631, 0.35434369]])), \"Returns incorrect result for non-zero multiple row matrix.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d2744d7bf700f9317d2e71636597161c",
     "grade": false,
     "grade_id": "cell-8818fe47b01398ae",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Instead of implementing backprop for softmax, we will just directly compute the gradient of cross entropy loss with respect to the softmax inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3a4c31e0ca644736a04815f8ff70b852",
     "grade": false,
     "grade_id": "cell-d4a8280b9a585e79",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def softmax_grad(preds, targets):\n",
    "    '''\n",
    "    Directly computes the gradient of cross entropy loss with respect to the softmax inputs.\n",
    "    \n",
    "    Inputs\n",
    "        preds : the matrix of softmax outputs, size (batch, numClasses)\n",
    "        targets : one-hot encoded target labels, size (batch, numClasses)\n",
    "        \n",
    "    Outputs\n",
    "        dZ : gradient of loss with respect to the softmax inputs, size (batch, numClasses)\n",
    "        \n",
    "    '''\n",
    "    assert isinstance(preds, np.ndarray) and preds.ndim == 2, \"preds should be a 2D numpy array.\"\n",
    "    assert isinstance(targets, np.ndarray) and targets.ndim == 2, \"targets should be a 2D numpy array.\"\n",
    "    \n",
    "    #Using equation from class\n",
    "    dZ = -targets + preds\n",
    "\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6344e518d99d0df5351664a5c8b365fa",
     "grade": true,
     "grade_id": "cell-bb249bcbab741071",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "preds = np.array([[.1, .3, .6],[.2, .5, .3]])\n",
    "targets = np.array([[0, 0, 1],[0, 1, 0]])\n",
    "dZ_ref = np.array([[.1, .3, -.4],[.2, -.5, .3]])\n",
    "assert np.allclose(softmax_grad(preds, targets), dZ_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flatten\n",
    "\n",
    "Implement the forward and backward propagation functions for the flattening operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1399b4fe027e560625434227482e6695",
     "grade": false,
     "grade_id": "cell-54fd643959ccfc32",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def flatten(X):\n",
    "    '''\n",
    "    Flattens the tensor of activations from a convolutional layer.\n",
    "    \n",
    "    Inputs\n",
    "        X : tensor of shape (B, C, H, W), where B is the batch dimension and C is the channel\n",
    "        \n",
    "    Outputs\n",
    "        Y : flattened tensor of shape (B, C x H x W)\n",
    "    '''\n",
    "    assert isinstance(Z, np.ndarray) and Z.ndim == 2, \"Input should be a 2D numpy array.\"\n",
    "    #Reshape the input\n",
    "    b,c,h,w = X.shape\n",
    "    Y = X.reshape(b, (c*h*w))\n",
    "    \n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "674d89638b1266ff1641ced6ab8c77c8",
     "grade": true,
     "grade_id": "cell-7ad15703978f4e98",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X = np.arange(16).reshape((2,2,2,2))\n",
    "Y_ref = np.array([[ 0,  1,  2,  3,  4,  5,  6,  7], [ 8,  9, 10, 11, 12, 13, 14, 15]])\n",
    "Y = flatten(X)\n",
    "assert np.array_equal(Y, Y_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0d523f57ae0722e74fd3a1563a88327d",
     "grade": false,
     "grade_id": "cell-7845aca7b40734ce",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def flatten_backprop(dY, C, H, W):\n",
    "    '''\n",
    "    Performs backward propagation through a flattening layer.\n",
    "    \n",
    "    Inputs\n",
    "        dY : gradient of the loss with respect to the output of the flattening layer, size (batch, m)\n",
    "        C : number of channels in the input to the flattening layer\n",
    "        H : height of the tensor input to the flattening layer\n",
    "        W : width of the tensor input to the flattening layer\n",
    "        \n",
    "    Outputs\n",
    "        dX : gradient of the loss with respect to the input of the flattening layer, size (batch, C, H, W)\n",
    "    '''\n",
    "    assert isinstance(dY, np.ndarray) and dY.ndim == 2, \"Input should be a 2D numpy array.\"\n",
    "    assert dY.shape[1] == C*H*W, \"Dimensions of dY must be (batch, CxHxW)\"\n",
    "    \n",
    "    #Reshape the grads\n",
    "    b = dY.shape[0]\n",
    "    dX = dY.reshape((b,C,H,W))\n",
    "    return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7065b16642297db0c3e9e6c4dd44937e",
     "grade": true,
     "grade_id": "cell-73a6af4b753ec1a5",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "dY = np.arange(16).reshape((2,8))\n",
    "C, H, W = 2, 2, 2\n",
    "dX_ref = np.arange(16).reshape((2,2,2,2))\n",
    "dX = flatten_backprop(dY, C, H, W)\n",
    "assert np.array_equal(dX, dX_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "08d2730fcb041ae9e4640af556351f23",
     "grade": false,
     "grade_id": "cell-a1e077783e37d003",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### im2col helper function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f39568194890810fca83d90796bf0fd5",
     "grade": false,
     "grade_id": "cell-945c96c8b4f9e7ac",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Your implementations for max pooling and the convolutional layer will depend on a utility function called `im2col`.  We provide an implementation below, taken from Stanford's [CS231n](http://cs231n.stanford.edu/) class.  These functions are very dense (and also implemented in cython), and you don't need to understand the details of the implementation.  You should, however, understand what the utility functions are doing at a high level.  Read [this](https://cs231n.github.io/convolutional-networks/) explanation of `im2col` in the section entitled \"Implementation as Matrix Multiplication.\"\n",
    "\n",
    "This section of the homework uses materials from the CS231n class and Agustinus Kristiadi's blog.  Therefore, please do not consult these websites or any affiliated websites (e.g. students posting their solutions to CS231n assignments) when completing this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7a8452ff54dc8af2284bf4d28424f327",
     "grade": false,
     "grade_id": "cell-1ac2975fffd54d8e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a479ca771c7238ad5a06829404f86368",
     "grade": false,
     "grade_id": "cell-51858b53aab97e3d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "# These functions are taken from https://github.com/yunjey/cs231n/blob/master/assignment2/cs231n/im2col_cython.pyx\n",
    "\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "cimport cython\n",
    "\n",
    "DTYPE = np.float64\n",
    "ctypedef np.float64_t DTYPE_t\n",
    "\n",
    "# ctypedef fused DTYPE_t:\n",
    "#     np.float32_t\n",
    "#     np.float64_t\n",
    "\n",
    "def im2col_cython(np.ndarray[DTYPE_t, ndim=4] x, int field_height,\n",
    "                  int field_width, int padding, int stride):\n",
    "    cdef int N = x.shape[0]\n",
    "    cdef int C = x.shape[1]\n",
    "    cdef int H = x.shape[2]\n",
    "    cdef int W = x.shape[3]\n",
    "    \n",
    "    cdef int HH = (H + 2 * padding - field_height) // stride + 1\n",
    "    cdef int WW = (W + 2 * padding - field_width) // stride + 1\n",
    "\n",
    "    cdef int p = padding\n",
    "    cdef np.ndarray[DTYPE_t, ndim=4] x_padded = np.pad(x,\n",
    "            ((0, 0), (0, 0), (p, p), (p, p)), mode='constant')\n",
    "\n",
    "    cdef np.ndarray[DTYPE_t, ndim=2] cols = np.zeros(\n",
    "            (C * field_height * field_width, N * HH * WW),\n",
    "            dtype=x.dtype)\n",
    "\n",
    "    # Moving the inner loop to a C function with no bounds checking works, but does\n",
    "    # not seem to help performance in any measurable way.\n",
    "\n",
    "    im2col_cython_inner(cols, x_padded, N, C, H, W, HH, WW,\n",
    "                        field_height, field_width, padding, stride)\n",
    "    return cols\n",
    "\n",
    "\n",
    "@cython.boundscheck(False)\n",
    "cdef int im2col_cython_inner(np.ndarray[DTYPE_t, ndim=2] cols,\n",
    "                             np.ndarray[DTYPE_t, ndim=4] x_padded,\n",
    "                             int N, int C, int H, int W, int HH, int WW,\n",
    "                             int field_height, int field_width, int padding, int stride) except? -1:\n",
    "    cdef int c, ii, jj, row, yy, xx, i, col\n",
    "\n",
    "    for c in range(C):\n",
    "        for yy in range(HH):\n",
    "            for xx in range(WW):\n",
    "                for ii in range(field_height):\n",
    "                    for jj in range(field_width):\n",
    "                        row = c * field_width * field_height + ii * field_height + jj\n",
    "                        for i in range(N):\n",
    "                            col = yy * WW * N + xx * N + i\n",
    "                            cols[row, col] = x_padded[i, c, stride * yy + ii, stride * xx + jj]\n",
    "\n",
    "\n",
    "\n",
    "def col2im_cython(np.ndarray[DTYPE_t, ndim=2] cols, int N, int C, int H, int W,\n",
    "                  int field_height, int field_width, int padding, int stride):\n",
    "    cdef np.ndarray x = np.empty((N, C, H, W), dtype=cols.dtype)\n",
    "    cdef int HH = (H + 2 * padding - field_height) // stride + 1\n",
    "    cdef int WW = (W + 2 * padding - field_width) // stride + 1\n",
    "    cdef np.ndarray[DTYPE_t, ndim=4] x_padded = np.zeros((N, C, H + 2 * padding, W + 2 * padding),\n",
    "                                        dtype=cols.dtype)\n",
    "\n",
    "    # Moving the inner loop to a C-function with no bounds checking improves\n",
    "    # performance quite a bit for col2im.\n",
    "    col2im_cython_inner(cols, x_padded, N, C, H, W, HH, WW, \n",
    "                        field_height, field_width, padding, stride)\n",
    "    if padding > 0:\n",
    "        return x_padded[:, :, padding:-padding, padding:-padding]\n",
    "    return x_padded\n",
    "\n",
    "\n",
    "@cython.boundscheck(False)\n",
    "cdef int col2im_cython_inner(np.ndarray[DTYPE_t, ndim=2] cols,\n",
    "                             np.ndarray[DTYPE_t, ndim=4] x_padded,\n",
    "                             int N, int C, int H, int W, int HH, int WW,\n",
    "                             int field_height, int field_width, int padding, int stride) except? -1:\n",
    "    cdef int c, ii, jj, row, yy, xx, i, col\n",
    "\n",
    "    for c in range(C):\n",
    "        for ii in range(field_height):\n",
    "            for jj in range(field_width):\n",
    "                row = c * field_width * field_height + ii * field_height + jj\n",
    "                for yy in range(HH):\n",
    "                    for xx in range(WW):\n",
    "                        for i in range(N):\n",
    "                            col = yy * WW * N + xx * N + i\n",
    "                            x_padded[i, c, stride * yy + ii, stride * xx + jj] += cols[row, col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "82f68c10fac0b778a240b52ffe315343",
     "grade": false,
     "grade_id": "cell-39eb2461b7a6f54e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "You should use the cython implementation of `im2col` and `col2im` in your work.  If for some reason you are unable to get the above functions to compile, I have also included equivalent python implementations that have the same behavior (but are much, much slower)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1a0fbc5cf7d4d9f83882e7a960ce0712",
     "grade": false,
     "grade_id": "cell-53dedae3612f17fa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# These functions are taken from https://github.com/yunjey/cs231n/blob/master/assignment2/cs231n/im2col.py\n",
    "\n",
    "def get_im2col_indices(x_shape, field_height, field_width, padding=1, stride=1):\n",
    "    # First figure out what the size of the output should be\n",
    "    N, C, H, W = x_shape\n",
    "    assert (H + 2 * padding - field_height) % stride == 0\n",
    "    assert (W + 2 * padding - field_height) % stride == 0\n",
    "    out_height = (H + 2 * padding - field_height) // stride + 1\n",
    "    out_width = (W + 2 * padding - field_width) // stride + 1\n",
    "\n",
    "    i0 = np.repeat(np.arange(field_height), field_width)\n",
    "    i0 = np.tile(i0, C)\n",
    "    i1 = stride * np.repeat(np.arange(out_height), out_width)\n",
    "    j0 = np.tile(np.arange(field_width), field_height * C)\n",
    "    j1 = stride * np.tile(np.arange(out_width), out_height)\n",
    "    i = i0.reshape(-1, 1) + i1.reshape(1, -1)\n",
    "    j = j0.reshape(-1, 1) + j1.reshape(1, -1)\n",
    "\n",
    "    k = np.repeat(np.arange(C), field_height * field_width).reshape(-1, 1)\n",
    "\n",
    "    return (k, i, j)\n",
    "\n",
    "\n",
    "def im2col_indices(x, field_height, field_width, padding=1, stride=1):\n",
    "    \"\"\" An implementation of im2col based on some fancy indexing \"\"\"\n",
    "    # Zero-pad the input\n",
    "    p = padding\n",
    "    x_padded = np.pad(x, ((0, 0), (0, 0), (p, p), (p, p)), mode='constant')\n",
    "\n",
    "    k, i, j = get_im2col_indices(x.shape, field_height, field_width, padding,\n",
    "                               stride)\n",
    "\n",
    "    cols = x_padded[:, k, i, j]\n",
    "    C = x.shape[1]\n",
    "    cols = cols.transpose(1, 2, 0).reshape(field_height * field_width * C, -1)\n",
    "    return cols\n",
    "\n",
    "\n",
    "def col2im_indices(cols, x_shape, field_height=3, field_width=3, padding=1,\n",
    "                   stride=1):\n",
    "    \"\"\" An implementation of col2im based on fancy indexing and np.add.at \"\"\"\n",
    "    N, C, H, W = x_shape\n",
    "    H_padded, W_padded = H + 2 * padding, W + 2 * padding\n",
    "    x_padded = np.zeros((N, C, H_padded, W_padded), dtype=cols.dtype)\n",
    "    k, i, j = get_im2col_indices(x_shape, field_height, field_width, padding,\n",
    "                               stride)\n",
    "    cols_reshaped = cols.reshape(C * field_height * field_width, -1, N)\n",
    "    cols_reshaped = cols_reshaped.transpose(2, 0, 1)\n",
    "    np.add.at(x_padded, (slice(None), k, i, j), cols_reshaped)\n",
    "    if padding == 0:\n",
    "        return x_padded\n",
    "    return x_padded[:, :, padding:-padding, padding:-padding]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "62704183774dc84b7853820c7e99dac6",
     "grade": false,
     "grade_id": "cell-77db8bc158e8038c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Max Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3d4345b35bc5eba9b24398550325fc0c",
     "grade": false,
     "grade_id": "cell-804367c8ee55f8b9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Implement the forward and backward propagation functions for max pooling.  You should use the helper functions provided above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6b6345dc7731ef8b5bf366dee1270114",
     "grade": false,
     "grade_id": "cell-8bb988a9cb601262",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def max_pool(X, sz, stride):\n",
    "    '''\n",
    "    Applies max pooling to the input tensor X.\n",
    "    \n",
    "    Inputs\n",
    "        X: input tensor of shape B x C x H x W, where B is the batch dimension and C is the number of channels\n",
    "        sz : the size of the pooling region, assumed to be a sz x sz square region\n",
    "        stride : specifies the stride of the pooling, assumed to be the same in both dimensions\n",
    "    \n",
    "    Outputs\n",
    "        Y: output tensor, result after performing max pooling\n",
    "        max_idxs: array of integers specifying the max locations, used for performing back propagation\n",
    "    '''\n",
    "    assert X.ndim == 4, \"Input should be a 4-dimensional tensor of shape B x C x H x W\"\n",
    "    \n",
    "    # assume X is 10x3x14x14 (10 3-channel images) and we are performing 2x2 max pooling with stride 2\n",
    "    B, C, H, W = X.shape\n",
    "    \n",
    "    # reshaped to 30x1x14x14 to prepare for im2col\n",
    "    X_reshaped = X.reshape(B*C, 1, H, W)\n",
    "    #Assume no padding\n",
    "    pad = 0\n",
    "    # TO DO:\n",
    "    #  - use im2col function to calculate X_col, of shape 4 x 1470\n",
    "    #  - calculate max_idxs, which specifies the index of the max element in each column of X_col\n",
    "    #  - calculate maxvals, which specifies the max element in each column of X_col\n",
    "    #  - calculate H_out, the height of the tensor after max pooling (must be integer)\n",
    "    #  - calculate W_out, the width of the tensor after max pooling (must be integer)\n",
    "    #Calc X_col\n",
    "    X_col = im2col_cython(X_reshaped, sz, sz, pad, stride)\n",
    "    #Calc max_idxs and maxvals\n",
    "    max_idxs = np.argmax(X_col, axis = 0)\n",
    "    maxvals = np.max(X_col, axis = 0)\n",
    "    \n",
    "    #Calc height and width\n",
    "    H_out = (H-sz + 2*pad)/stride +1\n",
    "    W_out = (W-sz + 2*pad)/stride +1\n",
    "\n",
    "    # reshaped to 7x7x10x3\n",
    "    Y = maxvals.reshape(int(H_out), int(W_out), B, C)\n",
    "    \n",
    "    # re-arrange tensor dimensions to get 10x3x7x7\n",
    "    Y = Y.transpose(2, 3, 0, 1)\n",
    "\n",
    "    return Y, max_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "63427e03a8fe8b27a9a1d44e224336c8",
     "grade": true,
     "grade_id": "cell-bd2b1bfe4d1a45bc",
     "locked": true,
     "points": 8,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "X = np.random.randint(0,20,(2,2,4,4)).astype(np.float64)\n",
    "Y, max_idxs = max_pool(X, 2, 2)\n",
    "Y_ref = np.array([15, 19, 18, 14, 19, 19, 18, 19, 19, 14, 10, 18, 15, 17,  6, 14]).reshape((2,2,2,2))\n",
    "max_idxs_ref = np.array([1, 3, 0, 2, 3, 3, 0, 1, 0, 2, 0, 3, 3, 3, 1, 3])\n",
    "assert np.array_equal(Y, Y_ref), \"Max pooled results are not correct\"\n",
    "assert np.array_equal(max_idxs, max_idxs_ref), \"Max index locations are not correct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b51f9751a0070ded12083b1106261a57",
     "grade": false,
     "grade_id": "cell-b95cf575a04517be",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def max_pool_backprop(dY, sz, stride, max_idxs):\n",
    "    '''\n",
    "    Performs back propagation on a max pooling layer.\n",
    "    \n",
    "    Inputs\n",
    "        dY: gradient of the loss with respect to the max pooling outputs, shape BxCxHxW\n",
    "        sz : the size of the pooling region, assumed to be a sz x sz square region\n",
    "        stride : specifies the stride of the pooling, assumed to be the same in both dimensions\n",
    "        max_idxs : array of integers specifying the max locations, computed during forward propagation\n",
    "    \n",
    "    Outputs\n",
    "        dX: gradient of the loss with respect to the max pooling inputs, shape BxCxHxW\n",
    "    '''\n",
    "    assert dY.ndim == 4, \"Ygrad should be a 4-dimensional tensor of shape B x C x H x W\"\n",
    "    \n",
    "    # get dimensions of max pooling output & input\n",
    "    B, C, H_out, W_out = dY.shape\n",
    "    H_in = (H_out - 1)*stride + sz\n",
    "    W_in = (W_out - 1)*stride + sz\n",
    "\n",
    "    # re-arrange tensor dimensions to match im2col\n",
    "    dY_flat = dY.transpose(2, 3, 0, 1).ravel()\n",
    "    \n",
    "    # TO DO:\n",
    "    #  - initialize dX_col to a matrix of zeros, should be same size as X_col in forward propagation\n",
    "    #  - put the values of dY_flat into the maximum index of each column in dX_col\n",
    "    #  - calculate dX, of size (B*C, 1, H, W)\n",
    "    #  - reshape dX to get original size (B, C, H, W)\n",
    "    #Caclulate height and width to inizialize as zeros\n",
    "    imcolW = B*C*np.power(int((H_in-sz)/stride + 1), 2)\n",
    "    imcolH = sz*sz\n",
    "    X_col = np.zeros((imcolH, imcolW))\n",
    "    \n",
    "    #Fill in X_col and use col2im\n",
    "    cols = np.arange(max_idxs.shape[0])\n",
    "    X_col[max_idxs, cols] = dY_flat\n",
    "    dX = col2im_cython(X_col, B*C, 1, H_in, W_in, sz, sz, 0, stride).reshape(B,C,H_in,W_in)\n",
    "\n",
    "    return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1cc0e3ea761163b6bb652c231bfa25f0",
     "grade": true,
     "grade_id": "cell-112696546d62d032",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "dY = np.arange(1,17).reshape((2,2,2,2))\n",
    "max_idxs = np.array([1, 3, 0, 2, 3, 3, 0, 1, 0, 2, 0, 3, 3, 3, 1, 3])\n",
    "dX = max_pool_backprop(dY, 2, 2, max_idxs)\n",
    "dX_ref = np.array([ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  2.,  3.,  0.,  0.,  0.,  0.,\n",
    "        0.,  0.,  4.,  0.,  0.,  0.,  0.,  0.,  5.,  0.,  6.,  0.,  0.,\n",
    "        0.,  0.,  7.,  0.,  0.,  8.,  9.,  0., 10.,  0.,  0.,  0.,  0.,\n",
    "        0., 11.,  0.,  0., 12.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 14.,\n",
    "       13.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 15.,  0., 16.]).reshape((2,2,4,4))\n",
    "assert np.array_equal(dX, dX_ref), \"dX is not correct\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d4be2cd2f9cddbab34763250ef8f55be",
     "grade": false,
     "grade_id": "cell-b1eb86b39a8a62a0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Convolutional Layer\n",
    "\n",
    "Implement the forward and backward propagation functions for a convolutional layer. Again, you should use the helper functions provided above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "61d1c5a4074bc54c4f5beb074cd915e3",
     "grade": false,
     "grade_id": "cell-77582995f15e7afb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def conv_layer(X, W, b, padding, stride):\n",
    "    '''\n",
    "    Performs forward propagation through a convolutional layer.\n",
    "    \n",
    "    Inputs\n",
    "        X: input tensor of shape B x C x H x W, where B is the batch dimension and C is the number of channels\n",
    "        W : weights for convolutional filters, of shape (F, C, H, W) where F is the filter index\n",
    "        b : bias coefficients for convolutional filters, array of size F\n",
    "        padding : padding to add to each edge of the image\n",
    "        stride : specifies the stride of the convolution, assumed to be the same in both dimensions\n",
    "    \n",
    "    Outputs\n",
    "        Y: output tensor, result after applying convolutional filters\n",
    "    '''\n",
    "    assert X.ndim == 4, \"Input should be a 4-dimensional tensor of shape B x C x H x W\"\n",
    "    assert W.ndim == 4, \"Weights should be a 4-dimensional tensor of shape B x C x H x W\"\n",
    "    assert b.ndim == 2 and b.shape[1] == 1, \"Offset should be a matrix of size F x 1\"\n",
    "    \n",
    "    # assume X is 10x4x14x14 (10 4-channel images) and W is 9x4x3x3 with stride = 1 and padding = 1\n",
    "    B, C, H_in, W_in = X.shape\n",
    "    num_filters, c_filter, h_filter, w_filter = W.shape\n",
    "    assert C == c_filter, \"Number of channels in X does not match number of channels in W\"\n",
    "    assert b.size == num_filters, \"Size of b does not match number of filters\"\n",
    "\n",
    "    # TO DO:\n",
    "    #  - use im2col function to calculate X_col, of shape 36 x 1960\n",
    "    #  - reshape weights into a matrix of size (9, 36)\n",
    "    #  - compute convolutions as matrix product plus offset, result is 9 x 1960    \n",
    "    #  - calculate H_out, the height of the output tensor\n",
    "    #  - calculate W_out, the width of the output tensor\n",
    "    \n",
    "    #Find X_col and reshape W\n",
    "    X_col = im2col_cython(X, h_filter, w_filter, padding, stride)\n",
    "    W_reshaped = W.reshape(num_filters, h_filter*w_filter*c_filter)\n",
    " \n",
    "    #Calculate Y with multiply\n",
    "    Y = W_reshaped@X_col + b\n",
    "    \n",
    "    #Find the H and W\n",
    "    H_out = int((H_in-h_filter + 2*padding)/stride +1)\n",
    "    W_out = int((W_in-w_filter + 2*padding)/stride +1)\n",
    "    \n",
    "    \n",
    "    # reshape to 10 x num_filters x 14 x 14\n",
    "    Y = Y.reshape(num_filters, H_out, W_out, B)\n",
    "    Y = Y.transpose(3, 0, 1, 2)\n",
    "    \n",
    "    return Y, X_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8b608f3a779dda84f37b87c01f715e20",
     "grade": true,
     "grade_id": "cell-d0292673849dbae5",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "X = np.random.randn(4,2,3,3)\n",
    "W = np.random.randn(3,2,3,3)\n",
    "b = np.random.randn(3).reshape((-1,1))\n",
    "Y, X_col = conv_layer(X, W, b, padding=1, stride=1)\n",
    "Y0_ref = np.array([-3.99137837,  0.78647053,  0.27615542, -2.48079681, -3.09172439,\n",
    "        0.20607078, -0.62932756,  2.10743663,  5.24395253,  6.82293952,\n",
    "        6.64998453,  2.14284251,  7.80405463,  1.01175079, 10.23511379,\n",
    "        5.0491166 ,  5.02819876, -1.49527759,  2.52938873,  5.05618984,\n",
    "       -2.78430683,  5.4168463 ,  5.65276193,  7.93504419,  7.02869897,\n",
    "        5.50315519,  3.81005155])\n",
    "X_col_ref = np.array([0.        , 0.        , 0.        , 0.        , 1.76405235,\n",
    "       0.40015721, 0.        , 2.2408932 , 1.86755799, 0.        ,\n",
    "       0.        , 0.        , 0.        , 0.4105985 , 0.14404357,\n",
    "       0.        , 0.76103773, 0.12167502])\n",
    "assert np.allclose(X_col_ref, X_col[:,0]), \"X_col is incorrect\"\n",
    "assert np.allclose(Y0_ref.reshape((3,3,3)), Y[0]), \"Output is incorrect\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8520f8754a334d3cb41aab73c6ee4afb",
     "grade": false,
     "grade_id": "cell-285c0d25716cd991",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def conv_layer_backprop(dY, W, b, padding, stride, X, X_col):\n",
    "    '''\n",
    "    Performs back propagation through a convolutional layer.\n",
    "    \n",
    "    Inputs\n",
    "        dY: gradient of loss with respect to the convolutional layer output, shape BxCxHxW\n",
    "        W : weights for convolutional filters, of shape (F, C, H, W) where F is the filter index\n",
    "        b : bias coefficients for convolutional filters, array of size F\n",
    "        padding : padding to add to each edge of the image\n",
    "        stride : specifies the stride of the convolution, assumed to be the same in both dimensions\n",
    "        X : input tensor of shape B x C x H x W, where B is the batch dimension and C is the number of channels\n",
    "        X_col : reshaped input after im2col, computed during forward propagation\n",
    "    \n",
    "    Outputs\n",
    "        dX : gradient of loss with respect to the convolutional layer input\n",
    "        dW : gradient of loss with respect to the convolutiona layer filter weights\n",
    "        db : gradient of loss with respect to the convolutiona layer bias coefficients\n",
    "        \n",
    "    '''\n",
    "    assert dY.ndim == 4, \"Gradients should be a 4-dimensional tensor of shape B x C x H x W\"\n",
    "    assert W.ndim == 4, \"Weights should be a 4-dimensional tensor of shape F x C x H x W\"\n",
    "    assert b.ndim == 2 and b.shape[1] == 1, \"Offset should be a matrix of size F x 1\"\n",
    "    assert X.ndim == 4, \"Input should be a 4-dimensional tensor of shape B x C x H x W\"\n",
    "    \n",
    "    # e.g. assume X is 10x4x14x14, Y is 10x9x14x14, W is 9x4x3x3 with stride = 1 and padding = 1\n",
    "    n_filter, d_filter, h_filter, w_filter = W.shape\n",
    "    batch_sz = dY.shape[0]\n",
    "    \n",
    "    # TO DO: calculate gradient of bias coefficients by\n",
    "    #   - summing gradients over all pixel positions\n",
    "    #   - averaging over all images in batch\n",
    "    #   - reshaping to appropriate size, 9x1 in example\n",
    "    db = dY.reshape(dY.shape[0], dY.shape[1], dY.shape[2]*dY.shape[3])\n",
    "    db = np.sum(db, axis = 2)\n",
    "    db = np.average(db, axis = 0)\n",
    "    db = db.reshape(-1, 1)\n",
    "\n",
    "\n",
    "    # TO DO: calculate gradient of conv layer filter weights by\n",
    "    #   - rearranging and reshaping output gradients, size 9 x 1960 in example [provided]\n",
    "    #   - calculate appropriate matrix product, result is size 9x36 in example\n",
    "    #   - reshape to final desired size, 9x4x3x3 in example\n",
    "    \n",
    "    dY_reshaped = dY.transpose(1, 2, 3, 0).reshape(n_filter, -1) \n",
    "    dW = np.matmul(dY_reshaped, X_col.T)\n",
    "    dW = dW.reshape(n_filter, d_filter, h_filter, w_filter)/dY.shape[0]\n",
    "\n",
    "    # TO DO: calculate gradient of convolutional layer inputs by\n",
    "    #   - reshaping conv layer filter weights (each row -> 1 filter), result is size 9x36 in example \n",
    "    #   - calculate dX_col as an appropriate matrix product, size 36x1960\n",
    "    #   - use col2im function to calculate dX, size 10x4x14x14\n",
    "    \n",
    "    W_reshaped = W.reshape(-1,w_filter*h_filter*d_filter)\n",
    "    #dY_reshaped = dY.transpose(0,2,3,1).reshape(-1, dY.shape[1]).T\n",
    "\n",
    "    dX_col = np.matmul(W_reshaped.T, dY_reshaped)\n",
    "    B,C,H_in, W_in = X.shape\n",
    "    dX = col2im_cython(dX_col, X.shape[0], X.shape[1], X.shape[2], X.shape[3],\n",
    "                       h_filter, w_filter, padding, stride)\n",
    "    \n",
    "        \n",
    "    return dX, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "33cadd7e56f4e3cfdd7afdc8758c52c6",
     "grade": true,
     "grade_id": "cell-0405da9fade9f4a9",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "X = np.zeros((4,2,3,3))\n",
    "X_col = np.random.randn(18, 36)\n",
    "W = np.random.randn(3,2,3,3)\n",
    "b = np.random.randn(3).reshape((-1,1))\n",
    "dY = np.random.randn(4,3,3,3)\n",
    "dX, dW, db = conv_layer_backprop(dY, W, b, 1, 1, X, X_col)\n",
    "dX3_ref = np.array([ 0.15393451,  9.80339438,  0.81894896,  6.17476504, -0.31962813,\n",
    "       -6.17407366, -3.57093524, -6.60591174,  3.21924057,  1.90177136,\n",
    "        2.05313522, -2.09301809, -4.71792436, -0.02449646, -1.54804584,\n",
    "       -2.7598266 ,  0.02735023,  3.94832202])\n",
    "dW0_ref = np.array([ 2.56976449,  0.26943072,  0.67234007, -1.43669202,  2.53140543,\n",
    "        2.9665219 ,  0.45886205, -0.62803276,  0.74089213, -2.05960387,\n",
    "        0.37660744,  0.35757989,  0.07539734, -2.61404337, -1.7515867 ,\n",
    "       -1.44254232,  0.97522927, -0.80112989])\n",
    "db_ref = np.array([-0.99280792, -0.99867817,  0.15267287])\n",
    "assert np.allclose(dX[3].reshape(-1), dX3_ref), \"Xgrad is not correct\"\n",
    "assert np.allclose(dW[0].reshape(-1), dW0_ref), \"weightsgrad is not correct\"\n",
    "assert np.allclose(db.reshape(-1), db_ref), \"bgrad is not correct\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "48704aa0389f6509014aab50b373c85b",
     "grade": false,
     "grade_id": "cell-a34e52c65a3596dd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Part 2\n",
    "\n",
    "In this part you will combine the building blocks together to construct a CNN model.  You will implement the following:\n",
    "- model initialization\n",
    "- forward propagation through model\n",
    "- back propagation through model\n",
    "- cross-entropy loss function\n",
    "- gradient checking\n",
    "- training loop\n",
    "- predicting on images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a09e03b010e476504d52185e36680221",
     "grade": false,
     "grade_id": "cell-96119ed0128a625d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We will use a slightly modified LeNet-5 architecture, which is summarized below.  The dimensions in parentheses show the output shape after each building block, using the convention channel x height x width (batch dimension not shown for brevity).\n",
    "- Conv layer 1 (input shape 1x28x28)\n",
    "    * convolutional layer (6 filters, 5x5, padding 2, stride 1) (6x28x28)\n",
    "    * ReLU activation (6x28x28)\n",
    "    * max pooling (2x2, no padding, stride 2) (6x14x14)\n",
    "- Conv layer 2 (input shape 6x14x14)\n",
    "    * convolutional layer (16 filters, 5x5, no padding, stride 1) (16x10x10)\n",
    "    * ReLU activation (16x10x10)\n",
    "    * max pooling (2x2, no padding, stride 2) (16x5x5)\n",
    "- Flattening (input shape 16x5x5)\n",
    "- Dense layer 1 (input shape 400)\n",
    "    * linear layer, 120 units (120)\n",
    "    * ReLU activation (120)\n",
    "- Dense layer 2 (input shape 120)\n",
    "    * linear layer, 84 units (84)\n",
    "    * ReLU activation (84)\n",
    "- Output layer (input shape 84)\n",
    "    * linear layer, 10 units (10)\n",
    "    * Softmax (10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "61f0b36c88081bd26959e4a8c24b07b9",
     "grade": false,
     "grade_id": "cell-0bb7c7aa24ce33c9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Model Initialization\n",
    "\n",
    "Implement the function below to initialize the CNN model parameters using He initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "843e5574f90138d9a0f31f9cd4556905",
     "grade": false,
     "grade_id": "cell-d010bbfc32e03fef",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def initialize_model_parameters(seed = 0):\n",
    "    '''\n",
    "    Initializes the parameters in the CNN model.\n",
    "    \n",
    "        W1, b1 -> conv layer 1\n",
    "        W2, b2 -> conv layer 2\n",
    "        W3, b3 -> dense layer 1\n",
    "        W4, b4 -> dense layer 2\n",
    "        W5, b5 -> output layer    \n",
    "      \n",
    "    The conv layer weights are size (F, C, H, W), where F is the filter index and C is channel.\n",
    "    The dense layer weights are size (n_in, n_out), where n_in is the number of input units.\n",
    "    \n",
    "    Parameters are returned in a tuple.\n",
    "    '''\n",
    "    #Declare the sizes from above\n",
    "    w1Size  = (6,1,5,5)\n",
    "    w2Size = (16,6,5,5)\n",
    "    w3Size = (400,120)\n",
    "    w4Size = (120,84)\n",
    "    w5Size = (84,10)\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "        \n",
    "    b1 = np.zeros((6, 1))\n",
    "    b2 = np.zeros((16, 1))\n",
    "    b3 = np.zeros((1, 120))\n",
    "    b4 = np.zeros((1, 84))\n",
    "    b5 = np.zeros((1, 10))\n",
    "    \n",
    "    # TO DO:\n",
    "    #  - initialize W1, W2, W3, W4, W5 using He initialization (must be defined in that order)\n",
    "    \n",
    "    mu = 0\n",
    "    #Initialize with He initialization. Use the given sizes and find standard deviations\n",
    "    \n",
    "    w1Sigma = np.sqrt(2/(w1Size[1]*w1Size[2]*w1Size[3]))\n",
    "    w2Sigma = np.sqrt(2/(w2Size[1]* w2Size[2]*w2Size[3]))\n",
    "    w3Sigma = np.sqrt(2/w3Size[0])\n",
    "    w4Sigma = np.sqrt(2/w4Size[0])\n",
    "    w5Sigma = np.sqrt(2/w5Size[0])\n",
    "    \n",
    "    #Initialize with the standard deviations\n",
    "    W1 = np.random.normal(mu, w1Sigma, w1Size)\n",
    "    W2 = np.random.normal(mu, w2Sigma, w2Size)\n",
    "    W3 = np.random.normal(mu, w3Sigma, w3Size)\n",
    "    W4 = np.random.normal(mu, w4Sigma, w4Size)\n",
    "    W5 = np.random.normal(mu, w5Sigma, w5Size)\n",
    "\n",
    "    params = (W1, b1, W2, b2, W3, b3, W4, b4, W5, b5)\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6c490efb5fb069c4c6d36eabcf3e5772",
     "grade": true,
     "grade_id": "cell-51b18bfc6af86619",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "(W1, b1, W2, b2, W3, b3, W4, b4, W5, b5) = initialize_model_parameters()\n",
    "assert np.allclose(W1[0,0,0,:], np.array([0.49894935, 0.11318155, 0.27682891, 0.63382031, 0.52822517])), \"W1 incorrect\"\n",
    "assert np.allclose(W2[0,0,0,:], np.array([-0.00787986,  0.19783978, -0.08599688, -0.0954289 , -0.01136832])), \"W2 incorrect\"\n",
    "assert np.allclose(W3[0,0:5], np.array([-0.10336051, -0.10951643,  0.00894977,  0.05607407,  0.04511511])), \"W3 incorrect\"\n",
    "assert np.allclose(W4[0,0:5], np.array([-1.73308838e-01, -2.27854099e-01, -1.04538200e-01, -1.65899206e-04, 1.87606738e-01])), \"W4 incorrect\"\n",
    "assert np.allclose(W5[0,0:5], np.array([ 0.09371314, -0.00454455, -0.00378867,  0.12806951, -0.03230202])), \"W5 incorrect\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6ab913a852735e84134ac995cd860367",
     "grade": true,
     "grade_id": "cell-a9ace74aaaf85fc4",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#intentionally blank\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bfd4ac40230bdb9cbf5c856cad2c727a",
     "grade": false,
     "grade_id": "cell-d1a047e051b141aa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Model implementation\n",
    "\n",
    "Implement forward and backward propagation for the model described above.  You should use the building block functions you implemented in Part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bc7afd486380a687a0a8535653a216c6",
     "grade": false,
     "grade_id": "cell-79342b2be2c6e94b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def cnn_forward_propagation(X, params):\n",
    "    '''\n",
    "    Performs forward propagation through the CNN model.\n",
    "    \n",
    "    Inputs\n",
    "        X : Input tensor of shape BxCxHxW, where B is the batch dimension and C is the number of image channels\n",
    "        params : CNN model parameters\n",
    "    \n",
    "    Outputs\n",
    "        Y : Output tensor of shape Bx10 containing the output probabilities of all 10 digits\n",
    "        cache : contains information that is necessary to perform backpropagation\n",
    "    '''\n",
    "    W1, b1, W2, b2, W3, b3, W4, b4, W5, b5 = params\n",
    "    cache = []\n",
    "    \n",
    "    #Conv layer 1\n",
    "    X1, X_col1 = conv_layer(X, W1, b1, 2, 1)\n",
    "    R1 = ReLU(X1)\n",
    "    P1, max_idx1 = max_pool(R1, 2, 2)\n",
    "    \n",
    "\n",
    "    \n",
    "    #Conv layer 2\n",
    "    X2, X_col2 = conv_layer(P1, W2, b2, 0, 1)\n",
    "    R2 = ReLU(X2)\n",
    "    P2, max_idx2 = max_pool(R2, 2, 2)\n",
    "    \n",
    "\n",
    "    \n",
    "    #Flattening\n",
    "    X3 = flatten(P2)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Linear 1\n",
    "    X4 = linear_layer(X3, W3, b3)\n",
    "    R4 = ReLU(X4)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Linear 2\n",
    "    X5 = linear_layer(R4, W4, b4)\n",
    "    R5 = ReLU(X5)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Output\n",
    "    X6 = linear_layer(R5, W5, b5)\n",
    "    Y = softmax(X6)\n",
    "    \n",
    "    \n",
    "    #Save all intermediates and necessary things\n",
    "    cache.append(X)\n",
    "    cache.append(X1)\n",
    "    cache.append(R1)\n",
    "    cache.append(P1)\n",
    "    \n",
    "    cache.append(X2)\n",
    "    cache.append(R2)\n",
    "    cache.append(P2)\n",
    "    \n",
    "    cache.append(X3)\n",
    "    \n",
    "    cache.append(X4)\n",
    "    cache.append(R4)\n",
    "    \n",
    "    cache.append(X5)\n",
    "    cache.append(R5)\n",
    "\n",
    "    cache.append(X_col1)\n",
    "    cache.append(max_idx1)\n",
    "    cache.append(X_col2)\n",
    "    cache.append(max_idx2)\n",
    "    return Y, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2f0ff47c6171089f9cb47e9d0275ef8c",
     "grade": false,
     "grade_id": "cell-8e09cb8ff6129f8e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Note that the unit test below does not check the cache variable, since you can define that however you wish.  You won't be able to implement the backpropagation function without the cache information though!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cc286010fb8ebafb859938dc5ce458c1",
     "grade": true,
     "grade_id": "cell-a6359a8fb78fd044",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "params = initialize_model_parameters()\n",
    "np.random.seed(0)\n",
    "X = np.random.randn(5,1,28,28)\n",
    "Y, cache = cnn_forward_propagation(X, params)\n",
    "assert np.allclose(Y[0,:], np.array([0.47816414, 0.00160818, 0.0038063 , 0.00636769, 0.36472729,\n",
    "       0.00920153, 0.01656039, 0.00104739, 0.06269261, 0.05582447])), \"Y is incorrect\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "66c6081b4c819daaef3a990eb7d10314",
     "grade": false,
     "grade_id": "cell-43165d15786d68d7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def cnn_backward_propagation(preds, targets, cache):\n",
    "    '''\n",
    "    Performs backward propagation through the CNN model.\n",
    "    \n",
    "    Inputs\n",
    "        preds : the model predictions for a single batch, size B x 10\n",
    "        targets : the one-hot encoded labels for a single batch, size B x 10\n",
    "        cache : the cached information from forward propagation\n",
    "        \n",
    "    Outputs\n",
    "        grad : gradient of loss with respect to all model parameters\n",
    "    '''\n",
    "    \n",
    "    # TO DO:\n",
    "    #  - unpack cache into separate variables\n",
    "    #  - calculate gradient of loss with respect to softmax inputs\n",
    "    #  - backpropagate gradient through all layers of model\n",
    "    \n",
    "    #Retrieve from cache\n",
    "    X, X1, R1, P1, X2, R2, P2, X3, X4, R4, X5, R5, X_col1, max_idx1, X_col2, max_idx2 = cache\n",
    "    \n",
    "    \n",
    "    #Softmax inputs grad\n",
    "    dZ = softmax_grad(preds, targets)\n",
    "    \n",
    "    #Linear 3 backprop\n",
    "    dX5, dW5, db5 = linear_layer_backprop(dZ, R5, W5, b5)\n",
    "    \n",
    "    #Linear 2 backprop\n",
    "    dZ = ReLU_backprop(dX5, X5)\n",
    "    dX4, dW4, db4 = linear_layer_backprop(dZ, R4, W4, b4)\n",
    "    \n",
    "    #Linear 1 backprop\n",
    "    dZ = ReLU_backprop(dX4, X4)\n",
    "    dX3, dW3, db3 = linear_layer_backprop(dZ, X3, W3, b3)\n",
    "    \n",
    "    #Flatten backprop\n",
    "    dZ = flatten_backprop(dX3, P2.shape[1], P2.shape[2], P2.shape[3])\n",
    "    \n",
    "    #Conv 2 backprop\n",
    "    dZ = max_pool_backprop(dZ, 2, 2, max_idx2)\n",
    "    dZ = ReLU_backprop(dZ, X2)\n",
    "    dX2, dW2, db2 = conv_layer_backprop(dZ, W2, b2, 0, 1, P1, X_col2)\n",
    "    \n",
    "    #Conv 1 backprop\n",
    "    dZ = max_pool_backprop(dX2, 2, 2, max_idx1)\n",
    "    dZ = ReLU_backprop(dZ, X1)\n",
    "    dX1, dW1, db1 = conv_layer_backprop(dZ, W1, b1, 2, 1, X, X_col1)\n",
    "    \n",
    "    grad = (dW1, db1, dW2, db2, dW3, db3, dW4, db4, dW5, db5)\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7a5ce2e021cd4a686a05d78bfa332591",
     "grade": true,
     "grade_id": "cell-62aa22336958da52",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "params = initialize_model_parameters()\n",
    "np.random.seed(0)\n",
    "X = np.random.randn(5,1,28,28)\n",
    "Yhat, cache = cnn_forward_propagation(X, params)\n",
    "idxs = np.argmax(np.random.random(Yhat.shape), axis=1)\n",
    "Y = np.zeros(Yhat.shape)\n",
    "Y[np.arange(Yhat.shape[0]), idxs] = 1\n",
    "dW1, db1, dW2, db2, dW3, db3, dW4, db4, dW5, db5 = cnn_backward_propagation(Yhat, Y, cache)\n",
    "assert np.allclose(dW5[0,0:5], np.array([0.16956413, 0.00056458, 0.00133372, 0.00227225, 0.12484518])), \"dW5 is incorrect\"\n",
    "assert np.allclose(db5[0,0:5], np.array([ 0.35241545, -0.19925145,  0.00224337, -0.18095215,  0.15588982])), \"db5 is incorrect\"\n",
    "assert np.allclose(dW4[1,0:5], np.array([0.03104646, 1.04463999, 0.13652738, 0.36762806, 0.10599433])), \"dW4 is incorrect\"\n",
    "assert np.allclose(db4[0,0:5], np.array([0.00472395, 0.26505554, 0.02937651, 0.10817509, 0.02289335])), \"db4 is incorrect\"\n",
    "assert np.allclose(dW3[0,0:5], np.array([ 0.        , -0.01011244,  0.        ,  0.01330549, -0.01228247])), \"dW3 is incorrect\"\n",
    "assert np.allclose(db3[0,0:5], np.array([ 0.        , -0.12108738,  0.        ,  0.02674001, -0.08492108])), \"db3 is incorrect\"\n",
    "assert np.allclose(dW2[0,0,0,:], np.array([ 0.04234263, -0.18558587, -0.11940635, -0.06108712, -0.2253452 ])), \"dW2 is incorrect\"\n",
    "assert np.allclose(db2[0:5,0], np.array([-0.05223691,  0.01643131,  0.0061777 ,  0.02526632, -0.01331299])), \"db2 is incorrect\"\n",
    "assert np.allclose(dW1[0,0,0,:], np.array([ 0.35738778,  0.13901316,  0.10407159, -0.10889821,  0.02460675])), \"dW1 is incorrect\"\n",
    "assert np.allclose(db1.reshape(-1), np.array([ 0.17264439,  0.88121064,  0.2280625 ,  0.13586609,  0.31916121, -0.11240721])), \"db1 is incorrect\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c41e49407322ee6c5f857dd5d0eeb1db",
     "grade": true,
     "grade_id": "cell-bc8ee2476c054b2b",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#intentionally blank\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "70ea6ea94c4213d1591cedc7d44c6d8b",
     "grade": false,
     "grade_id": "cell-708ee5f7f3d42d38",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Cross Entropy Loss\n",
    "\n",
    "Implement the cross entropy loss function.  You should average the loss across the data samples in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e241e0acaaa8040df9ff42af62d70598",
     "grade": false,
     "grade_id": "cell-fd29e8c260177ff4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def cross_entropy_loss(A, Y):\n",
    "    '''\n",
    "    Calculates the cross-entropy loss for a set of output probabilities and target labels.\n",
    "    \n",
    "    Inputs\n",
    "        A: a matrix where each row specifies the output layer activations for a single input\n",
    "        Y: a matrix where each row specifies the one-hot encoded target label for a single input.\n",
    "        \n",
    "    Output\n",
    "        J: the cross entropy loss averaged across all data samples\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    m = A.shape[0]\n",
    "    \n",
    "    #In case A is zero, clip to just above zero\n",
    "    epsilon = 1e-12\n",
    "    A = np.clip(A,epsilon, 1.-epsilon)\n",
    "    \n",
    "    \n",
    "    J = np.sum(np.multiply(np.log(A),Y))/(-m)\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f350018e7b88ffa8642c122ee938d920",
     "grade": true,
     "grade_id": "cell-ad8da40404799f0f",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "A = np.array([[.25, .20, .05, .5], [.7, .1, .1, .1]])\n",
    "Y = np.array([[0, 1, 0, 0], [1, 0, 0, 0]])\n",
    "J = cross_entropy_loss(A, Y)\n",
    "assert np.isscalar(J), \"Output is not a scalar.\"\n",
    "assert np.round(J, decimals=8) == 0.98305643, \"Returns incorrect loss value.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a627b8f01a62c76bbcb9a8281a5033d4",
     "grade": false,
     "grade_id": "cell-627f87394a6a70b2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Gradient Checking\n",
    "\n",
    "Implement a gradient checking function to make sure your backpropagation is correct.  Below are some helpful utility functions that you can use in your implementation.  Note that this function will take a while to run, so I recommend two things: \n",
    "- you perform gradient checking on a small subset of the parameters at first, so that you can get immediate feedback while debugging\n",
    "- you put in a print statement that periodically indicates progress, so you know roughly how long the test will take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2dce94ec0dfca60decfcef79d0c7d216",
     "grade": false,
     "grade_id": "cell-1f9323ff873d74d0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def flatten_model_params(params):\n",
    "    '''\n",
    "    Flattens all model parameters into a single long array.\n",
    "    \n",
    "    Inputs\n",
    "        params : tuple containing all model parameters\n",
    "        \n",
    "    Outputs\n",
    "        V : a 1-dim numpy array containing all model parameters\n",
    "    '''\n",
    "    W1, b1, W2, b2, W3, b3, W4, b4, W5, b5 = params\n",
    "    V = np.concatenate((W1.flatten(), b1.flatten(), W2.flatten(), b2.flatten(),\n",
    "                      W3.flatten(), b3.flatten(), W4.flatten(), b4.flatten(), W5.flatten(), b5.flatten()))\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9321db9bd9fdeebb908a75403d2a0463",
     "grade": false,
     "grade_id": "cell-8c1d699ad058fddd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def unflatten_model_params(V, params_sample):\n",
    "    '''\n",
    "    Takes a flattened array of model parameters and reformats it into a tuple of structured matrices.\n",
    "    \n",
    "    Inputs\n",
    "        V : 1-dim array containing all model parameters\n",
    "        params_sample : a sample list of model parameters to specify the number and sizes of all matrices\n",
    "        \n",
    "    Outputs\n",
    "        params : tuple containing model parameters\n",
    "    '''\n",
    "    offset = 0\n",
    "    params = []\n",
    "    for var in params_sample:\n",
    "        numCoeffs = len(var.flatten())\n",
    "        reshaped = V[offset:offset+numCoeffs].reshape(var.shape)\n",
    "        params.append(reshaped)\n",
    "        offset += numCoeffs\n",
    "    \n",
    "    return tuple(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f30750f6e5bc77693dc3ab347422be27",
     "grade": false,
     "grade_id": "cell-b8858a03e695986f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def gradient_checking(params, X, Y, epsilon = 1e-8):\n",
    "    \n",
    "    # TO DO:\n",
    "    #  - calculate the gradient using your cnn_backward_propagation function (grad_flattened)\n",
    "    #  - calculate the approximate gradient by perturbing each parameter +/- epsilon (grad_approx)\n",
    "    \n",
    "    preds, cache = cnn_forward_propagation(X, params)\n",
    "    grad_flattened = flatten_model_params(cnn_backward_propagation(preds, Y, cache))\n",
    "    grad_approx = np.zeros_like(grad_flattened)\n",
    "    V = flatten_model_params([W1,b1,W2,b2,W3,b3,W4,b4,W5,b5])\n",
    "\n",
    "    for i in range(len(V)):\n",
    "       \n",
    "        V_plus = V.copy()\n",
    "        V_plus[i] += epsilon\n",
    "        V_minus = V.copy()\n",
    "        V_minus[i] -= epsilon\n",
    "\n",
    "        #Calculate the slightly shifted weights\n",
    "        W1_plus, b1_plus, W2_plus, b2_plus, W3_plus, b3_plus, W4_plus, b4_plus, W5_plus, b5_plus = unflatten_model_params(V_plus, params)\n",
    "        W1_minus, b1_minus, W2_minus, b2_minus, W3_minus, b3_minus, W4_minus, b4_minus, W5_minus, b5_minus = unflatten_model_params(V_minus, params)\n",
    "        #Forward propagate\n",
    "        Yp, cachep = cnn_forward_propagation(X, [W1_plus, b1_plus, W2_plus, b2_plus, W3_plus, b3_plus, W4_plus, b4_plus, W5_plus, b5_plus])\n",
    "        Ym, cachem= cnn_forward_propagation(X, [W1_minus, b1_minus, W2_minus, b2_minus, W3_minus, b3_minus, W4_minus, b4_minus, W5_minus, b5_minus])\n",
    "\n",
    "        #Find losses for each\n",
    "        lossp = cross_entropy_loss(Yp, Y)\n",
    "        lossm = cross_entropy_loss(Ym, Y)\n",
    "\n",
    "        #Calculate the approximate grad based on the difference in losses\n",
    "        a_gradval = (lossp-lossm)/(2*epsilon)\n",
    "        grad_approx[i] = a_gradval\n",
    "        if i%5000 == 0:\n",
    "            print(str((100*i)/len(V)) + \" percent complete\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    err = np.linalg.norm(grad_flattened - grad_approx)\n",
    "    rel_err = err/(np.linalg.norm(grad_flattened) + np.linalg.norm(grad_approx))\n",
    "    print(\"Relative error in gradient approximation: {}\".format(rel_err))\n",
    "    if rel_err < 1e-7:\n",
    "        print('Passes gradient checking')\n",
    "        return True\n",
    "    else:\n",
    "        print('Fails gradient checking')        \n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e4ac44d45a0619272052af2c5733208f",
     "grade": true,
     "grade_id": "cell-49b99aa36b5b3a46",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 percent complete\n",
      "8.102939746540045 percent complete\n",
      "16.20587949308009 percent complete\n",
      "24.308819239620135 percent complete\n",
      "32.41175898616018 percent complete\n",
      "40.514698732700225 percent complete\n",
      "48.61763847924027 percent complete\n",
      "56.720578225780315 percent complete\n",
      "64.82351797232036 percent complete\n",
      "72.9264577188604 percent complete\n",
      "81.02939746540045 percent complete\n",
      "89.1323372119405 percent complete\n",
      "97.23527695848054 percent complete\n",
      "Relative error in gradient approximation: 7.389094877651427e-08\n",
      "Passes gradient checking\n"
     ]
    }
   ],
   "source": [
    "params = initialize_model_parameters()\n",
    "np.random.seed(0)\n",
    "X = np.random.randn(1,1,28,28)\n",
    "Y = np.zeros((1,10))\n",
    "Y[0,4] = 1\n",
    "passes = gradient_checking(params, X, Y)\n",
    "assert passes == True, \"Does not pass gradient checking\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a6f1cc36c95fa07620c5b3be3c44a3aa",
     "grade": false,
     "grade_id": "cell-1c333e805ef30465",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Training Model\n",
    "\n",
    "Implement a training loop using gradient descent with momentum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f73ebf3c45cb00ec77a02777e7cca3fa",
     "grade": false,
     "grade_id": "cell-89810a3b8742cf0d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def train_cnn_model(X_train, Y_train, X_valid, Y_valid, params, batch_size, alpha, beta, num_epochs):\n",
    "    '''\n",
    "    Learn the CNN model weights on a set of training examples.\n",
    "    \n",
    "    Inputs\n",
    "        X_train : a matrix containing the training data, where each row corresponds to a single training example\n",
    "        Y_train : a matrix containing the training labels, where each row contains a one hot encoded target label.\n",
    "        X_valid : a matrix containing the validation data\n",
    "        Y_valid : a matrix containing the validation labels\n",
    "        params : model parameters\n",
    "        batch_size : batch size used in mini-batch gradient descent\n",
    "        alpha : the learning rate\n",
    "        beta : the momentum coefficient\n",
    "        num_epochs: the number of times to iterate over the training data\n",
    "    \n",
    "    Outputs\n",
    "        params : learned weights of CNN model\n",
    "        hist : list of (loss_train, loss_valid) tuples for each epoch of training        \n",
    "    '''\n",
    "    N = X_train.shape[0]\n",
    "    M = X_valid.shape[0]\n",
    "    hist = []\n",
    "    V = np.zeros(len(flatten_model_params(params)))\n",
    "    \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        train_idxs_all = np.arange(N) # shuffle training data\n",
    "        np.random.shuffle(train_idxs_all) \n",
    "        train_loss = 0 # accumulate losses from mini-batches \n",
    "\n",
    "        for i in range(0, N, batch_size):\n",
    "            \n",
    "            idxs_batch = train_idxs_all[i:min(i+batch_size,N)]\n",
    "            X_batch = X_train[idxs_batch,:]\n",
    "            Y_batch = Y_train[idxs_batch,:]\n",
    "        \n",
    "            # TO DO:\n",
    "            #  - update model parameters (gradient descent with momentum)\n",
    "            #  - calculate batch loss\n",
    "            \n",
    "            #Forward and back prop\n",
    "            preds, cache = cnn_forward_propagation(X_batch, params)\n",
    "            grad = flatten_model_params(cnn_backward_propagation(preds, Y_batch, cache))\n",
    "            \n",
    "            #update with momentum\n",
    "            V = (beta*V +  grad*(1-beta))\n",
    "            params = unflatten_model_params(flatten_model_params(params) - alpha * V, params)\n",
    "            \n",
    "            #Calc loss\n",
    "            batch_loss = cross_entropy_loss(preds, Y_batch)\n",
    "            train_loss += batch_loss * X_batch.shape[0]\n",
    "        \n",
    "        train_loss = train_loss / N\n",
    "        \n",
    "        # TO DO:\n",
    "        #  - calculate validation loss\n",
    "        predsV, cache = cnn_forward_propagation(X_valid, params)\n",
    "        valid_loss = cross_entropy_loss(predsV, Y_valid)\n",
    "        \n",
    "        \n",
    "        print('Epoch {}: Training Loss = {:.5f}, Validation Loss = {:.5f}'.format(epoch, train_loss, valid_loss))\n",
    "        hist.append((train_loss, valid_loss))\n",
    "\n",
    "    return params, hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f79e19294ff5f1bb78a3abffd98e1f64",
     "grade": true,
     "grade_id": "cell-b14fc438dbf57327",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Training Loss = 4.71621, Validation Loss = 3.59759\n",
      "Epoch 1: Training Loss = 4.07598, Validation Loss = 3.17934\n",
      "Epoch 2: Training Loss = 3.34654, Validation Loss = 2.88531\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "X_train = np.random.randn(20,1,28,28)\n",
    "Y_train = np.zeros((20,10))\n",
    "idxs = np.argmax(np.random.randn(20,10), axis=1)\n",
    "Y_train[np.arange(20), idxs] = 1\n",
    "X_valid = np.random.randn(10,1,28,28)\n",
    "Y_valid = np.zeros((10,10))\n",
    "idxs = np.argmax(np.random.randn(10,10), axis=1)\n",
    "Y_valid[np.arange(10), idxs] = 1\n",
    "params = initialize_model_parameters()\n",
    "params, hist = train_cnn_model(X_train, Y_train, X_valid, Y_valid, params, 5, .001, .9, 3)\n",
    "assert np.allclose(params[0][0,0,0,:], np.array([0.4990256 , 0.11226664, 0.2764398 , 0.63352641, 0.52779482])), \"Trained weights are incorrect\"\n",
    "assert np.allclose(np.array(hist[2]), np.array([3.34653779, 2.88531046])), \"Loss values are incorrect\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1481e7685ebe3e6f3c72b34dfe997f78",
     "grade": false,
     "grade_id": "cell-540114b146348088",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Prediction\n",
    "\n",
    "Implement a function to make a prediction on a batch of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "37739208c55e756d58bec16c2a98aea6",
     "grade": false,
     "grade_id": "cell-817a865b8b17f3eb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def predict(params, X):\n",
    "    '''\n",
    "    Returns the predicted labels and class probabilities for a batch of images.\n",
    "    \n",
    "    Inputs\n",
    "        params : CNN model parameters\n",
    "        X : a batch of images, shape B x C x H x W\n",
    "        \n",
    "    Outputs\n",
    "        Y : vector of integers specifying the predicted class labels.\n",
    "        P : matrix of probabilities, where each row specifies the class probabilities for a single data sample.\n",
    "    '''\n",
    "    \n",
    "    #Run forward prop\n",
    "    P, cache = cnn_forward_propagation(X, params)\n",
    "    #Pick max element\n",
    "    Y = np.argmax(P, axis = 1)\n",
    "    \n",
    "    return Y, P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "84253f7778fabf98c3c4e52988e15a09",
     "grade": true,
     "grade_id": "cell-f6d2c46c7592d93e",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "X_batch = np.random.randn(3,1,28,28)\n",
    "params = initialize_model_parameters()\n",
    "Y, P = predict(params, X_batch)\n",
    "assert np.array_equal(Y, np.array([0,4,0])), \"Predicted labels are incorrect\"\n",
    "assert np.allclose(P[0], np.array([0.47816414, 0.00160818, 0.0038063 , 0.00636769, 0.36472729,\n",
    "       0.00920153, 0.01656039, 0.00104739, 0.06269261, 0.05582447])), \"Class probabilities are incorrect\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "52467adaabeca2caf2c142ff1ed55825",
     "grade": false,
     "grade_id": "cell-f253e34339515f5f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Part 3\n",
    "\n",
    "In the third part of this assignment, you will use your CNN implementation to classify handwritten digits on the MNIST dataset.  There are two deliverables for this part:\n",
    "- A well-labeled figure that shows the training and validation loss vs epoch.\n",
    "- A cell that prints out the classification accuracy of your trained model on the validation data.  Classification accuracy is simply the percent of the images that are classified as the correct digit.\n",
    "\n",
    "You may use as many code and Markdown cells as needed.  We have included some code to retrieve the data.\n",
    "\n",
    "Note: The training will take a long time, especially if you use the slower python implementation of im2col.  Train it long enough to show that it is converging, but don't worry about training it until convergence.  And make sure you don't use too small of a learning rate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fcc11a7c99ebe6353792d332944c7d6d",
     "grade": false,
     "grade_id": "cell-b0e0095176d404b1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist_data():\n",
    "    '''\n",
    "    Retrieves and formats the MNIST data.  We will only use a subset of the data to speed up training.\n",
    "    '''\n",
    "    (X_train, Y_train), (X_val, Y_val) = tf.keras.datasets.mnist.load_data()\n",
    "    X_train = X_train.reshape((X_train.shape[0], -1))/255.0\n",
    "    X_val = X_val.reshape((X_val.shape[0],-1))/255.0\n",
    "    Y_train = pd.get_dummies(Y_train).to_numpy() # convert to one-hot encoding\n",
    "    Y_val = pd.get_dummies(Y_val).to_numpy()\n",
    "    return X_train[0:5000,:], Y_train[0:5000,:], X_val[0:5000,:], Y_val[0:5000,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_val, Y_val = load_mnist_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshape and re-type data\n",
    "X_train = X_train.reshape((X_train.shape[0], 1, 28, 28)).astype(float)\n",
    "X_val = X_val.reshape((X_val.shape[0], 1, 28, 28)).astype(float)\n",
    "Y_train = Y_train.astype(float)\n",
    "Y_val = Y_val.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 1, 28, 28)\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f2b98353640>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Check that data still works\n",
    "print(X_train.shape)\n",
    "test = 1\n",
    "sample_img = np.squeeze(X_train[test,:])\n",
    "print(Y_train[test])\n",
    "plt.imshow(sample_img, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "76321da097911d6d5c08602f8dbd3910",
     "grade": true,
     "grade_id": "cell-9c730f6ea91e3306",
     "locked": false,
     "points": 15,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Training Loss = 2.24558, Validation Loss = 2.00317\n",
      "Epoch 1: Training Loss = 1.79316, Validation Loss = 1.55150\n",
      "Epoch 2: Training Loss = 1.26715, Validation Loss = 1.11682\n",
      "Epoch 3: Training Loss = 0.86862, Validation Loss = 0.82318\n",
      "Epoch 4: Training Loss = 0.63493, Validation Loss = 0.69629\n",
      "Epoch 5: Training Loss = 0.52614, Validation Loss = 0.58330\n",
      "Epoch 6: Training Loss = 0.44292, Validation Loss = 0.55775\n",
      "Epoch 7: Training Loss = 0.41268, Validation Loss = 0.52127\n",
      "Epoch 8: Training Loss = 0.37640, Validation Loss = 0.46679\n",
      "Epoch 9: Training Loss = 0.34258, Validation Loss = 0.45197\n",
      "Epoch 10: Training Loss = 0.31980, Validation Loss = 0.43210\n",
      "Epoch 11: Training Loss = 0.30310, Validation Loss = 0.40939\n",
      "Epoch 12: Training Loss = 0.28940, Validation Loss = 0.40166\n",
      "Epoch 13: Training Loss = 0.27425, Validation Loss = 0.38031\n",
      "Epoch 14: Training Loss = 0.25860, Validation Loss = 0.37390\n",
      "Epoch 15: Training Loss = 0.25037, Validation Loss = 0.37236\n",
      "Epoch 16: Training Loss = 0.24417, Validation Loss = 0.35423\n",
      "Epoch 17: Training Loss = 0.23625, Validation Loss = 0.34868\n",
      "Epoch 18: Training Loss = 0.23009, Validation Loss = 0.33637\n",
      "Epoch 19: Training Loss = 0.22022, Validation Loss = 0.32828\n",
      "Epoch 20: Training Loss = 0.21254, Validation Loss = 0.32470\n",
      "Epoch 21: Training Loss = 0.20858, Validation Loss = 0.31452\n",
      "Epoch 22: Training Loss = 0.20317, Validation Loss = 0.31838\n",
      "Epoch 23: Training Loss = 0.20197, Validation Loss = 0.30365\n",
      "Epoch 24: Training Loss = 0.19183, Validation Loss = 0.29562\n",
      "Epoch 25: Training Loss = 0.18714, Validation Loss = 0.29478\n",
      "Epoch 26: Training Loss = 0.18225, Validation Loss = 0.28769\n",
      "Epoch 27: Training Loss = 0.17898, Validation Loss = 0.29633\n",
      "Epoch 28: Training Loss = 0.17993, Validation Loss = 0.27872\n",
      "Epoch 29: Training Loss = 0.17461, Validation Loss = 0.27788\n",
      "Epoch 30: Training Loss = 0.17389, Validation Loss = 0.28216\n",
      "Epoch 31: Training Loss = 0.16911, Validation Loss = 0.27239\n",
      "Epoch 32: Training Loss = 0.16324, Validation Loss = 0.26593\n",
      "Epoch 33: Training Loss = 0.16072, Validation Loss = 0.26268\n",
      "Epoch 34: Training Loss = 0.15785, Validation Loss = 0.25994\n",
      "Epoch 35: Training Loss = 0.15440, Validation Loss = 0.26259\n",
      "Epoch 36: Training Loss = 0.15215, Validation Loss = 0.25443\n",
      "Epoch 37: Training Loss = 0.14876, Validation Loss = 0.25648\n",
      "Epoch 38: Training Loss = 0.14792, Validation Loss = 0.25154\n",
      "Epoch 39: Training Loss = 0.14549, Validation Loss = 0.25030\n",
      "Epoch 40: Training Loss = 0.14825, Validation Loss = 0.24957\n",
      "Epoch 41: Training Loss = 0.14441, Validation Loss = 0.25082\n",
      "Epoch 42: Training Loss = 0.14067, Validation Loss = 0.24448\n",
      "Epoch 43: Training Loss = 0.13858, Validation Loss = 0.24193\n",
      "Epoch 44: Training Loss = 0.13529, Validation Loss = 0.23665\n",
      "Epoch 45: Training Loss = 0.13295, Validation Loss = 0.23468\n",
      "Epoch 46: Training Loss = 0.12967, Validation Loss = 0.23263\n",
      "Epoch 47: Training Loss = 0.12935, Validation Loss = 0.23683\n",
      "Epoch 48: Training Loss = 0.12759, Validation Loss = 0.23087\n",
      "Epoch 49: Training Loss = 0.12526, Validation Loss = 0.23410\n"
     ]
    }
   ],
   "source": [
    "#Run training\n",
    "params = initialize_model_parameters()\n",
    "batch_size = 1000\n",
    "lr = 0.1\n",
    "beta = 0.9\n",
    "#Lower epochs for train time if needed. Results are around 90% accurate at 25 epochs\n",
    "epochs = 50\n",
    "params, hist = train_cnn_model(X_train, Y_train, X_val, Y_val, params, batch_size, lr, beta, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f2b982ba280>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot the training curve\n",
    "#Separate out the losses from training and the epochs\n",
    "valLosses = [x[1] for x in hist]\n",
    "trainLosses = [x[0] for x in hist]\n",
    "epochs = np.arange(epochs)\n",
    "\n",
    "#Plot them \n",
    "plt.title('Losses vs epoch')\n",
    "plt.plot(epochs, valLosses, label='Validation loss')\n",
    "plt.plot(epochs, trainLosses, label='Train loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Losses(Cross Entropy with Regularization)')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix for validation set:\n",
      "[[449   0   1   0   0   3   7   0   5   4]\n",
      " [  0 563   0   0   0   0   3   7   0   7]\n",
      " [  2   3 506   5   3   0   3  18   2   1]\n",
      " [  0   2   6 452   0  16   0   8  22  11]\n",
      " [  0   1   1   0 450   2   9   3   5   7]\n",
      " [  2   0   1  21   0 424  19   0   7   3]\n",
      " [  5   1   3   1   9   3 417   0   3   2]\n",
      " [  0   0   8  11   3   1   1 451   5   3]\n",
      " [  2   1   2   5   1   3   3   3 435   3]\n",
      " [  0   0   2   5  34   4   0  22   5 479]]\n",
      "Accuracy on Valid set:\n",
      "92.52%\n"
     ]
    }
   ],
   "source": [
    "#Run the predict function \n",
    "Y_pred, P = predict(params, X_val)\n",
    "#Extract the actual values\n",
    "Y_act = np.argmax(Y_val, axis = 1)\n",
    "\n",
    "#Make a confusion matrix and a tally for correct predictions\n",
    "confMat = np.zeros((10,10))\n",
    "correct = 0\n",
    "\n",
    "#Iterate through predictions, increment confusion matrix and correct tally\n",
    "for x in range(len(Y_pred)):\n",
    "    confMat[Y_pred[x]][Y_act[x]] +=1\n",
    "    if Y_pred[x] == Y_act[x]:\n",
    "        correct +=1\n",
    "    \n",
    "print(\"Confusion matrix for validation set:\")\n",
    "print(confMat.astype(int))\n",
    "print(\"Accuracy on Valid set:\")\n",
    "print(str(correct*100/len(Y_pred)) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "26cc047130d15b3a0a51dd5fa7082d50",
     "grade": true,
     "grade_id": "cell-b54e48e8fed1c7c9",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#intentionally blank\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
